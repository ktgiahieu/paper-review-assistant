{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "978762e0",
   "metadata": {},
   "source": [
    "# Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63a3bc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading base data...\n",
      "Loaded and merged base data. Shape: (2136, 4)\n",
      "\n",
      "Step 2: Collecting metareview data...\n",
      "Scanning for metareview data in: ../data/metareviews\n",
      "Collected mention data for 2136 flaws.\n",
      "Data shape after merging mention data: (2136, 6)\n",
      "\n",
      "Step 3: Collecting LLM review data...\n",
      "Scanning for LLM review data in: ../data/reviews\n",
      "Collected LLM reviews for 2136 flaws.\n",
      "Data shape after merging LLM review data: (2136, 7)\n",
      "\n",
      "Step 4: Finalizing and saving the dataset...\n",
      "\n",
      "Successfully created aggregated dataset with 2136 rows.\n",
      "File saved to: neurips_2024_aggregated_flaws.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def load_json_file(filepath):\n",
    "    \"\"\"Safely loads a single JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except (FileNotFoundError, json.JSONDecodeError, UnicodeDecodeError) as e:\n",
    "        print(f\"Warning: Could not read or parse {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "def collect_metareview_data(metareviews_path, venue_folder_name):\n",
    "    \"\"\"\n",
    "    Collects 'is_flaw_mentioned' and 'mention_reasoning' from metareview JSON files.\n",
    "    \"\"\"\n",
    "    mention_data = {}\n",
    "    print(f\"Scanning for metareview data in: {metareviews_path}\")\n",
    "\n",
    "    if not os.path.exists(metareviews_path):\n",
    "        print(f\"Warning: Metareviews directory not found at {metareviews_path}\")\n",
    "        return mention_data\n",
    "\n",
    "    for model_name in ['o3']:# os.listdir(metareviews_path):\n",
    "        model_path = os.path.join(metareviews_path, model_name)\n",
    "        if os.path.isdir(model_path):\n",
    "            venue_path = os.path.join(model_path, venue_folder_name)\n",
    "            if os.path.isdir(venue_path):\n",
    "                for status in ['accepted', 'rejected']:\n",
    "                    status_path = os.path.join(venue_path, status)\n",
    "                    if os.path.isdir(status_path):\n",
    "                        for filename in os.listdir(status_path):\n",
    "                            if filename.endswith(\".json\"):\n",
    "                                data = load_json_file(os.path.join(status_path, filename))\n",
    "                                if not data:\n",
    "                                    continue\n",
    "                                for paper_key, flaws in data.items():\n",
    "                                    openreview_id = paper_key.split('_')[0]\n",
    "                                    for flaw in flaws:\n",
    "                                        flaw_id = flaw.get('flaw_id')\n",
    "                                        if openreview_id and flaw_id:\n",
    "                                            key = (openreview_id, flaw_id)\n",
    "                                            mention_data[key] = {\n",
    "                                                'is_flaw_mentioned': flaw.get('is_flaw_mentioned'),\n",
    "                                                'mention_reasoning': flaw.get('mention_reasoning')\n",
    "                                            }\n",
    "    print(f\"Collected mention data for {len(mention_data)} flaws.\")\n",
    "    return mention_data\n",
    "\n",
    "def collect_llm_review_data(reviews_path, venue_folder_name):\n",
    "    \"\"\"\n",
    "    Collects the full LLM review content from individual review JSON files.\n",
    "    \"\"\"\n",
    "    review_data = {}\n",
    "    print(f\"Scanning for LLM review data in: {reviews_path}\")\n",
    "\n",
    "    if not os.path.exists(reviews_path):\n",
    "        print(f\"Warning: Reviews directory not found at {reviews_path}\")\n",
    "        return review_data\n",
    "\n",
    "    for model_name in ['o3']: #os.listdir(reviews_path):\n",
    "        model_path = os.path.join(reviews_path, model_name)\n",
    "        if os.path.isdir(model_path):\n",
    "            venue_path = os.path.join(model_path, venue_folder_name)\n",
    "            if os.path.isdir(venue_path):\n",
    "                for status in ['accepted', 'rejected']:\n",
    "                    status_path = os.path.join(venue_path, status)\n",
    "                    if not os.path.isdir(status_path): continue\n",
    "                    for paper_folder in os.listdir(status_path):\n",
    "                        paper_folder_path = os.path.join(status_path, paper_folder)\n",
    "                        if not os.path.isdir(paper_folder_path): continue\n",
    "                        \n",
    "                        openreview_id = paper_folder.split('_')[0]\n",
    "                        for filename in os.listdir(paper_folder_path):\n",
    "                            if filename.endswith(\"_review.json\"):\n",
    "                                # Extract flaw_id from filename\n",
    "                                # e.g., 0aN7VWwp4g_2410_23159_incorrect_csi_thresholds_review.json\n",
    "                                # -> incorrect_csi_thresholds\n",
    "                                match = re.match(r'(.+?)_(\\d+_\\d+)_(.+)_review\\.json', filename)\n",
    "                                if match:\n",
    "                                    flaw_id = match.group(3)\n",
    "                                else:\n",
    "                                    # Fallback for different naming\n",
    "                                    base_name = filename.replace('_review.json', '')\n",
    "                                    # Assuming the last part is the flaw id\n",
    "                                    flaw_id = '_'.join(base_name.split('_')[3:])\n",
    "\n",
    "                                if not flaw_id: continue\n",
    "                                \n",
    "                                data = load_json_file(os.path.join(paper_folder_path, filename))\n",
    "                                if data is not None:\n",
    "                                    key = (openreview_id, flaw_id)\n",
    "                                    review_data[key] = {'llm_review': json.dumps(data, indent=2)}\n",
    "    \n",
    "    print(f\"Collected LLM reviews for {len(review_data)} flaws.\")\n",
    "    return review_data\n",
    "\n",
    "def create_aggregated_dataset(venue_folder_name, base_data_dir, categorized_data_dir, output_filename):\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the data aggregation process.\n",
    "    \"\"\"\n",
    "    # Define paths to the different data sources\n",
    "    flawed_papers_dir = os.path.join(base_data_dir, 'flawed_papers', venue_folder_name)\n",
    "    metareviews_dir = os.path.join(base_data_dir, 'metareviews')\n",
    "    reviews_dir = os.path.join(base_data_dir, 'reviews')\n",
    "\n",
    "    # --- 1. Load Base Data: Categories and Descriptions ---\n",
    "    print(\"Step 1: Loading base data...\")\n",
    "    # Load categorized flaws (openreview_id, flaw_id, category_ids)\n",
    "    categories_path = os.path.join(categorized_data_dir, 'flawed_papers', venue_folder_name, 'categorized_flaw_cleaned.csv')\n",
    "    try:\n",
    "        categories_df = pd.read_csv(categories_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Base category file not found at {categories_path}. Cannot proceed.\")\n",
    "        return\n",
    "\n",
    "    # Load flaw descriptions\n",
    "    descriptions_path = os.path.join(flawed_papers_dir, 'flawed_papers_global_summary.csv')\n",
    "    try:\n",
    "        descriptions_df = pd.read_csv(descriptions_path)[['openreview_id', 'flaw_id', 'flaw_description']]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Flaw description file not found at {descriptions_path}.\")\n",
    "        descriptions_df = pd.DataFrame(columns=['openreview_id', 'flaw_id', 'flaw_description'])\n",
    "\n",
    "    # Merge categories and descriptions\n",
    "    df = pd.merge(categories_df, descriptions_df, on=['openreview_id', 'flaw_id'], how='left')\n",
    "    print(f\"Loaded and merged base data. Shape: {df.shape}\")\n",
    "\n",
    "    # --- 2. Collect and Merge Metareview Data ---\n",
    "    print(\"\\nStep 2: Collecting metareview data...\")\n",
    "    mention_data = collect_metareview_data(metareviews_dir, venue_folder_name)\n",
    "    mention_df = pd.DataFrame.from_dict(mention_data, orient='index').reset_index()\n",
    "    mention_df.rename(columns={'level_0': 'openreview_id', 'level_1': 'flaw_id'}, inplace=True)\n",
    "    df = pd.merge(df, mention_df, on=['openreview_id', 'flaw_id'], how='left')\n",
    "    print(f\"Data shape after merging mention data: {df.shape}\")\n",
    "\n",
    "    # --- 3. Collect and Merge LLM Review Data ---\n",
    "    print(\"\\nStep 3: Collecting LLM review data...\")\n",
    "    review_data = collect_llm_review_data(reviews_dir, venue_folder_name)\n",
    "    review_df = pd.DataFrame.from_dict(review_data, orient='index').reset_index()\n",
    "    review_df.rename(columns={'level_0': 'openreview_id', 'level_1': 'flaw_id'}, inplace=True)\n",
    "    df = pd.merge(df, review_df, on=['openreview_id', 'flaw_id'], how='left')\n",
    "    print(f\"Data shape after merging LLM review data: {df.shape}\")\n",
    "    \n",
    "    # --- 4. Finalize and Save ---\n",
    "    print(\"\\nStep 4: Finalizing and saving the dataset...\")\n",
    "    # Ensure all required columns are present\n",
    "    final_columns = [\n",
    "        'openreview_id', 'flaw_id', 'category_ids', 'flaw_description',\n",
    "        'llm_review', 'is_flaw_mentioned', 'mention_reasoning'\n",
    "    ]\n",
    "    for col in final_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "    \n",
    "    # Reorder columns and save to CSV\n",
    "    final_df = df[final_columns]\n",
    "    final_df.to_csv(output_filename, index=False)\n",
    "    print(f\"\\nSuccessfully created aggregated dataset with {len(final_df)} rows.\")\n",
    "    print(f\"File saved to: {output_filename}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- Configuration ---\n",
    "    # Define the target venue and directory structure\n",
    "    VENUE_FOLDER_NAME = 'NeurIPS2024_latest_flawed_papers_v1'\n",
    "    BASE_DATA_DIRECTORY = '../data' # Adjusted to match provided folder structure\n",
    "    CATEGORIZED_DATA_DIRECTORY = './extracted_data'\n",
    "    \n",
    "    # Define the name for the output CSV file\n",
    "    OUTPUT_FILENAME = 'neurips_2024_aggregated_flaws.csv'\n",
    "\n",
    "    # Run the aggregation process\n",
    "    create_aggregated_dataset(VENUE_FOLDER_NAME, BASE_DATA_DIRECTORY, CATEGORIZED_DATA_DIRECTORY, OUTPUT_FILENAME)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a28845a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>openreview_id</th>\n",
       "      <th>flaw_id</th>\n",
       "      <th>category_ids</th>\n",
       "      <th>flaw_description</th>\n",
       "      <th>llm_review</th>\n",
       "      <th>is_flaw_mentioned</th>\n",
       "      <th>mention_reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wSqpNeMVLU</td>\n",
       "      <td>missing_real_world_batch_experiments</td>\n",
       "      <td>\"1b,3b\"</td>\n",
       "      <td>Reviewers qBZ2, GGsr, and the program chairs a...</td>\n",
       "      <td>{\\n  \"summary\": \"The paper develops a unified ...</td>\n",
       "      <td>True</td>\n",
       "      <td>The review states: \"Empirical validation is mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bioHNTRnQk</td>\n",
       "      <td>kernel_regression_claim</td>\n",
       "      <td>\"3b\"</td>\n",
       "      <td>The paper repeatedly states that its theoretic...</td>\n",
       "      <td>{\\n  \"summary\": \"The paper develops a theoreti...</td>\n",
       "      <td>False</td>\n",
       "      <td>The review treats the kernel ridge regression ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7W0f7lifDk</td>\n",
       "      <td>low_output_resolution</td>\n",
       "      <td>\"2a\"</td>\n",
       "      <td>The method currently operates at only 256×256 ...</td>\n",
       "      <td>{\\n  \"summary\": \"The paper introduces Human-3D...</td>\n",
       "      <td>False</td>\n",
       "      <td>The review never refers to the 256×256 output ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UahrHR5HQh</td>\n",
       "      <td>missing_comparison_dirichlet_flow</td>\n",
       "      <td>\"1a\"</td>\n",
       "      <td>Reviewer vH9B identified the absence of an exp...</td>\n",
       "      <td>{\\n  \"summary\": \"The paper reframes Flow Match...</td>\n",
       "      <td>True</td>\n",
       "      <td>The review states: \"Baseline coverage is incom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CW0OVWEKKu</td>\n",
       "      <td>lack_of_rigorous_theory</td>\n",
       "      <td>\"2b\"</td>\n",
       "      <td>Reviewers highlighted the absence of a formal ...</td>\n",
       "      <td>{\\n  \"summary\": \"The paper revisits the proble...</td>\n",
       "      <td>True</td>\n",
       "      <td>The review explicitly states: \"**Heuristic, no...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  openreview_id                               flaw_id category_ids  \\\n",
       "0    wSqpNeMVLU  missing_real_world_batch_experiments      \"1b,3b\"   \n",
       "1    bioHNTRnQk               kernel_regression_claim         \"3b\"   \n",
       "2    7W0f7lifDk                 low_output_resolution         \"2a\"   \n",
       "3    UahrHR5HQh     missing_comparison_dirichlet_flow         \"1a\"   \n",
       "4    CW0OVWEKKu               lack_of_rigorous_theory         \"2b\"   \n",
       "\n",
       "                                    flaw_description  \\\n",
       "0  Reviewers qBZ2, GGsr, and the program chairs a...   \n",
       "1  The paper repeatedly states that its theoretic...   \n",
       "2  The method currently operates at only 256×256 ...   \n",
       "3  Reviewer vH9B identified the absence of an exp...   \n",
       "4  Reviewers highlighted the absence of a formal ...   \n",
       "\n",
       "                                          llm_review  is_flaw_mentioned  \\\n",
       "0  {\\n  \"summary\": \"The paper develops a unified ...               True   \n",
       "1  {\\n  \"summary\": \"The paper develops a theoreti...              False   \n",
       "2  {\\n  \"summary\": \"The paper introduces Human-3D...              False   \n",
       "3  {\\n  \"summary\": \"The paper reframes Flow Match...               True   \n",
       "4  {\\n  \"summary\": \"The paper revisits the proble...               True   \n",
       "\n",
       "                                   mention_reasoning  \n",
       "0  The review states: \"Empirical validation is mi...  \n",
       "1  The review treats the kernel ridge regression ...  \n",
       "2  The review never refers to the 256×256 output ...  \n",
       "3  The review states: \"Baseline coverage is incom...  \n",
       "4  The review explicitly states: \"**Heuristic, no...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(OUTPUT_FILENAME)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6456f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>openreview_id</th>\n",
       "      <th>flaw_id</th>\n",
       "      <th>flaw_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wSqpNeMVLU</td>\n",
       "      <td>missing_real_world_batch_experiments</td>\n",
       "      <td>Reviewers qBZ2, GGsr, and the program chairs a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bioHNTRnQk</td>\n",
       "      <td>kernel_regression_claim</td>\n",
       "      <td>The paper repeatedly states that its theoretic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7W0f7lifDk</td>\n",
       "      <td>low_output_resolution</td>\n",
       "      <td>The method currently operates at only 256×256 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UahrHR5HQh</td>\n",
       "      <td>missing_comparison_dirichlet_flow</td>\n",
       "      <td>Reviewer vH9B identified the absence of an exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CW0OVWEKKu</td>\n",
       "      <td>lack_of_rigorous_theory</td>\n",
       "      <td>Reviewers highlighted the absence of a formal ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  openreview_id                               flaw_id  \\\n",
       "0    wSqpNeMVLU  missing_real_world_batch_experiments   \n",
       "1    bioHNTRnQk               kernel_regression_claim   \n",
       "2    7W0f7lifDk                 low_output_resolution   \n",
       "3    UahrHR5HQh     missing_comparison_dirichlet_flow   \n",
       "4    CW0OVWEKKu               lack_of_rigorous_theory   \n",
       "\n",
       "                                    flaw_description  \n",
       "0  Reviewers qBZ2, GGsr, and the program chairs a...  \n",
       "1  The paper repeatedly states that its theoretic...  \n",
       "2  The method currently operates at only 256×256 ...  \n",
       "3  Reviewer vH9B identified the absence of an exp...  \n",
       "4  Reviewers highlighted the absence of a formal ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['openreview_id', 'flaw_id', 'flaw_description']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2ec9537",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"{OUTPUT_FILENAME.split('.')[0]}_shortened.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ad491899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running script with settings: Input='sample_input.csv'...\n",
      "--- Step 1: Extracting Reviewer IDs ---\n",
      "Reading flaws from sample_input.csv...\n",
      "Extracting and filtering reviewer IDs from flaw descriptions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning descriptions: 100%|██████████| 2136/2136 [00:00<00:00, 23957.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated intermediate CSV with 2066 entries at: output/flaws_with_reviewers.csv\n",
      "\n",
      "--- Step 2: Fetching Full Reviews from API (Iterative) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching reviews for 2066 entries with identified reviewers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers: 100%|██████████| 2066/2066 [07:36<00:00,  4.52it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated final reviews CSV at: output/reviews_data.csv\n",
      "\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "import openreview\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# --- Configuration for Reviewer ID Extraction ---\n",
    "\n",
    "# A blocklist of common 4-letter words, acronyms, and other false positives.\n",
    "# This helps prevent matching common English words or technical terms.\n",
    "REVIEWER_ID_BLOCKLIST = {\n",
    "    'thus', 'flow', 'they', 'this', 'self', 'both', 'relu', 'gnns', 'llms',\n",
    "    'snip', 'elbo', 'geom', 'grad', 'mesa', '2sls', 'ivar', 'wdcf', 'from',\n",
    "    'with', 'that', 'what', 'when', 'were', 'have', 'been', 'also', 'some'\n",
    "}\n",
    "\n",
    "\n",
    "def extract_reviewers_from_flaws(input_csv_path: str, output_csv_path: str):\n",
    "    \"\"\"\n",
    "    Reads a CSV with flaw descriptions, extracts potential reviewer IDs, filters out\n",
    "    false positives, and writes a new CSV with the valid IDs. Rows without any\n",
    "    found IDs are discarded.\n",
    "\n",
    "    Args:\n",
    "        input_csv_path (str): Path to the input CSV file.\n",
    "        output_csv_path (str): Path to save the intermediate output CSV file.\n",
    "    \"\"\"\n",
    "    print(f\"Reading flaws from {input_csv_path}...\")\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file not found at {input_csv_path}\")\n",
    "        return\n",
    "\n",
    "    # This regex is more specific, targeting only 4-character alphanumeric strings.\n",
    "    # Further filtering is applied in the code to increase accuracy.\n",
    "    reviewer_id_pattern = re.compile(r'\\b[a-zA-Z0-9]{4}\\b')\n",
    "\n",
    "    results = []\n",
    "    print(\"Extracting and filtering reviewer IDs from flaw descriptions...\")\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Scanning descriptions\"):\n",
    "        description = row.get('flaw_description', '')\n",
    "        if pd.isna(description):\n",
    "            description = ''\n",
    "\n",
    "        # Find all potential 4-character IDs\n",
    "        found_ids = reviewer_id_pattern.findall(str(description))\n",
    "        \n",
    "        # Filter out false positives\n",
    "        filtered_ids = []\n",
    "        for an_id in found_ids:\n",
    "            # Rule 1: Discard if it's a pure number (e.g., '2024')\n",
    "            if an_id.isdigit():\n",
    "                continue\n",
    "            # Rule 2: Discard if it's a common word or acronym in our blocklist\n",
    "            if an_id.lower() in REVIEWER_ID_BLOCKLIST:\n",
    "                continue\n",
    "            filtered_ids.append(an_id)\n",
    "\n",
    "        # Get the unique set of valid IDs\n",
    "        unique_ids = sorted(list(set(filtered_ids)))\n",
    "\n",
    "        # Only include rows where at least one valid reviewer ID was found\n",
    "        if unique_ids:\n",
    "            results.append({\n",
    "                'openreview_id': row['openreview_id'],\n",
    "                'flaw_id': row['flaw_id'],\n",
    "                'reviewers_ids': ','.join(unique_ids)  # Join IDs into a single string\n",
    "            })\n",
    "\n",
    "    if not results:\n",
    "        print(\"Warning: No valid reviewer IDs were found in any of the descriptions. The output file will be empty.\")\n",
    "        # Create an empty file with the correct headers\n",
    "        pd.DataFrame(columns=['openreview_id', 'flaw_id', 'reviewers_ids']).to_csv(output_csv_path, index=False)\n",
    "        return\n",
    "\n",
    "    output_df = pd.DataFrame(results)\n",
    "    output_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Successfully generated intermediate CSV with {len(output_df)} entries at: {output_csv_path}\")\n",
    "\n",
    "def get_openreview_client():\n",
    "    \"\"\"Initializes and returns a connected OpenReview v2 client.\"\"\"\n",
    "    username = os.environ.get('OPENREVIEW_USERNAME')\n",
    "    password = os.environ.get('OPENREVIEW_PASSWORD')\n",
    "    if not username or not password:\n",
    "        raise ValueError(\"OPENREVIEW_USERNAME and OPENREVIEW_PASSWORD environment variables must be set in a .env file.\")\n",
    "\n",
    "    try:\n",
    "        client = openreview.api.OpenReviewClient(\n",
    "            baseurl='https://api2.openreview.net',\n",
    "            username=username,\n",
    "            password=password\n",
    "        )\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect to OpenReview: {e}\")\n",
    "        return None\n",
    "\n",
    "def fetch_reviews_and_generate_final_csv(intermediate_csv_path: str, output_csv_path: str):\n",
    "    \"\"\"\n",
    "    Reads the intermediate CSV, fetches review content for each paper individually,\n",
    "    and generates the final CSV with the detailed reviews.\n",
    "\n",
    "    Args:\n",
    "        intermediate_csv_path (str): Path to the CSV with reviewer IDs.\n",
    "        output_csv_path (str): Path to save the final output CSV file.\n",
    "    \"\"\"\n",
    "    client = get_openreview_client()\n",
    "    if not client:\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        df = pd.read_csv(intermediate_csv_path)\n",
    "        df = df.dropna(subset=['reviewers_ids'])\n",
    "        df = df[df['reviewers_ids'] != '']\n",
    "    except (FileNotFoundError, pd.errors.EmptyDataError):\n",
    "        print(f\"Intermediate file not found or is empty at {intermediate_csv_path}. Cannot fetch reviews.\")\n",
    "        pd.DataFrame(columns=['openreview_id', 'flaw_id', 'reviewer_id', 'human_review']).to_csv(output_csv_path, index=False)\n",
    "        return\n",
    "\n",
    "    final_results = []\n",
    "    print(f\"Fetching reviews for {len(df)} entries with identified reviewers...\")\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing papers\"):\n",
    "        openreview_id = row['openreview_id']\n",
    "        flaw_id = row['flaw_id']\n",
    "        reviewer_ids = str(row['reviewers_ids']).split(',')\n",
    "\n",
    "        try:\n",
    "            # --- Per-Paper API Call ---\n",
    "            note = client.get_note(openreview_id, details='replies')\n",
    "            note_replies = note.details.get('replies', [])\n",
    "            \n",
    "            if not note_replies:\n",
    "                tqdm.write(f\"Note for '{openreview_id}' found, but it has no replies/reviews. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            review_map = {}\n",
    "            for reply in note_replies:\n",
    "                # Official reviews are identified by having content like 'rating', 'strengths', etc.\n",
    "                if reply and 'signatures' in reply and 'content' in reply and any(key in reply.get('content', {}) for key in ['review', 'rating', 'strengths', 'weaknesses', 'summary']):\n",
    "                    for sig in reply['signatures']:\n",
    "                        # Assumes signature format like '.../Reviewer_mZ7h'\n",
    "                        reviewer_id_from_sig = sig.split('_')[-1]\n",
    "                        review_content_parts = []\n",
    "                        for key, value_obj in reply['content'].items():\n",
    "                            if isinstance(value_obj, dict) and 'value' in value_obj:\n",
    "                                actual_value = value_obj.get('value')\n",
    "                                if isinstance(actual_value, str) and actual_value.strip():\n",
    "                                    review_content_parts.append(f\"## {key.replace('_', ' ').title()}\\n{actual_value.strip()}\")\n",
    "                        review_map[reviewer_id_from_sig] = \"\\n\\n\".join(review_content_parts)\n",
    "\n",
    "\n",
    "            for target_id in reviewer_ids:\n",
    "                if target_id in review_map:\n",
    "                    final_results.append({\n",
    "                        'openreview_id': openreview_id,\n",
    "                        'flaw_id': flaw_id,\n",
    "                        'reviewer_id': target_id,\n",
    "                        'human_review': review_map[target_id]\n",
    "                    })\n",
    "                else:\n",
    "                    pass\n",
    "                    # tqdm.write(f\"Warning: Could not match review for ID '{target_id}' in paper '{openreview_id}'.\")\n",
    "        \n",
    "        # except openreview.core.OpenReviewException as e:\n",
    "        #     tqdm.write(f\"OpenReview API Error for '{openreview_id}': {e}. Skipping.\")\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"An unexpected error occurred while processing {openreview_id}: {e}. Skipping.\")\n",
    "\n",
    "    if not final_results:\n",
    "        print(\"No review data was successfully matched. The final CSV will be empty.\")\n",
    "        pd.DataFrame(columns=['openreview_id', 'flaw_id', 'reviewer_id', 'human_review']).to_csv(output_csv_path, index=False)\n",
    "    else:\n",
    "        output_df = pd.DataFrame(final_results)\n",
    "        output_df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"Successfully generated final reviews CSV at: {output_csv_path}\")\n",
    "\n",
    "def run_review_processing(input_csv_path: str, output_dir: str = \"output\"):\n",
    "    \"\"\"\n",
    "    Orchestrates the two-step process of extracting reviewer IDs and fetching reviews.\n",
    "\n",
    "    Args:\n",
    "        input_csv_path (str): Path to the input CSV file containing flaw data.\n",
    "        output_dir (str, optional): Directory to save the output files. Defaults to \"output\".\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    intermediate_csv_path = os.path.join(output_dir, 'flaws_with_reviewers.csv')\n",
    "    final_csv_path = os.path.join(output_dir, 'reviews_data.csv')\n",
    "\n",
    "    print(\"--- Step 1: Extracting Reviewer IDs ---\")\n",
    "    extract_reviewers_from_flaws(input_csv_path, intermediate_csv_path)\n",
    "\n",
    "    print(\"\\n--- Step 2: Fetching Full Reviews from API (Iterative) ---\")\n",
    "    fetch_reviews_and_generate_final_csv(intermediate_csv_path, final_csv_path)\n",
    "    \n",
    "    print(\"\\nProcessing complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Configuration ---\n",
    "    INPUT_FILE = \"sample_input.csv\"\n",
    "    OUTPUT_DIR = \"output\"\n",
    "    # -------------------\n",
    "\n",
    "    print(f\"Running script with settings: Input='{INPUT_FILE}'...\")\n",
    "    \n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"\\nERROR: The input file '{INPUT_FILE}' was not found.\")\n",
    "        print(\"Please create it or update the INPUT_FILE variable in the script.\")\n",
    "    else:\n",
    "        run_review_processing(input_csv_path=INPUT_FILE, output_dir=OUTPUT_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d7dde9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e078fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d50c508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checklist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
