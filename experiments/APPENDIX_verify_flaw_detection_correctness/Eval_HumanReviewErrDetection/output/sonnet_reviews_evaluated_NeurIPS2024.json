[
  {
    "openreview_id": "wSqpNeMVLU",
    "reviewer_id": "GGsr",
    "evaluation": {
      "flaw_id": "missing_real_world_batch_experiments",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the weaknesses section: 'The experiments are not sufficient. I would like to see improvements in batch speculative sampling in real-world scenarios.'",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review does identify the absence of real-world batch experiments, the reasoning is superficial and lacks the depth described in the ground truth. The reviewer simply states they 'would like to see improvements in batch speculative sampling in real-world scenarios' but fails to explain why this is a critical flaw. The ground truth describes this as 'the only major remaining weakness' that leaves 'the paper's core claim about the practical efficiency gains of batch speculative decoding...unvalidated' and makes the claim that 'new real-world experiments are required for publication.' The review does not capture the severity of this omission or its impact on the paper's core claims about practical efficiency gains."
    }
  },
  {
    "openreview_id": "wSqpNeMVLU",
    "reviewer_id": "qBZ2",
    "evaluation": {
      "flaw_id": "missing_real_world_batch_experiments",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions the lack of empirical validation in the weaknesses section: 'While the theoretical contributions are significant, the paper would benefit from more extensive empirical validation.' It also notes in the limitations section: 'The results may not guarantee optimality in practical situations because real-world circumstances are more complex and varied than those considered in the theoretical analysis.'",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review does identify the lack of empirical validation, its reasoning is superficial and generic compared to the ground truth. The ground truth describes a specific, critical gap: the complete absence of real-world batch speculative decoding experiments that would validate the paper's core practical claims. The ground truth emphasizes that this gap was highlighted by multiple reviewers as the 'only major remaining weakness' and that the authors themselves acknowledged it was 'impossible' to demonstrate improvements in practice. In contrast, the review's reasoning is much weaker - it merely suggests the paper 'would benefit from more extensive empirical validation' without recognizing the severity of having zero real-world validation for the core practical claims. The review treats this as a minor enhancement rather than a fundamental flaw that leaves the paper's main practical contributions unvalidated."
    }
  },
  {
    "openreview_id": "bioHNTRnQk",
    "reviewer_id": "jLLV",
    "evaluation": {
      "flaw_id": "kernel_regression_claim",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the Weaknesses section: 'In addition, the paper multiple times claims to analyse the kernel ridge regression model. However, *none* of the models described are in the kernel regression setting. While it may be an uncomplicated extension, as discussed in Appendix B, the current version of the paper does not perform this extension - and none of the results are presented in this setting. Thus, currently the paper is overselling its contributions.' The review also mentions it again in the Further comments section: 'line 30 - kernel regression is not analysed in this paper'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns very well with the ground truth description. The reviewer correctly identifies that: (1) the paper claims to analyze kernel ridge regression multiple times, (2) no such analysis is actually provided in the paper, (3) while Appendix B mentions it as a potential extension, it's not actually performed, (4) this constitutes 'overselling its contributions' - which matches the ground truth's characterization of it as an 'over-claim that oversells the contribution' and 'misrepresents its methodological scope.' The reviewer captures both the nature of the flaw (unfulfilled claims about scope) and its negative impact (misrepresentation of contributions)."
    }
  },
  {
    "openreview_id": "UahrHR5HQh",
    "reviewer_id": "vH9B",
    "evaluation": {
      "flaw_id": "missing_comparison_dirichlet_flow",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions the missing comparison with Dirichlet Flow: 'Given this, the lack of an appropriate comparison to Dirichlet Flow in the experiments represents a significant drawback of the current paper.' The review also discusses how Dirichlet Flow already implements the core idea, stating: 'the core idea of learning the posterior first (i.e., the variational formulation) and evaluating the marginal velocity using the expected conditional velocity w.r.t. the posterior has already been proposed in Dirichlet Flow [1].'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning perfectly aligns with the ground truth description. The reviewer correctly identifies that: (1) Dirichlet Flow already implements the core idea of learning the posterior first, which is the main contribution claimed by the paper, (2) this overlap makes the comparison essential rather than optional, and (3) the absence of this comparison represents a 'significant drawback.' The reviewer even addresses potential counterarguments by noting that using a specific Dirichlet distribution is 'mainly a design choice which does not affect the generality of Dirichlet Flow.' This demonstrates a deep understanding of why this missing comparison is a major weakness - it calls into question the novelty and positioning of the work."
    }
  },
  {
    "openreview_id": "FEmag0szWo",
    "reviewer_id": "xf7T",
    "evaluation": {
      "flaw_id": "missing_complexity_analysis",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in the Questions section: 'On the theoretical side, there is no discussion about the role of m and n in the results. In the current form of the statements, m and n are fixed, so the size of the graph inputs is fixed. Can the authors explain why this is a not a fundamental issue if one is interested in say, training a GNN that predicts SB scores across MILPs of different sizes?' This directly addresses the missing complexity analysis regarding the size and practical implications of the required GNNs.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth flaw. While the ground truth specifically mentions that 'the necessary MP-GNN may itself be exponentially large, which could negate the practical usefulness of the result,' the review identifies the same core issue by questioning the lack of discussion about the role of problem size parameters (m and n) and pointing out that current results assume fixed sizes. The reviewer correctly identifies that this is 'a fundamental issue' for practical applications across MILPs of different sizes, which captures the essence of the complexity concern described in the ground truth - that without bounds on GNN complexity, the theoretical results may have limited practical value."
    }
  },
  {
    "openreview_id": "xNncVKbwwS",
    "reviewer_id": "VeB9",
    "evaluation": {
      "flaw_id": "bounded_domain_gradient_assumption",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the Limitations section: 'Yes, the paper points out in the conclusion that the bounded domain / gradient assumption is a significant limitation that they hope to address in future work.'",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies the bounded domain/gradient assumption as a significant limitation, it provides no reasoning for *why* this is a flaw or what its implications are. The review simply acknowledges that the authors themselves noted it as a limitation to be addressed in future work. The ground truth describes that this assumption restricts the scope and applicability of the paper's core claims, as all theoretical guarantees depend on knowing these fixed, finite bounds. The review fails to explain these substantive implications - it doesn't discuss how this limits the practical applicability of the results or restricts the analysis to a narrower class of problems. The reasoning is superficial, merely noting the authors' own acknowledgment without providing analytical insight into why this constitutes a meaningful limitation."
    }
  },
  {
    "openreview_id": "SiALFXa0NN",
    "reviewer_id": "CSRh",
    "evaluation": {
      "flaw_id": "relu_only_implementation",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the Weaknesses section: 'My main concern is the gap between the theoretical contributions and the implementation. As mentioned in lines 71-73, \"the implementation (N3V) supports NNs with Relu\" and \"theoretical contribution (VerSAILLE) reaches far beyond this\".'",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies the gap between theoretical scope and implementation capabilities, the reasoning is superficial and incomplete compared to the ground truth. The review merely asks 'What are the reasons for this gap and what are the difficulties in overcoming it?' without explaining why this is problematic. The ground truth describes this as a 'critical limitation that must be addressed for the paper's claims of wide applicability to hold' and notes that multiple reviewers flagged it as a significant issue. The generated review fails to articulate the negative implications of this limitation on the paper's credibility, scope validation, or practical utility. It treats the gap more as a curiosity than a fundamental flaw that undermines the paper's broader claims."
    }
  },
  {
    "openreview_id": "SiALFXa0NN",
    "reviewer_id": "JF4H",
    "evaluation": {
      "flaw_id": "relu_only_implementation",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in Question (1): 'However, the method is only applied experimentally to networks with ReLU activation functions, and most related work focuses solely on this single type of network. Have other types of networks been considered, and has the method been tested for effectiveness across different types NNs?'",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies that the implementation only works with ReLU networks, it fails to capture the critical reasoning from the ground truth. The review treats this as a minor curiosity or potential extension ('Have other types of networks been considered?'), rather than recognizing it as a fundamental gap between the theoretical claims and practical implementation. The ground truth emphasizes that this is a 'critical limitation' that undermines the paper's 'claims of wide applicability' and that multiple reviewers flagged this as a serious issue. The review's reasoning is superficial and doesn't understand the severity of the mismatch between the broad theoretical framework (VerSAILLE/Mosaic for piece-wise Noetherian activations) and the narrow practical tool (N3V/SNNT for ReLU only)."
    }
  },
  {
    "openreview_id": "Kx8I0rP7w2",
    "reviewer_id": "bM2e",
    "evaluation": {
      "flaw_id": "insufficient_experimental_method_description",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in multiple places in the Weaknesses section: 'The experimental setup in Section 4 seems somewhat overcomplicated. Is this a standard method for converting weighted networks to distance networks? If so, refer to works where this methodology is studied.' and 'Similarly, the setup of Section 5 is also quite complicated without providing much motivation or justification.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth. It correctly identifies that the conversion procedures lack proper explanation and references to standard practice ('Is this a standard method... If so, refer to works where this methodology is studied'). The reviewer also notes the lack of motivation and justification for the complicated constructions, which matches the ground truth's mention of 'lack of motivation for the complicated constructions.' Additionally, the reviewer identifies potential issues with induced dependencies ('this distance-conversion will introduce dependencies between the edge weights'), which aligns with the ground truth's reference to 'possible induced dependencies.' While the review doesn't explicitly mention reproducibility or transparency, it implicitly addresses these concerns by calling for proper references and methodological justification."
    }
  },
  {
    "openreview_id": "2RS0fL7Eet",
    "reviewer_id": "AcA7",
    "evaluation": {
      "flaw_id": "limited_experimental_comparison",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in the weaknesses section: 'The paper does not provide a comprehensive comparison with existing IVaR methods, particularly in terms of computational complexity and empirical performance on real-world datasets. Such a comparison would strengthen the paper's contributions.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. The reviewer correctly identifies that the lack of comprehensive comparisons with existing IVaR methods (which would include established IVaR/2SLS baselines mentioned in the ground truth) is problematic, and specifically notes the absence of empirical performance evaluation on real-world datasets and computational complexity comparisons. The reviewer also correctly explains why this is a flaw by stating that 'Such a comparison would strengthen the paper's contributions,' indicating they understand that without these comparisons, the paper's contributions are weakened and less convincing. This reasoning matches the ground truth's indication that this was identified as a 'major weakness' and 'critical issue' by multiple reviewers."
    }
  },
  {
    "openreview_id": "2RS0fL7Eet",
    "reviewer_id": "E97U",
    "evaluation": {
      "flaw_id": "limited_experimental_comparison",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in the weaknesses section: '**2. Further Experimental Validation**: Although this work is primarily theoretical, augmenting the experiment section with a more comprehensive set of experiments would be beneficial. It would be particularly valuable to see how the proposed algorithms perform on real-world datasets.' The review identifies that experiments are limited to synthetic data and suggests the need for real-world dataset evaluation.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies that experimental validation is insufficient and mentions the lack of real-world data evaluation, it fails to capture the full scope of the flaw as described in the ground truth. The ground truth specifically mentions the lack of 'direct comparisons with established IVaR/2SLS baselines' and 'complexity metrics' as major weaknesses identified by multiple reviewers. The generated review only makes a general suggestion for 'more comprehensive experiments' and 'real-world datasets' but doesn't specifically identify the critical missing baseline comparisons with established methods like IVaR/2SLS, nor does it recognize this as a major weakness that affects the paper's contribution assessment. The reasoning is superficial compared to the ground truth description."
    }
  },
  {
    "openreview_id": "jXsxGt80sv",
    "reviewer_id": "4rks",
    "evaluation": {
      "flaw_id": "computational_overhead_tradeoff",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions the flaw through these relevant statements: 'This paper utilizes multi-agent collaboration, but it seems that the paper does not explore the impact of different numbers of agents or agent pairs on the results' and the question 'What impact would removing some or adding additional agent pairs have on the results?' These directly relate to the scalability and computational cost concerns described in the ground truth.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies that the paper lacks analysis of different numbers of agent pairs and their impact, it fails to capture the deeper reasoning about why this is problematic. The ground truth emphasizes that this missing analysis undermines the paper's core claim of being a 'practical, scalable data-optimization framework' and that without concrete computational cost analysis, wall-clock time measurements, and performance-vs-cost trade-offs, the scalability claims remain insufficiently supported. The review treats this more as a curiosity about experimental variations rather than a fundamental flaw that questions the practical viability and scalability claims of the entire framework."
    }
  },
  {
    "openreview_id": "jXsxGt80sv",
    "reviewer_id": "b1DV",
    "evaluation": {
      "flaw_id": "computational_overhead_tradeoff",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions the flaw in the weaknesses section: 'What is the overhead of this proposed method, like wall-clock time?' and 'can this method be scalable to large scale dataset optimization, like web-crawl data?' These questions directly address computational overhead and scalability concerns identified in the ground truth.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies the missing elements (overhead analysis, wall-clock time, and scalability concerns), it fails to provide the deeper reasoning found in the ground truth. The review merely poses questions about these issues but doesn't explain why their absence is problematic. The ground truth emphasizes that without detailed computational cost analysis and scalability measurements, the paper's core claim of being a 'practical, scalable data-optimization framework remains insufficiently supported.' The review doesn't connect the missing overhead analysis to the broader implications for the framework's claimed practicality and scalability, nor does it explain how this omission undermines the paper's central contributions."
    }
  },
  {
    "openreview_id": "zaXuMqOAF4",
    "reviewer_id": "vMAr",
    "evaluation": {
      "flaw_id": "stair_pe_equivalence_misclaimed",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly identifies the equivalence issue in Concern 2, stating: 'The Equation of Star PE is almost the same as Self-Extend LLM' and then provides detailed mathematical analysis showing that 'when W%G==0, the Equation of Self-Extend LLM becomes... N+ ⌊(t-i-N)/E⌋, which is the same as Star PE.' The reviewer also notes that 'the paper may need to rethink the major contribution' and suggests the authors 'should clarify its original contribution.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer's reasoning aligns very well with the ground truth. They correctly identify that: (1) Star PE (referred to as 'Stair PE' in ground truth) is mathematically equivalent to Self-Extend PE from arXiv:2401.01325, (2) this equivalence undermines the claimed novelty/originality of the contribution, and (3) the paper needs to reconsider its major contributions and clarify what is actually original. The reviewer also identifies a formula/figure mismatch (Concern 1), which corresponds to the ground truth mention of 'correct the formula/figure mismatch.' While the reviewer uses 'Star PE' instead of 'Stair PE,' this appears to be the same method being discussed. The reasoning demonstrates understanding that mathematical equivalence to prior work means the contribution is not novel as claimed."
    }
  },
  {
    "openreview_id": "Ai76ATrb2y",
    "reviewer_id": "enWp",
    "evaluation": {
      "flaw_id": "missing_experimental_details_and_code",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in the Weaknesses section: 'My main critique of this paper is that, given the prominence that the hybrid LLP+Geom mechanism plays in the experimental results, the paper spends comparatively little time explaining how either the proposed metrics or the posterior probability for this mechanism are computed.' It is further elaborated in Question 1: 'While I have a guess for how these posterior probabilities were calculated... the paper is not clear how the posterior probabilities were calculated, particularly for LLP+Geom and LLP+Lap. Could the authors please elaborate on how this was done?'",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies that the paper lacks explanation of how posterior probabilities are computed for mechanisms like LLP-Geom/Lap, it falls short of the ground truth reasoning in a critical way. The ground truth emphasizes that this missing information makes 'the empirical auditing pipeline unreproducible' - a significant methodological flaw that undermines the scientific validity of the work. The review, however, frames this more as a clarity/explanation issue rather than a fundamental reproducibility problem. The reviewer asks for elaboration and mentions it as a critique about spending 'comparatively little time explaining,' but doesn't identify the core issue that without these computational details, other researchers cannot reproduce the empirical results, which is the essential problem highlighted in the ground truth."
    }
  },
  {
    "openreview_id": "Ai76ATrb2y",
    "reviewer_id": "Us4Y",
    "evaluation": {
      "flaw_id": "ambiguous_mathematical_results",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in multiple ways: (1) In the weaknesses section: 'The formalization is insufficiently rigorous and often unclear' and 'The claimed results don't seem to be always sound'; (2) Detailed analysis of specific theorems, particularly Theorem 3.2 where the reviewer states 'it is unclear what is the distribution of α', 'it is unclear what is the role of the threshold β', and identifies notation mismatches like the syntactic mismatch between $\\mathcal{D}^m$ and $(\\mathbf{x},\\mathbf{y})$ in line 154; (3) Points out undefined variables and ambiguous notation throughout the mathematical derivations.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning strongly aligns with the ground truth description. The reviewer correctly identifies that the mathematical results are unclear and ill-specified, providing specific examples of notation mismatches (e.g., the $\\mathcal{D}^m$ vs $(\\mathbf{x},\\mathbf{y})$ mismatch) and undefined variables (e.g., unclear distribution of α, undefined role of threshold β). The reviewer explains why these issues are problematic - they make the formalization 'insufficiently rigorous' and call the soundness of results into question. The detailed analysis of Theorem 3.2 demonstrates exactly the kind of ambiguities mentioned in the ground truth, including notation inconsistencies and unclear variable definitions. The reviewer also identifies issues in proofs (line 528 analysis) that make derivations hard to understand, which directly corresponds to the ground truth's mention of unclear derivations that compromise analytical rigor."
    }
  },
  {
    "openreview_id": "loMa99A4p8",
    "reviewer_id": "QQVN",
    "evaluation": {
      "flaw_id": "missing_elbo_intuition",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions this flaw in Weakness #1: 'In section 3.5, the authors discussed why the generalization makes the ELBO rely on the entire trajectory; however, they state the fact without giving any intuitive explanations. The authors should discuss this issue more. In addition, providing some toy examples to clearly show how the extended framework is differentiated from the existing one could significantly improve the paper's presentation.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. Both identify that the paper lacks intuitive explanations for why the ELBO becomes trajectory-dependent under the proposed framework. The review correctly notes that the authors 'state the fact without giving any intuitive explanations' and suggests concrete improvements including 'providing some toy examples' to better illustrate the framework. This matches the ground truth's mention of Reviewer QQVN's request for 'illustrative discussion or toy examples.' The review appropriately frames this as a presentation issue that should be addressed to improve the paper's clarity and completeness."
    }
  },
  {
    "openreview_id": "SRWs2wxNs7",
    "reviewer_id": "JEXU",
    "evaluation": {
      "flaw_id": "missing_baselines",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions missing baselines in weakness (1): 'This paper needs to include important baselines. It fails to compare with other efficient DiT models, such as Pixart-sigma or others.' The reviewer also mentions PixArt-sigma specifically, which is one of the baselines mentioned in the ground truth (PixArt-Sigma). Additionally, in the Questions section, the reviewer asks about computational metrics: 'How do you get the GFLOPs as illustrated in the paper? How to estimate the GPU hours according to such GFLOPs?' which relates to the missing resource statistics mentioned in the ground truth.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. The reviewer correctly identifies that the missing baselines (specifically mentioning PixArt-sigma, which matches PixArt-Sigma from the ground truth) make comparisons unfair, stating 'SiT seems to be the only baseline method that takes flow as the objective. However, it does not employ a different architecture other than DiT, which should not be a very fair baseline to compare with U-DiTs.' This directly addresses the fairness of comparison issue highlighted in the ground truth. The reviewer also questions the computational metrics (GFLOPs, GPU hours), which corresponds to the ground truth's mention of missing 'tables of FLOPs, GPU-hours, or parameter counts needed for a fair comparison.' The reasoning demonstrates understanding that these missing elements undermine the paper's claims about efficiency and performance, which matches the ground truth's concern that 'the paper's core claims about efficiency and performance remain insufficiently substantiated.'"
    }
  },
  {
    "openreview_id": "lWHe7pmk7C",
    "reviewer_id": "h6sN",
    "evaluation": {
      "flaw_id": "missing_few_shot_rawnerf_comparison",
      "is_flaw_mentioned": false,
      "mention_reasoning": "The review does not mention the absence of RawNeRF results under few-shot scenarios. While the review mentions some performance comparison issues in weakness 2 ('The performance improvement over baseline with rawnerf loss is marginal and even no synthesis quality improvement compared to the reported scores in the original RawNeRF paper'), this refers to general performance comparisons rather than the specific missing few-shot RawNeRF comparison that is described in the ground truth flaw.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "Since the flaw was not mentioned, there is no reasoning to evaluate. The ground truth describes a critical missing comparison between the proposed method and RawNeRF under the same few-shot conditions, which is essential for validating the method's advantages. The review completely misses this important methodological gap and its implications for the paper's claims about few-shot performance."
    }
  },
  {
    "openreview_id": "SoYCqMiVIh",
    "reviewer_id": "5r2K",
    "evaluation": {
      "flaw_id": "lack_quantitative_validation_pixel_level",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions this flaw in the first weakness: 'On the Pixel-level disease experiments, it's hard to judge how realistic the simulated diseases progressions are without quantitative comparison with baseline/groundtruth, such as co-occurances of events. Since it's a scientific study on method compared with baseline, the claims become unfalsifiable if such setup is provided.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning closely aligns with the ground truth description. Both emphasize that: (1) the pixel-level disease progression results lack quantitative validation against ground truth or baselines, (2) this absence makes it difficult to assess the realism/accuracy of the results, and (3) most importantly, this makes the core claims 'unfalsifiable' - which directly matches the ground truth's assertion that this flaw makes claims unfalsifiable. While the review doesn't mention specific metrics like Intersection-over-Union as noted in the ground truth, it correctly identifies the fundamental methodological problem and its impact on the scientific validity of the work."
    }
  },
  {
    "openreview_id": "HAcaANQNMK",
    "reviewer_id": "yYTB",
    "evaluation": {
      "flaw_id": "weak_mmlu_llama2_results",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions the need for MMLU benchmarks specifically: 'For generative models such as GPT-3 and Llama2 discussed in this paper, there is a need for more comprehensive performance validation, including metrics like mmlu and math benchmarks.' This directly relates to the ground truth flaw about weak MMLU results on Llama-2.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies that MMLU benchmarks are needed for Llama2, it frames this as a general need for more comprehensive evaluation rather than identifying the specific problem described in the ground truth. The ground truth flaw is about ESPACE causing a 'significant performance drop on Llama-2 for the MMLU benchmark' - a concrete negative result that requires stronger empirical validation. The review's reasoning treats this as a missing evaluation rather than recognizing that there are poor existing results that need improvement. The review doesn't capture the severity of the performance drop or the need for remediation that the ground truth describes."
    }
  },
  {
    "openreview_id": "8ohsbxw7q8",
    "reviewer_id": "4R3n",
    "evaluation": {
      "flaw_id": "lack_bias_variance_analysis",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the 'Weaknesses' section under 'Lack of bias-variance analysis': 'Authors acknowledge that eager gradients is a biased version of the standard policy gradients but don't provider either theoretical or empirical analysis of the bias-variance tradeoff between the two methods.' The reviewer also asks in the 'Questions' section: 'My main question is what is the nature of the bias the GDPO introduces?' and states in 'Limitations': 'I believe the paper needs a clearer discussion of the bias introduced by policy gradients.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. The reviewer correctly identifies that: 1) The eager policy gradient is a biased estimator, 2) There is no theoretical or empirical analysis of the bias-variance properties, 3) This lack of analysis undermines the paper's claims about GDPO being a principled improvement. The reviewer specifically notes that without understanding 'the nature of the bias' and the 'bias-variance tradeoff,' the connection to the underlying MDP framework becomes unclear. This matches the ground truth concern that without proper statistical analysis, the central claim that 'GDPO is a principled and generally applicable improvement over DDPO—remains insufficiently supported.' The reviewer goes beyond just noting the omission and explains why this is problematic for the paper's theoretical foundations."
    }
  },
  {
    "openreview_id": "8ohsbxw7q8",
    "reviewer_id": "gTVc",
    "evaluation": {
      "flaw_id": "lack_bias_variance_analysis",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the weaknesses section: 'Limited theoretical analysis: While the eager policy gradient is empirically effective, the paper lacks a rigorous theoretical treatment of its properties, particularly regarding the bias-variance trade-off.' Additionally, in the strengths section, the reviewer notes 'Despite the lack of theory supporting it' when discussing the eager policy gradient method.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies that there is a lack of theoretical analysis of the eager policy gradient's bias-variance properties, the reasoning is superficial compared to the ground truth. The review merely states that theoretical treatment is missing but fails to explain the critical implications this has for the paper's central claims. The ground truth emphasizes that without this analysis, 'the paper's central claim—that GDPO is a principled and generally applicable improvement over DDPO—remains insufficiently supported.' The review does not connect the missing theoretical justification to the fundamental credibility of the method's claims of being a 'principled' improvement, nor does it explain how the lack of understanding of statistical properties (bias, variance, convergence behavior) undermines the scientific rigor of the contribution. The review treats this more as a minor limitation rather than a fundamental flaw that questions the paper's core contribution."
    }
  },
  {
    "openreview_id": "ez7w0Ss4g9",
    "reviewer_id": "5fn3",
    "evaluation": {
      "flaw_id": "limited_experimental_scope",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions the limited experimental scope in multiple places: 'The paper would benefit from experiments on more diverse datasets, including real-world data', 'The numerical experiments are limited in scope. They primarily focus on linear generative models, which may not fully capture the complexities of real-world data. Expanding the experiments to include more diverse datasets and model architectures would strengthen the findings', and 'How well do the theoretical results generalize to non-linear models and more complex data distributions?'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. The reviewer correctly identifies that the experimental validation is too narrow, specifically calling out the limitation to linear generative models and the need for more realistic/non-linear settings and additional datasets. The review explains why this is problematic: linear models 'may not fully capture the complexities of real-world data' and don't represent 'the behavior of JEPA and MAE in practical settings.' This matches the ground truth's indication that the narrow empirical validation was deemed insufficient by multiple reviewers who requested demonstrations on more realistic/non-linear settings and additional datasets. The reviewer also correctly identifies that expanding experiments would 'strengthen the findings' and 'demonstrate the practical implications,' which aligns with the ground truth's note that additional analysis was deemed 'essential for the paper's publishability.'"
    }
  },
  {
    "openreview_id": "ez7w0Ss4g9",
    "reviewer_id": "Nsp4",
    "evaluation": {
      "flaw_id": "limited_experimental_scope",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions limited experimental scope in the Quality weakness: 'this is a theory paper with very limited experimental support (beyond the numerical evaluation of the theory)' and requests 'a clear distillation of an experiment designed to highlight the difference between JEPA and MAE.' The reviewer also asks in Questions section about demonstrating effects on real-world systems and providing qualitative predictions, and in Limitations states 'It would be great if the authors provided experimental support for the applicability of their theory to real-world systems.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth. The reviewer correctly identifies that the experimental validation is too narrow and requests more comprehensive demonstrations. They specifically ask for experiments on real-world systems implementing JEPA and MAE, qualitative predictions from theory to practice, and better correspondence between theoretical and experimental results. This matches the ground truth's description of reviewers requesting 'demonstrations on more realistic/non-linear settings and additional datasets.' The reviewer also correctly identifies the significance implications - that without broader experimental scope, the practical implications for practitioners and algorithm development remain unclear."
    }
  },
  {
    "openreview_id": "CTIFk7b9jU",
    "reviewer_id": "Tej6",
    "evaluation": {
      "flaw_id": "missing_statistical_significance",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the Weaknesses section: 'No statistical significance analysis is provided.' and reinforces it in the Questions section: 'Are the improvements significant?'",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies that statistical significance analysis is missing, it provides only a superficial statement without explaining why this is problematic. The ground truth indicates this flaw undermines the paper's main performance claims and that the lack of paired-sample significance tests (like t-tests) for quantitative gains was considered essential by reviewers and the AC. The generated review fails to explain the negative implications of this omission - that without statistical testing, readers cannot assess whether the reported performance improvements are statistically meaningful or could have occurred by chance. The review simply states the absence but doesn't articulate why this threatens the validity of the paper's claims or impacts reproducibility and scientific rigor."
    }
  },
  {
    "openreview_id": "CTIFk7b9jU",
    "reviewer_id": "ssxr",
    "evaluation": {
      "flaw_id": "missing_physiological_plausibility_eval",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the Weaknesses section: 'However, they did not evaluate the physics/physiological plausibility. For the myocardium, which is considered an incompressible material, the determinant of the Jacobian |J| should be around 1. I suggest the authors add some evaluation comparing the physiological plausibility.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies why this is a flaw by explaining the physiological basis: myocardium is incompressible material and therefore the Jacobian determinant should be around 1. While the review doesn't explicitly use the term 'clinical trustworthiness' as in the ground truth, it correctly captures the essence of the problem - that biomechanical plausibility assessment is missing and necessary for proper evaluation of cardiac motion tracking. The reviewer also provides a relevant reference and makes a concrete suggestion for improvement, demonstrating understanding of why this omission is problematic."
    }
  },
  {
    "openreview_id": "ZRYFftR4xn",
    "reviewer_id": "f4nV",
    "evaluation": {
      "flaw_id": "insufficient_justification_strict_convexity",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in weakness w1: 'My main concern is that the assumption of strict convexity is not very natural while providing the hardness of learnability in a non-strict convexity situation is good. It would be more better to show the validity of the assumption by showing that some known application of cooperative games satisfies it. For example, it is better to provide a discussion that in some known convex games, such as induced subgraph games with positive weights[1], airport games[2], and some others[3-6], which parameters of these convex games are related to the parameter \\varsigma.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns very well with the ground truth description. The reviewer correctly identifies that: (1) the strict convexity assumption lacks adequate justification, (2) the paper should provide concrete examples of existing game classes that satisfy this assumption, and (3) specifically mentions the same types of games referenced in the ground truth (induced subgraph games, airport games). The reviewer goes beyond just noting the omission by explaining why this is problematic - the assumption is 'not very natural' and needs validation through known applications. This demonstrates understanding of the negative implications for the paper's practical relevance and theoretical foundation, which matches the ground truth's emphasis on the gap that 'must be filled for publication.'"
    }
  },
  {
    "openreview_id": "Ke40kfOT2E",
    "reviewer_id": "6CM1",
    "evaluation": {
      "flaw_id": "no_sampling_capability",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions the sampling limitation in the Questions section: 'In the conclusion, it is remarked that differentiable sampling from PICs is not possible. What is meant by differentiable here, and is it possible to sample from the materialized PC if one is not interested in gradients (e.g. for visualization)?'",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review does mention the sampling issue, it fails to provide the correct reasoning about why this is a significant flaw. The reviewer poses it as a clarifying question rather than identifying it as a major limitation that undermines the model's utility as a generative method. The ground truth emphasizes that this limitation 'limits the practical usefulness of the model as a generative method' and that it's a gap that needs addressing. The review doesn't recognize this as a fundamental problem with the approach - instead, it asks technical questions about differentiability and whether sampling might be possible under certain conditions, missing the core issue that PICs, despite being positioned as continuous-latent generative models, fundamentally lack sampling capability."
    }
  },
  {
    "openreview_id": "Ke40kfOT2E",
    "reviewer_id": "iaCH",
    "evaluation": {
      "flaw_id": "no_sampling_capability",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in two places: (1) In the Weaknesses section: 'Not a major weakness, but as the authors also point out, it is currently not possible to sample from PICs, limiting their impact as a generative model.' (2) In the Limitations section: 'The authors acknowledge the limitation of PICs that, despite being a continuous latent variable model like many generative models, they do not support sampling. This is left as future work.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies why the lack of sampling capability is a flaw. It specifically notes that this limitation affects PICs' 'impact as a generative model' and acknowledges the contradiction between PICs being positioned as 'continuous latent variable model like many generative models' while not supporting sampling. This aligns well with the ground truth description which states that the gap 'limits the practical usefulness of the model as a generative method.' The review correctly understands that sampling is a fundamental expectation for generative models, and its absence undermines the practical utility of PICs in this domain. However, the review characterizes it as 'not a major weakness,' which somewhat understates the significance compared to the ground truth calling it a 'major limitation.'"
    }
  },
  {
    "openreview_id": "AbZyNGWfpN",
    "reviewer_id": "CYCL",
    "evaluation": {
      "flaw_id": "missing_comparative_evaluation",
      "is_flaw_mentioned": false,
      "mention_reasoning": "The review does not mention the absence of key recent PEFT and sparse-tuning baselines such as GPS, MOSA, VQT, and DoRA. While the review acknowledges that 'SNELL achieves state-of-the-art performance on various benchmarks' and discusses experimental results, it does not critique the experimental section for missing important baseline comparisons or question whether the performance claims are adequately substantiated.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "Since the flaw was not mentioned in the review, there is no reasoning to evaluate. The ground truth describes a critical experimental flaw where multiple reviewers identified missing key baselines that left core performance and memory-efficiency claims unsubstantiated. The generated review fails to identify this gap entirely, instead accepting the experimental results at face value without questioning the completeness of the comparative evaluation. The review does not address the fundamental issue that without proper baseline comparisons, the claimed superiority of SNELL cannot be properly validated."
    }
  },
  {
    "openreview_id": "AbZyNGWfpN",
    "reviewer_id": "QyQ5",
    "evaluation": {
      "flaw_id": "missing_comparative_evaluation",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in Weakness #2: 'As a work on sparse tuning, it's really a pity that this paper does not compare with the latest pure sparse tuning work, GPS[1], nor is it mentioned in the related work section. This severely limits the contribution of this work. In fact, comparing with the state-of-the-art sparse tuning work is essential.' The review specifically identifies GPS as a missing baseline and notes the absence of other recent work.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. The reviewer correctly identifies that: (1) key recent baselines like GPS are missing from comparisons, (2) this omission 'severely limits the contribution of this work,' and (3) 'comparing with the state-of-the-art sparse tuning work is essential.' This matches the ground truth's emphasis that the missing comparisons leave 'core performance and memory-efficiency claims unsubstantiated.' The reviewer also notes that some methods are missing from the related work section, further supporting the comprehensiveness concern. While the review doesn't mention all the specific baselines listed in the ground truth (MOSA, VQT, DoRA), it captures the core issue and its negative impact on the paper's validity."
    }
  },
  {
    "openreview_id": "jImXgQEmX3",
    "reviewer_id": "QRnu",
    "evaluation": {
      "flaw_id": "missing_uncertainty_measures",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the Weaknesses section: 'All experiments do not report any measure of uncertainty. Thus, it is impossible to determine if the conclusions are statistically significant. Including measures of uncertainty would strengthen the validity of the results.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. Both identify that the absence of uncertainty measures (error bars, confidence intervals, statistical significance tests) prevents proper assessment of result reliability. The review correctly explains the negative impact: 'it is impossible to determine if the conclusions are statistically significant' and that this affects 'the validity of the results.' This matches the ground truth's emphasis that the flaw 'prevents judging whether the observed improvements are reliable' and was considered 'a major weakness' and 'critical limitation.' The review demonstrates understanding of why this omission is problematic rather than just noting what's missing."
    }
  },
  {
    "openreview_id": "jImXgQEmX3",
    "reviewer_id": "CkwG",
    "evaluation": {
      "flaw_id": "insufficient_methodological_detail",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions several aspects of insufficient methodological detail: 'equations 1, 3 start talking about a policy which was not described earlier', 'The 'processed based feedback' was not clear to me', 'The experimental setup could have been made clearer, including the various settings such as with/without finetuning and process/outcome feedback', and 'several notations were not clearly defined'.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. It identifies the same core issues: unclear KTO objective/policy definition (mentioning equations talking about policy not described earlier), insufficient details about process-based feedback mechanism (stating 'processed based feedback was not clear'), and unclear experimental setup (requesting clarification of various settings). While the review doesn't explicitly use the word 'reproducibility', it implies this concern by stating that notations and methods were not clearly defined, which would hinder others' ability to replicate the work. The reasoning demonstrates understanding of why these omissions are problematic for scientific rigor and evaluation."
    }
  },
  {
    "openreview_id": "M20p6tq9Hq",
    "reviewer_id": "gAcJ",
    "evaluation": {
      "flaw_id": "limited_identifiability_scope",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in multiple places: (1) In the strengths section: 'the identifiability notion up to upstream layers is an interesting concept (needs to be better discussed what it means and how it compares to other notions)', (2) In the major weaknesses: 'I also encourage the authors to address the (limitations of) identifiability up to upstream layers; though, as I wrote above, find the concept interesting, my understanding is that this is a very limited identifiability notion', and (3) In the questions section asking about how this relates to other identifiability concepts.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer's reasoning aligns well with the ground truth description. The reviewer correctly identifies that 'identifiability up to upstream layers' represents 'a very limited identifiability notion,' which matches the ground truth's characterization of it as 'a very limited form of identifiability' and 'a major, acknowledged limitation of the study's scope.' The reviewer also appropriately calls for the authors to better discuss what this limitation means and how it compares to other notions of identifiability, suggesting they understand this is a significant constraint on the method's capabilities. While the reviewer doesn't explicitly mention that variables within the same layer cannot be disentangled, they clearly recognize the fundamental limitation in scope that this identifiability restriction represents."
    }
  },
  {
    "openreview_id": "nfK0ZXFFSn",
    "reviewer_id": "tABH",
    "evaluation": {
      "flaw_id": "prompt_independence_assumption",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the Weaknesses section: 'Currently, the way problem is formulated (Equations 1, 2, 3, 4), it appears that the fact that an LLM generation is a hallucination is independent of the corresponding user prompt. This does not seem to be a good assumption to make.' The reviewer also reinforces this concern in the Questions section: 'By assuming the specific mixture of two distributions. The authors are assuming that hallucinated data is generated with probability π independent of the input.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies why this is a flaw - the reviewer states that assuming hallucination probability is independent of the user prompt 'does not seem to be a good assumption to make,' which aligns with the ground truth description that this is an 'unrealistic, major weakness.' The reviewer demonstrates understanding of the methodological issue by specifically referencing the problematic equations (1-4) and the independence assumption with probability π. The reviewer also appropriately suggests that the authors should 'describe their modeling choice (Eq 4) a little more in detail, specially how it relates to the user-prompt conditioning,' showing they understand the need for prompt-conditioned formulations as mentioned in the ground truth."
    }
  },
  {
    "openreview_id": "bE7GWLQzkM",
    "reviewer_id": "LdFf",
    "evaluation": {
      "flaw_id": "unclear_similarity_definition",
      "is_flaw_mentioned": false,
      "mention_reasoning": "The review does not mention any issues with the definition or consistency of feature similarity (ρ_a) and readout similarity (ρ_b) across different sections of the paper. While the review discusses these concepts in the context of their impact on continual learning, it does not flag any definitional inconsistencies, confusion, or clarity issues with how these key notions are presented in the main text, appendix, and experiments.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "Since the flaw was not mentioned in the review, there is no reasoning to evaluate. The ground truth describes a significant issue where key concepts are defined inconsistently across different parts of the paper, causing confusion that required clarification in the rebuttal and substantive revisions. The generated review completely missed this fundamental clarity and consistency problem, instead treating the similarity definitions as unproblematic and focusing on other aspects like generalizability to more tasks."
    }
  },
  {
    "openreview_id": "bE7GWLQzkM",
    "reviewer_id": "Uj69",
    "evaluation": {
      "flaw_id": "unclear_similarity_definition",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions the flaw in the Weaknesses section: 'I'm confused on the paper's definition of 'similarity' in general, for both feature similarity $\\rho_a$ and readout similarity $\\rho_b$.' and elaborates in Question 1: 'Could the authors please explain what they mean by 'element-wise correlation' for $\\rho_a$ and $\\rho_b$? When reading the task sampling procedure in appendix A.1, $\\rho_a$ and $\\rho_b$ are the probability of entries in the mixing matrices being identical; when deriving the analytical solutions, they seem to mean column-wise correlation?'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer correctly identifies why this is a problematic flaw. They explain that the definitions are inconsistent across different parts of the paper ('different definitions of key concepts in one paper'), specifically noting discrepancies between the appendix (probability of identical entries) and the analytical derivations (column-wise correlation). The reviewer also correctly identifies that this leads to confusion and affects the paper's clarity. They further recognize the broader implications by asking how different similarity definitions affect conclusions and noting that a third definition appears to be used for permuted MNIST. This reasoning aligns well with the ground truth description of inconsistent definitions across main text, appendix, and experiments leading to confusion."
    }
  },
  {
    "openreview_id": "MSsQDWUWpd",
    "reviewer_id": "kcVu",
    "evaluation": {
      "flaw_id": "missing_proofs_and_unclear_structure",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions both components of the flaw. For missing proofs: 'In Section 4, two Theorems are provided without proofs. Proofs are also not provided in the Appendix.' For unclear structure: 'The structure of the paper is confusing' and 'Theoretical statements and their implications are hard to understand. Clearly stating all symbols and providing additional details would help.'",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies that proofs are missing and the structure is confusing, it fails to explain why these are critical flaws that impact the paper's publishability. The review mentions these issues as weaknesses but doesn't articulate the deeper implications - that missing proofs make the theoretical claims unverifiable and that unclear structure/notation makes the work inaccessible to readers. The ground truth emphasizes these are 'critical changes' needed for the paper to be 'publishable,' but the review treats them as standard weaknesses rather than fundamental barriers to acceptance. The reasoning is superficial compared to the ground truth's emphasis on the severity of these omissions."
    }
  },
  {
    "openreview_id": "X64IJvdftR",
    "reviewer_id": "qqXj",
    "evaluation": {
      "flaw_id": "dependency_in_proof",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions this flaw in weakness point 3: 'Line 190 says that the bounding subset is selected during the SRS sampling, this means the hypothesis test here is not independent of the SRS; thus in the proof, the multiplication of events could fail. The authors should rethink and potentially fix their proof accordingly.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning closely aligns with the ground truth description. Both identify that: (1) the bounding subset selection during SRS sampling creates a dependency issue, (2) this breaks the independence assumption needed for the hypothesis tests, (3) this invalidates the multiplication of event probabilities in the proof, and (4) the proof needs to be fixed. The review demonstrates correct understanding of why this is a fundamental theoretical problem rather than just a minor technical issue. The reasoning captures the core logical chain: selection during sampling → loss of independence → invalid probability multiplication → flawed proof."
    }
  },
  {
    "openreview_id": "hW5QWiCctl",
    "reviewer_id": "4DfY",
    "evaluation": {
      "flaw_id": "missing_statistical_significance_runtime",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions both components of the flaw: (1) 'For the performance mentioned in the tables, the authors should provide standard deviation to understand whether their method is statistically significant or not. The authors should conduct t-test [1] to confirm this.' (2) 'The authors should provide the computational complexity of their method with respect to the baselines. The complexity should be about the training and inference time of their method.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer's reasoning aligns well with the ground truth description. For statistical significance, the reviewer correctly identifies that without standard deviations and statistical tests (t-test), it's impossible to determine if the reported improvements are statistically meaningful - this matches the ground truth concern about lacking 'measures of statistical variability' and not establishing 'significance via tests such as a t-test.' For computational complexity, the reviewer correctly asks for 'training and inference time' comparisons with baselines, which directly corresponds to the ground truth mention of 'explicit training and inference-time complexity comparisons with baselines.' The reviewer also provides intuitive reasoning for why this matters (suspecting the algorithm might be slow due to edge iteration), showing understanding of the practical implications."
    }
  },
  {
    "openreview_id": "hW5QWiCctl",
    "reviewer_id": "Ysem",
    "evaluation": {
      "flaw_id": "overclaimed_clinical_impact_generalizability",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw multiple times. Key quotes include: 'These claims are significantly over-claimed. The examples shown here are very much toy examples on small image patches' and 'The datasets are pretty small and have been extensively tested in the past. There is an over-fitting concern' and 'all examples are primarily 2D patches. From the qualitative examples shown in the paper, they looks more like \"toy examples\". Does the methods work on full 3D CT scans' and 'The numerical improvements reported in the paper, overall are marginal' and 'The results here are far from clinically useful.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns very well with the ground truth description. The reviewer correctly identifies that: 1) The experiments are confined to small, largely 2D datasets (mentions 'small datasets', '2D patches', 'toy examples'), 2) There are concerns about overfitting ('over-fitting concern on these small and widely used datasets'), 3) The gains are marginal ('numerical improvements reported in the paper, overall are marginal'), 4) There are serious concerns about real-world applicability ('Does the methods work on full 3D CT scans, like extracting lung airway trees?'), and 5) The clinical impact claims are overstated ('These claims are significantly over-claimed', 'The results here are far from clinically useful'). The reviewer provides specific examples of why the claims don't match the evidence (2D patches vs. real 3D CT scans) and explains the negative implications for real-world medical applications."
    }
  },
  {
    "openreview_id": "Kc37srXvan",
    "reviewer_id": "Daqu",
    "evaluation": {
      "flaw_id": "missing_pointnext_and_scratch_baselines",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions both components of the flaw: (1) Missing PointNext baseline in Weakness #3: 'In Table 4, an important baseline, PointNext, is omitted. According to the PointNext paper, PointNext exhibits significantly better performance than the proposed PointMamba (87% without pre-training vs. 84.4% with pre-training).' (2) Missing from-scratch training results in Weakness #2: 'The authors only demonstrate the performance of **pre-trained** PointMamba... This implies that PointMamba cannot surpass previous methods when trained from scratch. To effectively showcase the solid merits of PointMamba, it would be more reasonable to provide comparisons without pre-training.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth. For the missing PointNext baseline, the reviewer correctly identifies that this omission 'raises doubts about the effectiveness of PointMamba' and undermines the claim of providing a 'solid Mamba-based baseline' by noting that 'comprehensive comparisons with previous methods should not be omitted.' For the missing from-scratch results, the reviewer correctly explains that showing only pre-trained results is insufficient to demonstrate the method's true merits, stating it would be 'more reasonable to provide comparisons without pre-training.' Both critiques directly support the ground truth's assertion that these gaps 'undermine the claim that PointMamba is a solid baseline.'"
    }
  },
  {
    "openreview_id": "zJNSbgl4UA",
    "reviewer_id": "7ef6",
    "evaluation": {
      "flaw_id": "limited_baselines_related_work",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions this flaw in Weakness 1: 'My major concern is that, the same aim of adapting ViTs to dynamically changing resource constraints, can also be achieved by multi-exit networks, e.g., [*1, *2, *3]. However, the paper does not discuss these highly relevant works or compare with them. Hence, I vote for rejection.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. The reviewer correctly identifies that the paper lacks discussion and experimental comparison against highly relevant prior work that provides flexible inference (multi-exit networks). The reviewer goes beyond just noting the omission by: (1) specifically identifying what type of relevant work was missed (multi-exit networks that achieve the same goal), (2) providing concrete examples with citations, (3) explaining why this is problematic (these works achieve the same aim of adapting to dynamic resource constraints), and (4) indicating this is a major concern that affects their recommendation. This demonstrates understanding of why insufficient baseline comparison is a significant flaw in academic work."
    }
  },
  {
    "openreview_id": "zJNSbgl4UA",
    "reviewer_id": "rLHb",
    "evaluation": {
      "flaw_id": "limited_baselines_related_work",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the 'Limited baselines and related work' section under weaknesses, stating: 'some relevant related works dealing with tranformer networks are either just briefly mentioned, e.g., Matformer [18], or not mentioned at all, e.g., SortedNet [a], Early exit [b]' and 'Matformer and SortedNet could be included in the experimental evaluation'. The reviewer also asks in the questions section to 'Add a discussion of differences and when possible experimental comparison with Matformer and SortedNet baselines'.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies why this is a problematic flaw. It explains that relevant prior work dealing with flexible inference in transformers (Matformer, SortedNet, Early exit) are either inadequately discussed or completely missing from the experimental comparison. The reviewer specifically notes that 'Matformer and SortedNet could be included in the experimental evaluation' and points out that some works are 'just briefly mentioned' while others are 'not mentioned at all'. This aligns well with the ground truth description that indicates reviewers stated the manuscript 'lacks sufficient discussion and experimental comparison against the most relevant prior work that also provides flexible inference'. The reviewer also correctly identifies that this impacts the experimental rigor by noting that US-Net was originally designed for CNNs and may not be the most appropriate baseline for ViTs, while more relevant transformer-based baselines were overlooked."
    }
  },
  {
    "openreview_id": "KEe4IUp20I",
    "reviewer_id": "HARv",
    "evaluation": {
      "flaw_id": "batching_efficiency",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions the batching efficiency issue in Weakness #1: 'While normalizing for FLOPs, I think there is some potential that the proposal will make the transformer blocks significantly harder to batch (due to the input-dependent dynamic structure of the model), hence the clock-time of the inference might actually be slower with the same FLOPs.' The reviewer also reinforces this concern in the Limitations section, stating they are 'relatively confident that this could be a potential limitation that needs to be addressed.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer's reasoning correctly identifies why this is a practical limitation. They specifically mention that the 'input-dependent dynamic structure of the model' makes batching harder, which directly aligns with the ground truth description of 'input-dependent, dynamically inserted global blocks make it hard to batch sequences efficiently.' The reviewer also correctly explains the negative implication - that despite having the same FLOPs, the actual clock-time inference might be slower due to batching difficulties. This demonstrates a solid understanding of the practical consequences of the architectural design choice, matching the ground truth's characterization of this as a 'practical limitation.'"
    }
  },
  {
    "openreview_id": "xqc8yyhScL",
    "reviewer_id": "mgYE",
    "evaluation": {
      "flaw_id": "overstated_novelty_and_title_claims",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions both aspects of the flaw. For overstated novelty: 'Novelty - this is mostly an evaluation paper, that does not introduce any new approach or technique.' For problematic title claims: 'the title of the paper is \"Is Programming by Example solved by LLMs?\". This title leads the paper... but I think that it's an undefined question. What does \"solve\" mean? By construction, and according to the \"no free lunch\" theorem, PBE can never be \"solved\".'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review provides detailed and correct reasoning for both aspects of the flaw. For the novelty issue, it correctly identifies that the paper is 'mostly an evaluation paper' that doesn't introduce new techniques, and notes that the empirical findings were 'as expected' and 'nothing... was surprising.' For the title problem, it provides sophisticated reasoning about why claiming PBE is 'solved' is scientifically problematic, invoking the 'no free lunch' theorem and questioning what 'solve' even means in this context. The reviewer also notes the arbitrariness of accuracy thresholds for defining 'solved.' This reasoning aligns well with the ground truth description that the title 'wrongly implies PBE is solved' and that novelty claims are overstated."
    }
  },
  {
    "openreview_id": "O5XbOoi0x3",
    "reviewer_id": "JxBk",
    "evaluation": {
      "flaw_id": "missing_diversity_evaluation",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions the missing diversity analysis in weakness W2: 'Regarding the evaluation, diversity comparisons are missing from the current paper. Although FID is not the best metric, the authors should still report it for a comprehensive understanding. Additionally, assessing per-prompt diversity is important. The authors are encouraged to generate a set of images with the same prompt and measure their variations as a proxy metric.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth. The reviewer correctly identifies that diversity analysis is missing and explains why this is problematic for comprehensive evaluation. They specifically mention both FID (which matches the ground truth mention of FID) and per-prompt diversity (which aligns with the ground truth reference to per-prompt diversity). The reviewer goes beyond just noting the omission by explaining that diversity metrics are needed 'for a comprehensive understanding' and expressing particular interest in 'the influence of reward optimization on generation diversity,' showing they understand the importance of this analysis for evaluating the method's impact on generation diversity."
    }
  },
  {
    "openreview_id": "LmjLRHVCMG",
    "reviewer_id": "4Ck3",
    "evaluation": {
      "flaw_id": "missing_assumption_clarity",
      "is_flaw_mentioned": false,
      "mention_reasoning": "The review does not mention any issues with missing assumptions, particularly Assumption C.2 regarding the strong-convexity/PL link that underlies Theorem 5.4. While the review mentions in weakness #1 that 'some of the descriptions are not clear enough and difficult to understand,' this is a vague, general statement that does not specifically address the missing assumption or its connection to convergence guarantees. The review does not reference Theorem 5.4, Assumption C.2, or any issues with theoretical assumptions being omitted from the main text.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "Since the flaw was not mentioned, there is no reasoning to evaluate. The ground truth describes a specific technical issue where a critical assumption (C.2) about the strong-convexity/PL link was omitted from the main text, making Theorem 5.4's convergence guarantee hard to interpret. The review completely fails to identify this omission and provides no reasoning about why missing assumptions would impact the interpretability of theoretical results. The reviewer's vague comment about unclear descriptions does not demonstrate understanding of the specific theoretical gap or its implications for the convergence analysis."
    }
  },
  {
    "openreview_id": "LmjLRHVCMG",
    "reviewer_id": "Khnn",
    "evaluation": {
      "flaw_id": "insufficient_train_from_scratch_cnn_evidence",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions this flaw in the 'Optimizer scalability & experimental scope' section, stating: 'However, the current experiments focus heavily on fine-tuning settings and I wonder if the findings hold for other settings, too. I would be convinced further about the findings if the authors provided an additional experiment for 'traditional' image classification (say CIFAR100 or CIFAR10) with a ResNet (say ~10M parameters).'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer's reasoning closely aligns with the ground truth description. The reviewer correctly identifies that the experimental scope is too narrow, focusing 'heavily on fine-tuning settings,' and argues that this is inadequate to support broader claims about the method's effectiveness. The reviewer specifically requests 'an additional experiment for traditional image classification (say CIFAR100 or CIFAR10) with a ResNet' to be more convinced about the findings' generalizability. This matches the ground truth's description of Reviewer Khnn judging the original experimental scope as 'inadequate to support the paper's claims for more realistic, memory-heavy settings' and requesting train-from-scratch experiments with ResNet architectures. The reviewer also provides constructive suggestions for implementation approaches, demonstrating understanding of the technical feasibility."
    }
  },
  {
    "openreview_id": "cg1vwt5Xou",
    "reviewer_id": "g6ab",
    "evaluation": {
      "flaw_id": "nontight_random_iid_bounds",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions this flaw in the weaknesses section: 'The analyses for random order and IID cases are not ideal. It seems neither upper nor lower bounds are tight for general γ.'",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies that the bounds for random order and IID cases are not tight, the reasoning is quite superficial compared to the ground truth. The ground truth provides much deeper context: (1) the algorithms are neither optimal nor efficiently implementable, (2) the authors themselves acknowledge this as a major limitation in their own limitations section, (3) this represents consensus among multiple reviewers that the problem is not yet settled in these models, and (4) this constitutes a major limitation that prevents the paper from claiming complete results. The review simply states the bounds are 'not ideal' and 'not tight' without explaining the broader implications for the completeness of the work or the significance of this limitation."
    }
  },
  {
    "openreview_id": "cg1vwt5Xou",
    "reviewer_id": "us2g",
    "evaluation": {
      "flaw_id": "nontight_random_iid_bounds",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in the weaknesses section: 'For random order models and iid models, the algorithms designed are sub-optimal and cannot be implemented easily.' Additionally, in the summary it states: 'develop an upper bound that is more general than previous works (considering classical prophet inequalities) in the random order model, but the lower bound algorithm is neither computationally feasible nor optimal'",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies that the algorithms for random order and IID models are sub-optimal and not efficiently implementable, it fails to capture the deeper implications described in the ground truth. The ground truth emphasizes that these are only 'loose upper/lower bounds' and that this constitutes a 'major limitation that must be addressed before the paper can claim complete results' - essentially that the current analysis 'does not yet settle the problem in these two models.' The review treats this more as a technical shortcoming rather than recognizing it as a fundamental gap that prevents the paper from providing complete results for these models. The review doesn't convey the severity or the consensus nature of this limitation as described in the ground truth."
    }
  },
  {
    "openreview_id": "ni3Ud2BV3G",
    "reviewer_id": "43yC",
    "evaluation": {
      "flaw_id": "unclear_novelty_and_positioning",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in multiple places: 'As someone who is fairly familiar with kernel theory of neural networks, I still found it hard to identify which theorems and other results presented by the authors here are actually novel and which are applications of known results to their setting. For instance, isn't the decay rate of $1/(d+3)$ classical and known from Sobolev theory? I would suggest that the authors clarify this.' Additionally: 'the authors could be clearer about what their contributions are' and 'I would have liked to see more explanation of how to interpret your results and discussion of how they fit into the greater understanding of these networks/kernels.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. The reviewer correctly identifies that the paper fails to adequately distinguish its theoretical contributions (especially regarding theorems and results) from prior work in kernel/DNN theory. The reviewer specifically mentions difficulty identifying novel theorems versus applications of known results, questions whether certain decay rates are just classical Sobolev theory results, and calls for clearer positioning of contributions within the broader literature. This matches the ground truth concern about not adequately explaining how core theorems differ from or improve on prior kernel/DNN results. The reviewer also suggests the need for better integration with existing literature understanding, which aligns with the ground truth mention of needing comparisons with prior work."
    }
  },
  {
    "openreview_id": "ni3Ud2BV3G",
    "reviewer_id": "Yc4R",
    "evaluation": {
      "flaw_id": "unclear_novelty_and_positioning",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions this flaw in multiple places: 'The result in Theorem 4.2 appear novel and interesting, but it is a bit difficult to position this in the existing literature of, kernel or DNN. I believe that the authors should make more efforts on that to make the message of this paper more clear.' and in the questions section: 'Also, is this the FIRST time such results are established (e.g., in terms of real interpolation space of the RKHS)? How should we compare this result to other kernels/methods/models? It may be helpful to compare this behavior to the literature.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer's reasoning aligns well with the ground truth. They correctly identify that Theorem 4.2 is difficult to position within existing literature and explain why this is problematic - it makes the message of the paper unclear. The reviewer goes further to ask specific questions about how the results compare to other methods and literature, demonstrating understanding that proper positioning and comparison with prior work is essential for evaluating the contribution's significance. The reviewer also acknowledges the authors' response and asks them to include these discussions in the revised version, which matches the ground truth description of authors promising to add comparisons and strengthen positioning."
    }
  },
  {
    "openreview_id": "ni3Ud2BV3G",
    "reviewer_id": "Yc4R",
    "evaluation": {
      "flaw_id": "missing_proposition_2_2_proof",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions this flaw in the Questions section with the exact question: 'Is Proposition 2.2 novel result? If yes, where is its proof?' This explicitly identifies both the missing proof and questions the novelty of Proposition 2.2.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies that the proof of Proposition 2.2 is missing and questions its novelty, it does not provide any reasoning about why this is problematic. The reviewer simply asks the question without explaining the implications of missing proofs or unclear novelty claims. The ground truth indicates this was a significant enough issue that the authors acknowledged the omission and committed to providing the full proof and clarifying novelty in the final paper, suggesting it had meaningful impact on the paper's completeness and contribution claims. The review treats it as a simple question rather than analyzing why missing proofs are problematic for verification, reproducibility, or assessment of the paper's contributions."
    }
  },
  {
    "openreview_id": "7WoOphIZ8u",
    "reviewer_id": "cZZE",
    "evaluation": {
      "flaw_id": "insufficient_motivation",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions the motivational flaw in the weaknesses section: 'My main concern is of a motivational nature: why is the question set out inn ll. 35-36 interesting? I do not find ll. 37 sufficiently convincing.' The reviewer also asks in the questions section: 'Can you elaborate on the motivation?'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer's reasoning aligns well with the ground truth flaw description. Both identify that the paper lacks a convincing explanation for why studying derivatives of SGD iterates is interesting or important. The reviewer specifically questions why the research question is interesting and finds the authors' explanation insufficient, which matches the ground truth description of lacking 'a convincing explanation of why studying derivatives of SGD iterates is interesting.' The reviewer's request for elaboration on motivation also aligns with the authors' acknowledgment that they need to 'present a more convincing case for their motivation.'"
    }
  },
  {
    "openreview_id": "7WoOphIZ8u",
    "reviewer_id": "d938",
    "evaluation": {
      "flaw_id": "overly_strong_theoretical_assumptions",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions strong assumptions in multiple places: (1) In weaknesses, it states 'it would be helpful to add more discussions about Assumption 1, Assumption 2' and questions what examples satisfy these assumptions. (2) In questions, it specifically calls out 'the assumption σ=0 seems to be super strong to me' and asks for examples that satisfy this assumption. (3) It notes the need to discuss 'whether the assumptions for the theoretical part can be satisfied for the examples considered in the numerical section.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. It correctly identifies that the assumptions are very strong and questions their practical relevance by asking for examples that satisfy them and noting the disconnect between theoretical assumptions and practical applications. The reviewer specifically calls the σ=0 assumption 'super strong' and questions whether the theoretical assumptions can be satisfied in practice, which directly corresponds to the ground truth criticism about questioning practical relevance. The review also implicitly identifies the restrictive scope by asking for more discussion about what examples satisfy the assumptions, indicating concern about the limited applicability of the results."
    }
  },
  {
    "openreview_id": "7WoOphIZ8u",
    "reviewer_id": "mZD2",
    "evaluation": {
      "flaw_id": "overly_strong_theoretical_assumptions",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the Weaknesses section: 'I believe the main weaknesses of the paper are that the assumptions used to establish the theory are too strong' and in the Questions section: 'The smoothness assumption in Assumption 1(b) requires that the gradient are jointly L-Lipschitz continuous in x and θ. Can this assumption be relaxed?'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies why overly strong assumptions are problematic. It states that the assumptions are 'too strong' and questions whether they 'can be relaxed,' which aligns with the ground truth concern about practical relevance and restrictive scope. The review also notes that 'the practical significance of the theory is not clearly articulated,' which directly corresponds to the ground truth criticism about questioning the practical relevance of these strong assumptions. The reviewer's focus on the smoothness/strong-convexity assumptions (specifically mentioning the L-Lipschitz continuity requirement) matches the ground truth description of the flaw."
    }
  },
  {
    "openreview_id": "Z0wIbVTBXc",
    "reviewer_id": "AXnA",
    "evaluation": {
      "flaw_id": "insufficient_theoretical_justification_forward_process",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions this flaw in weakness #1: 'The work starts from the prediction of forward marginal distribution, and derive the conditional ODE and SDE. However, whether these dynamics corresponding to the same marginal distributions need some rigorous reasoning (e.g., Fokker-Planck Equation or continuity equation). For instance, the claim in L132 and L132 should be explained. Please supplement it.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. The reviewer correctly identifies that rigorous theoretical justification is missing for showing that the proposed learnable forward SDE/ODE yields the claimed marginal distributions. The reviewer specifically mentions the need for 'rigorous reasoning (e.g., Fokker-Planck Equation or continuity equation)' which matches exactly with the ground truth's requirement for 'rigorous reasoning (e.g., via the Fokker-Planck/continuity equation)'. The reviewer also correctly identifies that this theoretical proof is essential to validate the core methodology by requesting supplementation of the missing explanation. The reasoning demonstrates understanding of why this omission is problematic - without proper theoretical justification, the connection between the proposed dynamics and the claimed marginal distributions remains unproven."
    }
  },
  {
    "openreview_id": "9B6J64eTp4",
    "reviewer_id": "JqAE",
    "evaluation": {
      "flaw_id": "insufficient_reproducibility_details",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the final sentence of the Weaknesses section: 'Finally, not enough details are shared with respect to the models used, making reproducibility of the approach challenging.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies both the core issue (insufficient details about models) and its negative consequence (making reproducibility challenging). This aligns well with the ground truth description which states that the paper 'does not give enough model/training details for others to reproduce the work.' The reviewer's reasoning demonstrates understanding that missing implementation details create barriers for reproducibility, which matches the ground truth's emphasis on the need for additional implementation information to enable reproduction of the work."
    }
  },
  {
    "openreview_id": "9B6J64eTp4",
    "reviewer_id": "DWX2",
    "evaluation": {
      "flaw_id": "notation_and_figure_clarity",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions a notation issue: 'Typo: L212: geodesic distance should be e_g?' This is a direct reference to confusing or erroneous notation, which aligns with the ground truth flaw about notation and figure clarity issues.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review does identify a specific notation issue (the geodesic distance notation at L212), it treats it merely as a minor typo rather than recognizing it as part of a broader pattern of notation and figure clarity problems that significantly impact the paper's publishability. The ground truth indicates this was a substantial issue highlighted by multiple reviewers that required acknowledgment from authors and commitment to revise for the camera-ready version. The review's reasoning is superficial - it doesn't explain why notation clarity matters or recognize the broader scope of the problem affecting figures and formulas throughout the paper."
    }
  },
  {
    "openreview_id": "9B6J64eTp4",
    "reviewer_id": "JqAE",
    "evaluation": {
      "flaw_id": "notation_and_figure_clarity",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions several issues with notation and figures that align with the ground truth flaw. Specifically, it states: 'One suggestion is to provide a more detailed explanation of Figure 3, and provide dimensions of matrices U, F etc. Also, the formula in L.167-168 does not seem to be correct, especially regarding how the viewpoint v' is used.' Additionally, under minor comments, it mentions 'L.105: And the opacity value ... incomplete' and 'Section 5: the order in which figures are presented is not following the text, this creates some confusion to the reader.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies why these notation and figure issues are problematic. It explains that unclear presentation 'creates some confusion to the reader' and that the complex stage-wise learning schema requires 'additional effort should be devoted in explaining the steps as clear as possible.' The review also notes that insufficient details make 'reproducibility of the approach challenging,' which aligns with the ground truth's emphasis on the need for clearer presentation for the paper to be publishable. The reviewer provides specific examples (Figure 3 matrix dimensions, formula at L.167-168) that match the ground truth description and explains the negative impact on comprehension and reproducibility."
    }
  },
  {
    "openreview_id": "hLoiXOzoly",
    "reviewer_id": "GFMs",
    "evaluation": {
      "flaw_id": "limited_cross_model_evaluation",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the weaknesses section: 'There is a lack of comparison with other standard fitness prediction models: The baseline models are restricted to CNNs. It would be beneficial to include protein language models (e.g., ESM[4], xTrimoPGLM[5]) and potentially structure-based models(e.g., ESM-GearNet[6]), as well as comparisons with other epistasis-aware networks.' This directly corresponds to the ground truth flaw about only using CNN baselines and lacking tests with state-of-the-art pretrained protein language or structure models.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. It correctly identifies that the baseline models are restricted to CNNs and specifically calls out the need for protein language models (ESM, xTrimoPGLM) and structure-based models (ESM-GearNet), which matches the ground truth's emphasis on lacking 'state-of-the-art pretrained protein language or structure models.' The reviewer also notes in the limitations section that 'The experimental evaluation is limited, as it only includes CNN baselines' and states 'The impact and novelty of the work could be limited without more thorough experimental results,' which captures the significance of this limitation for demonstrating that the Bradley-Terry loss transfers across architectures, as mentioned in the ground truth."
    }
  },
  {
    "openreview_id": "hLoiXOzoly",
    "reviewer_id": "pZCs",
    "evaluation": {
      "flaw_id": "limited_cross_model_evaluation",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in weakness #1: 'Several previous methods account for high-order interactions, including MSA-based methods like EVE[1] and MSA-Transformer[2], and language model-based methods such as Tranception[3] and esm1v[4]. The authors should include these methods for comparison.' This directly addresses the lack of comparison with state-of-the-art pretrained protein language models, which aligns with the ground truth flaw about only using CNN baselines.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning correctly identifies why this is a problematic limitation. By specifically calling out the absence of comparisons with 'language model-based methods such as Tranception and esm1v' and 'MSA-based methods like EVE and MSA-Transformer,' the reviewer demonstrates understanding that these represent state-of-the-art pretrained protein models that should be included for a comprehensive evaluation. The reviewer also states 'I will be glad to raise my score if more solid comparisons can be provided,' indicating they recognize this limitation significantly impacts the work's credibility and scope. This reasoning aligns well with the ground truth description about the need to demonstrate that Bradley-Terry loss transfers across different architectures beyond just CNN baselines."
    }
  },
  {
    "openreview_id": "hLoiXOzoly",
    "reviewer_id": "GFMs",
    "evaluation": {
      "flaw_id": "insufficient_benchmark_coverage",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in multiple places: 'There is a lack of comparison with other standard fitness prediction tasks: For fitness prediction tasks, there are several standard experiments not performed, such as fold classification, Gene Ontology (GO), Enzyme Commission (EC) prediction, fluorescence, localization prediction, and stability prediction. Referencing benchmarks such as TAPE[1], DeepLoc[2], and dataset in DeepFRI[3] would strengthen the evaluation.' and 'There is a lack of comparison with other standard fitness prediction models' and 'The experimental evaluation is limited, as it only includes CNN baselines and FLIP dataset.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. It correctly identifies the narrow experimental scope (mentioning FLIP specifically as in the ground truth), calls for broader standard benchmarks (specifically mentioning TAPE and DeepLoc which are similar to the ProteinGym, TAPE, DeepLoc mentioned in ground truth), and notes the absence of comparisons with strong SOTA methods (mentioning ESM models specifically, which matches the ground truth's mention of ESM-based models). The review also correctly explains why this is problematic - it would 'strengthen the evaluation' and notes that 'The impact and novelty of the work could be limited without more thorough experimental results,' which aligns with the ground truth's characterization of this as a 'critical limitation.'"
    }
  },
  {
    "openreview_id": "hLoiXOzoly",
    "reviewer_id": "pZCs",
    "evaluation": {
      "flaw_id": "insufficient_benchmark_coverage",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions the flaw in two key places: (1) 'Several previous methods account for high-order interactions, including MSA-based methods like EVE[1] and MSA-Transformer[2], and language model-based methods such as Tranception[3] and esm1v[4]. The authors should include these methods for comparison.' (2) 'ProteyGym[5] is a more comprehensive benchmark dataset widely used in the literature. Please also include the evaluation of this dataset.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. The reviewer correctly identifies that the paper lacks comparisons with strong SOTA methods (EVE, MSA-Transformer, Tranception, ESM-based models) and calls for inclusion of broader standard benchmarks like ProteinGym. The reviewer specifically mentions that ProteinGym is 'a more comprehensive benchmark dataset widely used in the literature,' which captures the essence of the narrow experimental scope criticism mentioned in the ground truth. The reviewer also states 'I will be glad to raise my score if more solid comparisons can be provided,' indicating they understand this limitation affects the paper's evaluation quality, which aligns with the ground truth's characterization of this as a 'critical limitation.'"
    }
  },
  {
    "openreview_id": "r8M9SfYMDi",
    "reviewer_id": "H2jF",
    "evaluation": {
      "flaw_id": "unclear_curriculum_description",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions the unclear curriculum description in multiple places: In Question 3, the reviewer asks 'Can you elaborate more on the pacing strategy (see more on my confusion below in the suggestions)?' In the Suggestions section, the reviewer states 'I did not quite understand the exact details of the pacing mechanism for the cyclic curriculum. It would be helpful to provide a pseudo-code or explain it through mathematical formalisms. The table shows a static distribution of sampling odds, but if the model is shifting from easy to hard examples, then the sampling odds should be changing, no? It's not clear to me how exactly that is being done.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer's reasoning aligns very well with the ground truth description. The reviewer specifically identifies the lack of clarity around: (1) the pacing mechanism ('pacing strategy'), (2) sampling odds (questioning how they change during curriculum), and (3) requests pseudo-code ('It would be helpful to provide a pseudo-code'). These directly match the ground truth elements: pacing mechanism, sampling odds, and pseudo-code. The reviewer also explains why this is problematic - it creates confusion about how the method actually works, which aligns with the ground truth noting this as 'a major source of confusion.' The reviewer goes beyond just noting the omission and explains the specific technical confusion it causes."
    }
  },
  {
    "openreview_id": "r8M9SfYMDi",
    "reviewer_id": "H2jF",
    "evaluation": {
      "flaw_id": "lr_curriculum_interaction_gap",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in Question 2: 'Wouldn't a cyclic curriculum still be a problem if the learning rate starts to degrade before the first curriculum cycle? If I understand correctly, you have to run the cycles quicker than the learning rate decay? More discussion on this can be helpful.' This directly addresses the interaction between the cyclic curriculum and learning rate schedule.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer's reasoning correctly identifies the core issue described in the ground truth. They understand that the cyclic curriculum could suffer from the same low-learning-rate problem it aims to solve if the learning rate degrades before curriculum cycles complete. The reviewer specifically notes that cycles need to run 'quicker than the learning rate decay' and requests more discussion on this interaction, which aligns perfectly with the ground truth description of the theoretical motivation being incomplete without analyzing the interaction between curriculum and learning-rate schedules."
    }
  },
  {
    "openreview_id": "wWguwYhpAY",
    "reviewer_id": "VtKc",
    "evaluation": {
      "flaw_id": "misaligned_claims_intro",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly identifies this flaw in weakness W1, stating 'The major weakness of the paper is the misalignment between the experimental results and the motivations of this research' and then providing detailed analysis of two specific misaligned claims: computational efficiency gains (W1a) and the locality problem solution (W1b).",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning closely aligns with the ground truth description. For computational efficiency (W1a), the reviewer correctly identifies that the claimed gains are not supported by evidence, noting that 'no experiments regarding computational efficiency and the advantages of parallelized inference are necessary to motivate this claim.' For the locality problem (W1b), the reviewer points out the lack of supporting evidence ('I am unaware of studies that formally investigate this INR property') and contradictory experimental results (citing Figure 3c and Figure 6 as counter-examples). The review accurately captures both the absence of proper analysis/evidence and the misalignment between claims and experimental results, which matches the ground truth's emphasis on evidence being 'mis-aligned with these claims' and the lack of 'timing or locality analysis.'"
    }
  },
  {
    "openreview_id": "yAAQWBMGiT",
    "reviewer_id": "iMuZ",
    "evaluation": {
      "flaw_id": "unclear_novelty_vs_sparsification",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in the Weaknesses section: 'The core results of this paper are not technically very novel and surprising, the algorithm could be interpreted as a generalization of the leverage score sampling via JL trick due to Spielman and Srivastava, STOC'08.' This directly addresses the similarity to classical Johnson-Lindenstrauss-based methods mentioned in the ground truth.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies the core issue described in the ground truth. It specifically notes that the proposed method 'could be interpreted as a generalization of the leverage score sampling via JL trick' and states that 'the core results of this paper are not technically very novel and surprising.' This aligns well with the ground truth concern that the contribution 'risks being viewed as incremental' due to its similarity to 'classical Johnson-Lindenstrauss-based sparsification / leverage-score sampling.' The reviewer correctly identifies the lack of clear differentiation from prior methods as a weakness, though they don't explicitly call for the authors to add a discussion comparing their method to prior JL-trick and spectral sparsification methods as mentioned in the ground truth."
    }
  },
  {
    "openreview_id": "Aj0Zf28l6o",
    "reviewer_id": "sBnm",
    "evaluation": {
      "flaw_id": "missing_atom_level_baseline",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in Question 5: 'The work designs a diffusion process from substructure space to atom space, I wonder how it compare with a standard diffusion model that conditioned on substructure coordinates. Have the authors by any chance investigated similar settings?' This directly relates to the missing atom-level baseline that would start from full coordinates and correct them.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review identifies the missing comparison with atom-level baselines, the reasoning is superficial and incomplete compared to the ground truth. The review poses it as a curious question ('I wonder how it compare...') rather than identifying it as a critical flaw that undermines the paper's central claims. The ground truth emphasizes that this missing baseline prevents validation of the coarse-to-fine framework's superiority, especially given that nearly half the fragments are single atoms, making the coarse-graining benefit unclear. The review fails to recognize the severity of this omission or its impact on the paper's core contributions and validity."
    }
  },
  {
    "openreview_id": "Aj0Zf28l6o",
    "reviewer_id": "pba2",
    "evaluation": {
      "flaw_id": "incomplete_strict_threshold_evaluation",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in the weaknesses section: 'The authors should consider also reporting their performance metrics on the stricter threshold on GEOM-Drugs (delta = 0.75 A).' This directly corresponds to the ground truth flaw about missing evaluation under the stricter RMSD threshold.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies that the stricter threshold evaluation is missing, it fails to explain why this is a significant flaw. The ground truth describes that this omission makes fair comparison with recent SOTA methods (like MCF) impossible and leaves empirical verification of the method's advantages inadequate. The review simply states the authors 'should consider' reporting these metrics, treating it as a minor suggestion rather than a critical gap that undermines the paper's claims about performance advantages. The review lacks the deeper reasoning about comparative evaluation standards and the inadequacy of current empirical verification."
    }
  },
  {
    "openreview_id": "Ni9kebsSTt",
    "reviewer_id": "L2uT",
    "evaluation": {
      "flaw_id": "unclear_novelty_and_contribution_impact",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw multiple times: In the Weaknesses section: 'It's unclear to me what are the most important contributions of this work. What is really new and has the most impact on the results? Even with the help of the related work section I cannot answer this question, hence my low confidence score.' This is repeated again in the Questions section.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies that the novelty and contribution impact are unclear, the reasoning is superficial compared to the ground truth. The ground truth specifically mentions that the core issue is the lack of clarity about which NEST components are novel and responsible for gains, with the solution being to move ablation studies from the appendix to the main paper. The review only states the problem exists but doesn't identify the specific missing elements (ablation analysis) or explain how this affects the paper's credibility and support for its core claims. The review treats this as a general clarity issue rather than a fundamental problem with insufficient evidence for the paper's main contributions."
    }
  },
  {
    "openreview_id": "PGOuBHYdbr",
    "reviewer_id": "4UPn",
    "evaluation": {
      "flaw_id": "unclear_proof_integration",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in Question 5: 'Can you add to section 5 some clarity on what steps are novel and what are inspired by previous theoretical work?' and in the Weaknesses section: 'There are some parts where the explanations are probably a bit too brief to be accessible to those without expert knowledge of the field' and 'There are some further points where I feel clarification could be made around theoretical and experimental results.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth flaw. The reviewer specifically asks for clarity on 'what steps are novel' which matches the ground truth concern about how the argument departs from prior work. The reviewer also notes that explanations are 'too brief to be accessible to those without expert knowledge,' which aligns with the ground truth description of the presentation being 'too notation-heavy for non-specialists.' The reviewer's concern about making 'clarification...around theoretical and experimental results' corresponds to the ground truth issue about difficulty in verifying the correctness of the main theorem. The reviewer identifies the accessibility problem and its impact on understanding the contribution, which matches the ground truth reasoning about why this is problematic."
    }
  },
  {
    "openreview_id": "PGOuBHYdbr",
    "reviewer_id": "PpKq",
    "evaluation": {
      "flaw_id": "unclear_proof_integration",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions the flaw through multiple relevant statements: 'The paper is notation-heavy', 'It is hard to parse and keep track for a person outside the exact line of research', 'the authors should write a more sketchy proof to highlight the key technique', 'which are novel contributions? Of all the techniques involved, which is the key to this new result?', and 'I feel it would be of great help if a clear logic line for the proof is demonstrated for people outside this line of research. At the current state, it is rather hard for me to evaluate the technical contribution.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning strongly aligns with the ground truth description. Both identify that the paper lacks a clear, high-level explanation of the proof technique and is too notation-heavy for non-specialists. The review correctly explains why this is problematic: it makes the work 'hard to parse and keep track' and difficult to 'evaluate the technical contribution.' The reviewer specifically calls for the same solutions mentioned in the ground truth - a 'more sketchy proof to highlight the key technique' and clearer identification of novel vs. standard components. The reasoning goes beyond just noting the omission to explaining its negative impact on accessibility and verification, which matches the ground truth's concern about difficulty in verifying correctness for non-specialists."
    }
  },
  {
    "openreview_id": "PGOuBHYdbr",
    "reviewer_id": "yJpu",
    "evaluation": {
      "flaw_id": "unclear_proof_integration",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in multiple places: 'The paper does not sufficiently clarify how the exploration boost is integrated into the analysis. The proof strategy is not well synthesized.' and in the Questions section: 'Can you explain better what makes your analysis work exactly ? What are the proof techniques that you used, and how this differs from [18] ?'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. It identifies that the paper lacks clarity on how the exploration boost is integrated into the analysis, which matches the ground truth's concern about not providing 'a clear, high-level explanation of the new proof technique—especially how the added exploration boost is woven into the analysis.' The review also specifically asks how the work differs from [18], which directly corresponds to the ground truth's mention that reviewers found it unclear 'how the argument departs from prior work such as [18].' The reviewer also notes that 'The proof strategy is not well synthesized,' which aligns with the ground truth's concern about the presentation being too notation-heavy and difficult to verify for non-specialists."
    }
  },
  {
    "openreview_id": "PGOuBHYdbr",
    "reviewer_id": "4UPn",
    "evaluation": {
      "flaw_id": "missing_regime_comparison",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the Weaknesses section: 'It leaves the reader uncertain as to where in the space of T, m and d, the bound of this paper improves upon the bound of [18]. Adding some clarity around this would improve the paper, and help to establish the extent of its contribution.' Additionally, in Questions section, the reviewer asks: 'For which values of m and T specifically does the bound of this paper improve upon the bound in [18]? I appreciate this is problem dependent, but if this could at least be answered for the example considered in the experiments, that would be beneficial.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer's reasoning aligns well with the ground truth description. The reviewer correctly identifies that: (1) the paper lacks specification of concrete ranges where the new polynomial bound improves over the exponential term, (2) this omission makes it difficult to assess the practical significance of the main claim, and (3) this uncertainty about the extent of contribution is problematic. The reviewer goes beyond just noting the omission and explains why it's a flaw - it prevents readers from understanding where the results are actually better and hampers assessment of the contribution's significance. This matches the ground truth concern about the practical significance being 'hard to assess' without the regime comparison."
    }
  },
  {
    "openreview_id": "PGOuBHYdbr",
    "reviewer_id": "yJpu",
    "evaluation": {
      "flaw_id": "missing_regime_comparison",
      "is_flaw_mentioned": false,
      "mention_reasoning": "The review does not mention the specific issue of missing concrete ranges of m, T, d where the new polynomial regret bound improves over the exponential term. While the review discusses regret bounds and mentions that the paper 'establishes new regret bounds that improve upon previous results,' it does not identify the absence of specific parameter ranges where the improvement is meaningful or practical.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "Since the flaw was not mentioned, there is no reasoning to evaluate. The review fails to identify that the paper lacks specification of the concrete parameter ranges (m, T, d) where the polynomial regret bound actually provides practical improvement over existing exponential bounds. The ground truth indicates this omission makes it difficult to assess the practical significance of the main claim, but the reviewer did not recognize this critical gap in the analysis."
    }
  },
  {
    "openreview_id": "CL9k2PaUQb",
    "reviewer_id": "227u",
    "evaluation": {
      "flaw_id": "loose_sample_complexity_bound",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions the issue with Corollary 1's sample complexity bound in the Questions section: 'More generally, I am surprised that, in the proposed Mallows models, k! inputs are needed. I would have guessed that k^2 are sufficient, as it suffices to be correct on all comparisons of pairs of alternatives.' The review also states in the Weaknesses section that 'The theoretical guarantees seem far from tight.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies the core issue: the k! bound appears to be overly loose when k^2 should be sufficient. The reviewer provides sound mathematical reasoning, explaining that k^2 comparisons should suffice 'as it suffices to be correct on all comparisons of pairs of alternatives.' This aligns perfectly with the ground truth description that reviewers argued the correct bound should be Θ(k²) rather than Ω(k!). The review also correctly characterizes the theoretical guarantees as 'far from tight,' which matches the ground truth's description of the bound being 'wrong/overly loose.' The reviewer demonstrates understanding of why this is problematic - the bound is unnecessarily pessimistic about the algorithm's efficiency."
    }
  },
  {
    "openreview_id": "9SghPrjYU1",
    "reviewer_id": "d55R",
    "evaluation": {
      "flaw_id": "large_dataset_requirement",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions this flaw in the Weaknesses section: '(-) The derived suboptimality bounds (in Theorems 3.4 and 4.2) and the lower bound (in Theorem 5.1) require the number K of the trajectories in the offline dataset to scale with poly(d,H), which could be quite restrictive in the offline setting.' Additionally, Question 1 asks: 'Is it possible to establish suboptimality bounds and the lower bound for universal K?'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies why this is a problematic flaw. It notes that requiring K to scale polynomially with (d,H) 'could be quite restrictive in the offline setting,' which aligns with the ground truth's indication that this is a fundamental limitation. The reviewer also asks the right follow-up question about whether universal (smaller) K bounds are possible, which directly matches the concern raised by reviewer d55R mentioned in the ground truth. While the review doesn't have access to the authors' response acknowledging the limitation (as described in the ground truth), the reviewer correctly identifies the restrictive nature of the large-K assumption as a weakness in the offline RL context."
    }
  },
  {
    "openreview_id": "AYntCZvoLI",
    "reviewer_id": "cfcU",
    "evaluation": {
      "flaw_id": "insufficient_cross_architecture_and_analysis",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions both key aspects of the planted flaw. For cross-architecture testing: 'CCA Loss should assist more network architectures to prove its effectiveness. Using NAFNet alone is not enough.' For analysis/visualization: 'This paper lacks analysis. Is the causal context organization of the model better after using CCA Loss? Some visual analysis can be given, for example, after using CCA Loss, can the model notice areas that the model without CCA Loss did not notice?'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. It correctly identifies that testing on only NAFNet is insufficient to demonstrate the general effectiveness of CCA-loss, which matches the ground truth's emphasis on needing 'tests on additional architectures.' The review also correctly identifies the lack of analysis showing how the loss actually improves causal-context organization, specifically requesting 'visual analysis' to demonstrate whether 'the causal context organization of the model better after using CCA Loss.' This directly corresponds to the ground truth's mention of lacking 'visualizations' and 'visual evidence that causal context is improved.' The reviewer understands that these limitations weaken the paper's claims about the effectiveness of CCA-loss."
    }
  },
  {
    "openreview_id": "AYntCZvoLI",
    "reviewer_id": "r1X1",
    "evaluation": {
      "flaw_id": "missing_stronger_codec_comparisons",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in weakness point 1: 'In Table 1, why is the performance only compared with anchor BPG instead of VVC? Whether this strategy is effective with stronger foundational compression codecs? It is suggested to validate your method with stronger codecs and compare the performance with VVC as shown in Table 2.' The reviewer also mentions in weakness point 3 that it would be better to compare with ELIC.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer's reasoning aligns well with the ground truth description. They correctly identify that Table 1 lacks comparison with stronger codecs like VVC (mentioned in ground truth as essential), and they specifically suggest validating the method with stronger codecs. They also mention the need for ELIC comparisons, which matches the ground truth requirement for 'full VVC/ELIC comparisons.' The reviewer frames this as a validation concern - questioning whether the strategy is effective against stronger baselines - which captures the essence of why these comparisons are essential for proper evaluation and publication quality."
    }
  },
  {
    "openreview_id": "pX71TM2MLh",
    "reviewer_id": "Npgg",
    "evaluation": {
      "flaw_id": "limited_experimental_scope",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions limited experimental scope in multiple places: 'I believe the evaluation of the attack is inadequate. For instance, assessments should be conducted across a broader range of datasets and models, as well as considering various triggers and multiple target classes for the attack' and 'I am skeptical about the reported 100% ASR values in Table 1, as this is uncommon among current mainstream backdoor works' and in the Limitations section: 'The method proposed in this paper has not been effectively validated for its efficacy across different downstream tasks and a broader range of model architectures'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. It correctly identifies that the experimental scope is too narrow (needing 'broader range of datasets and models') and connects this limitation to the suspicious 100% ASR claims, stating skepticism about these results being 'uncommon among current mainstream backdoor works.' This matches the ground truth's point that 'The 100% ASR claim may stem from this limited, potentially biased setting.' The reviewer also correctly identifies the negative implications - inadequate evaluation and questionable validity of results - which aligns with the ground truth concern about insufficient empirical support compared to recent work."
    }
  },
  {
    "openreview_id": "qxS4IvtLdD",
    "reviewer_id": "PpUR",
    "evaluation": {
      "flaw_id": "missing_distortion_metrics",
      "is_flaw_mentioned": false,
      "mention_reasoning": "The review does not mention the absence of standard recovery metrics like PSNR and SSIM anywhere in the text. While the review discusses the method's performance and asks for 'more detailed computational comparison' and 'detailed computational results,' it does not specifically identify the missing quantitative evaluation metrics (PSNR/SSIM) that are essential for assessing reconstruction quality in inverse problems like super-resolution and deblurring.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "Since the flaw was not mentioned in the review, there is no reasoning to evaluate. The review fails to identify this critical omission entirely. The ground truth indicates that missing PSNR and SSIM metrics makes it 'difficult to fully assess reconstruction quality' and represents a lack of 'essential quantitative evaluation that is needed for publication.' The reviewer missed this fundamental evaluation gap, which is particularly important for inverse problems like super-resolution where reconstruction quality assessment is crucial."
    }
  },
  {
    "openreview_id": "pf4OuJyn4Q",
    "reviewer_id": "TztD",
    "evaluation": {
      "flaw_id": "single_dataset_generalization",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the first bullet point under 'Large points': 'All analysis is only performed on the TL;DR summarisation dataset, which is somewhat different from Gao et al. and the setting these preference learning algorithms are generaly used in (dialogue and instruction-following). While reproducing the whole analysis on another dataset is too much to ask, testing out some of the core hypotheses, or reproducing some of figure 1, on a different dataset (e.g. alpaca farm, or anthropic HH) would increase the robustness and generality of the results.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. The reviewer correctly identifies that using only the TL;DR dataset limits the generalizability of the findings, specifically noting that it's 'somewhat different from Gao et al. and the setting these preference learning algorithms are generally used in (dialogue and instruction-following).' The reviewer explains why this is problematic - it leaves unclear whether the over-optimization trends would hold for other tasks. The reviewer also provides constructive suggestions for addressing this limitation by testing core hypotheses on different datasets like Alpaca Farm or Anthropic HH, which demonstrates understanding of how this flaw impacts the robustness and generality of the results. This reasoning matches the ground truth concern about whether 'observed over-optimization trends hold for other tasks.'"
    }
  },
  {
    "openreview_id": "pf4OuJyn4Q",
    "reviewer_id": "TztD",
    "evaluation": {
      "flaw_id": "evaluation_distribution_shift",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the second bullet point under 'Large points': 'Using GPT-4 winrate as the \"gold\" reward or reference output also makes the results less general. The preference distribution that produced the dataset the DAAs were trained on is not the same one being used to evaluate here.' The reviewer further elaborates: 'This would more accurately mirror the real-world setting where human preferences would be used for both training and evaluation.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. The reviewer correctly identifies that there is a mismatch between the training distribution (human preferences) and evaluation distribution (GPT-4 win-rate), and explains why this is problematic: it 'makes the results less general' and threatens the validity of claims about real-world settings. The reviewer also suggests concrete solutions, proposing to use the same reward function for both training and evaluation to 'more accurately mirror the real-world setting where human preferences would be used for both training and evaluation.' This demonstrates understanding of why the distribution shift threatens the validity of the core claims, which matches the ground truth's emphasis on this being a threat to validity."
    }
  },
  {
    "openreview_id": "cU8d7LeOyx",
    "reviewer_id": "DjMB",
    "evaluation": {
      "flaw_id": "unsupported_large_cycles",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in the experimental design section: 'In the experiments, cycle length in the union graph was not controlled despite this being required for the authors' method to recover $G^{\\text{phys}}_{R=r}$. This complicates comparison with the brute-force baseline \"PC-B\".'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer correctly identifies the core issue described in the ground truth: that cycle length in union graphs is critical for the method's guarantees, but the experimental validation fails to control for this factor. The reviewer accurately points out that this limitation 'complicates comparison with the brute-force baseline' which aligns with the ground truth's assertion that this makes 'empirical validation of the theory questionable.' While the review doesn't mention the authors' specific concession about not being able to handle large cycles, it correctly identifies the fundamental problem - that the experiments don't validate the theoretical assumptions about cycle length limitations, undermining the empirical support for the method's completeness guarantees."
    }
  },
  {
    "openreview_id": "E7en5DyO2G",
    "reviewer_id": "6NTx",
    "evaluation": {
      "flaw_id": "missing_uncertainty_evaluation",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the Weaknesses section: 'The main weakness of the method is the lack of experiments evaluating the resulting model uncertainty achieved after training. From what I read, it seems to only be predictive performance via misclassification and negative log predictive density.' The reviewer also specifically suggests investigating 'predictive uncertainty via expected calibration error, or some other calibration metric.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer's reasoning aligns well with the ground truth. They correctly identify that the paper lacks direct assessment of predictive uncertainty and only reports basic performance metrics (accuracy and NLL). The reviewer specifically mentions calibration metrics (expected calibration error) as a needed evaluation approach, which directly matches the ground truth's reference to 'calibration error' as an example of missing uncertainty evaluation. The reviewer frames this as 'the main weakness' and emphasizes it in the Questions section, indicating they understand this is a significant methodological gap rather than a minor omission."
    }
  },
  {
    "openreview_id": "E7en5DyO2G",
    "reviewer_id": "uSab",
    "evaluation": {
      "flaw_id": "insufficient_novelty_discussion",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions this flaw in the Weaknesses section: 'My main concern is about the claimed novelty of the approach... The idea of using a one-step natural gradient descent on the expected log-loss directly (without the KL term) has already been proposed in the literature and analyzed; for example, [Lyu & Tsang, 2021] studied this algorithm...' The reviewer specifically references Lyu & Tsang 2021, which is one of the exact papers mentioned in the ground truth flaw description.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. The reviewer correctly identifies that the paper lacks sufficient discussion of how their approach differs from existing work, specifically mentioning the overlap with Lyu & Tsang 2021's one-step natural gradient method. The reviewer explains that the proposed method is 'simply the solution of Equation (2)... but with a linearized expected log-loss' and draws connections to existing natural gradient approaches. While the ground truth mentions additional references (Khan & Rue 2023) and the need for expanded related-work sections, the core reasoning about insufficient novelty discussion and overlap with existing natural gradient methods is accurately captured by the reviewer."
    }
  },
  {
    "openreview_id": "JEKXTLjEIq",
    "reviewer_id": "EXym",
    "evaluation": {
      "flaw_id": "ambiguous_complexity_metric",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions this flaw in the weaknesses section: 'It is not clear whether the bounds hold for the running time, as claimed, or only for the number of comparisons.' The reviewer also specifically identifies the additional computational overhead: 'The main algorithm has a step where the index of a median element is identified, which the analysis does not seem to be account for. Moreover, the portfolio algorithm has a step where m indices are sorted, which would seem to take time O(m log m).' This is further emphasized in Question 1: 'Please clarify whether/how the claimed running time bounds can be attained, or if the bounds hold for the number of comparisons instead.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer's reasoning closely aligns with the ground truth description. They correctly identify that: (1) the paper claims 'running time' bounds but the analysis may only cover comparisons, (2) additional computational work like computing medians and sorting m medians is not accounted for in the complexity analysis, and (3) these additional operations could dominate the actual running time. The reviewer demonstrates understanding of why this is problematic - it's a misrepresentation of what the theoretical guarantees actually provide. The ground truth indicates this was flagged as a 'major misrepresentation' by multiple reviewers, and this review captures the same concern with appropriate technical detail about the specific unaccounted computational steps."
    }
  },
  {
    "openreview_id": "JEKXTLjEIq",
    "reviewer_id": "kf39",
    "evaluation": {
      "flaw_id": "ambiguous_complexity_metric",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in the Questions section: 'If so, as I understand, the proposed algorithm needs to rebuild the binary search tree every time it receives a new query along with its distributional predictions. Then, this would lower bound the time complexity to answer each query with $O(n)$. Is that correct?' This directly addresses the concern about actual time complexity versus query complexity.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer correctly identifies the core issue described in the ground truth flaw. They recognize that while the paper claims certain time complexity bounds, the actual implementation would require additional work (rebuilding the binary search tree for each query) that would result in O(n) time complexity per query, which is much worse than what the theoretical analysis suggests. This aligns perfectly with the ground truth description that the analyses only bound query complexity while ignoring extra computational work like computing medians and sorting. The reviewer's reasoning demonstrates understanding that the claimed 'time complexity' guarantees don't account for the real computational overhead of the algorithm's implementation."
    }
  },
  {
    "openreview_id": "JEKXTLjEIq",
    "reviewer_id": "ud7J",
    "evaluation": {
      "flaw_id": "incorrect_lower_bound_proof",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions the flaw in the Questions section: 'I am a little confused by the proof of the lower bound. Where exactly are you using the form of the predictions $\\hat{p}$ in your argument?' The reviewer then provides a concrete counterexample showing how the proof logic can yield only an $\\Omega(\\log n)$ bound instead of the claimed $\\Omega(\\log \\eta)$ bound, asking 'could you point out what is flawed in the following proof, which gets a $\\Omega(\\log{n})$ lower bound?'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer's reasoning aligns very well with the ground truth. The ground truth states that the proof 'does not use the prediction $\\hat p$ and can only give a weaker $\\Omega(\\log n)$ bound.' The reviewer identifies exactly these issues: (1) they question where the prediction $\\hat{p}$ is actually used in the argument, noting that the reasoning 'is agnostic to the specific form of $\\hat{p}$', and (2) they demonstrate through a concrete construction that the same proof logic yields only an $\\Omega(\\log n)$ bound rather than the stronger claimed $\\Omega(\\log \\eta)$ bound. The reviewer's analysis shows they understand the fundamental gap in the proof - that it fails to properly utilize the prediction and therefore cannot establish the stronger lower bound."
    }
  },
  {
    "openreview_id": "ocxVXe5XN1",
    "reviewer_id": "D8Mx",
    "evaluation": {
      "flaw_id": "no_high_probability_guarantee",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the Weaknesses section: 'As the authors mention, the results pertain to the expected generalization error and therefore lack the high-probability generalization guarantee, which is more critical.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies both aspects of the flaw described in the ground truth: (1) that the results only bound expected generalization error rather than providing high-probability guarantees, and (2) that high-probability guarantees are 'more critical' (aligning with the ground truth's statement that they are 'more useful in practice'). The reviewer also notes that the authors themselves acknowledge this limitation ('As the authors mention'), which matches the ground truth description that states the authors 'explicitly acknowledge this gap.' While the reasoning is concise, it accurately captures the core issue and its practical implications."
    }
  },
  {
    "openreview_id": "ocxVXe5XN1",
    "reviewer_id": "gd7k",
    "evaluation": {
      "flaw_id": "missing_comparison_with_lugosi_framework",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions this flaw in weakness point 1: 'The point that one can replace KL with an arbitrary f-divergence was shown in the following paper: Lugosi G, Neu G. Online-to-PAC conversions: Generalization bounds via regret analysis. arXiv preprint arXiv:2305.19674. 2023 May 31. I could not find a clear comparison of the approach proposed in this work and the the paper by Lugosi et.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer correctly identifies the missing comparison with Lugosi & Neu (2023) and explains why this is problematic. They note that the prior work already showed that KL divergence can be replaced with arbitrary f-divergences, and they specifically call out the lack of a 'clear comparison' between the approaches. This aligns well with the ground truth description that mentions the manuscript 'lacks a clear, systematic comparison with the recent online-to-PAC / convex-analysis bounds of Lugosi & Neu (2023).' The reviewer also connects this to a broader concern about properly situating the contribution, asking 'what sort of new insights can we obtain' by replacing KL with f-divergences, which reflects the ground truth concern about being 'incomplete in situating its contribution relative to this competing framework.'"
    }
  },
  {
    "openreview_id": "7Sh0XkN1KS",
    "reviewer_id": "zGqe",
    "evaluation": {
      "flaw_id": "missing_empirical_validation",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions the lack of experimental validation multiple times: 'This paper does not offer experimental validation either, so I cannot ensure the correctness of the claim for Gaussian kernels' and in the Questions section asks 'The most important question would be if the authors could provide some experimental validations on these two claims.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. The reviewer correctly identifies that the absence of experimental validation undermines the ability to verify the theoretical claims, specifically stating they 'cannot ensure the correctness of the claim for Gaussian kernels' without empirical support. The reviewer also emphasizes this as 'the most important question' and raises concerns about whether the theoretical predictions would hold in practice, particularly given known failures of the underlying framework in other contexts. This demonstrates understanding that empirical validation is essential for substantiating the theoretically-predicted regimes, which matches the ground truth description that empirical support is essential to make the core claims publishable."
    }
  },
  {
    "openreview_id": "7Sh0XkN1KS",
    "reviewer_id": "FXXZ",
    "evaluation": {
      "flaw_id": "unclear_predicted_risk_derivation",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the Summary section: '*It is important to note that the paper assumes that this is a good estimate of the true excess risk, which is not immediately evident and I think requires more justification (see weaknesses).*' It is then elaborated in the Weaknesses section under point 2) 'Eigenlearding Framework and Predicted Risk': 'In general, I would prefer results showing that the predicted risk is a good estimate of the true risk... However, the primary papers that have been cited [1,12,25] are all about **linear** regression and not **kernel** regression... Hence I do not think those papers are sufficient theoretical justification.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer's reasoning aligns very well with the ground truth. The ground truth states that 'Several reviewers questioned the adequacy of the justification that the 'predicted risk' formula truly approximates the test risk for kernel ridge regression. The paper currently cites mainly linear-model results; the link for kernels is opaque.' The generated review makes the exact same point, noting that the cited papers [1,12,25] are all about linear regression and not kernel regression, making them insufficient for justifying the predicted risk approximation in the kernel setting. The reviewer correctly identifies that this is a gap in theoretical grounding that affects the validity of the main analysis, which matches the ground truth's assessment that 'clearer theoretical grounding is necessary for the validity of their main analysis.'"
    }
  },
  {
    "openreview_id": "7Sh0XkN1KS",
    "reviewer_id": "Ymgo",
    "evaluation": {
      "flaw_id": "unclear_predicted_risk_derivation",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in the weaknesses section: 'The eigenframework which they use rely on a number of assumptions to be valid.' This directly alludes to concerns about the theoretical foundation and validity of the approach used to derive the predicted risk formula.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review mentions that the eigenframework relies on assumptions, it does not provide the specific reasoning outlined in the ground truth. The ground truth describes a more detailed concern: that the justification for the 'predicted risk' formula approximating test risk for kernel ridge regression is inadequate, particularly noting that the paper cites mainly linear-model results and the connection to kernels is opaque. The review's mention is too superficial and doesn't capture the core issue about the gap between linear model results and kernel applications, nor does it highlight the insufficient theoretical grounding for the validity of the main analysis."
    }
  },
  {
    "openreview_id": "95VyH4VxN9",
    "reviewer_id": "j4mM",
    "evaluation": {
      "flaw_id": "limited_robustness_analysis",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions the robustness issue in the Questions section: 'ANN seems more robust than SNN (Figs. 5, 7). In the figures of ANN outputs, are the vehicle markers clearer and more accurate? What do you think SNN will do to lead to this result? Do you have any solutions to this? Can you include experiments on this to address this problem? (This question is important.)' The reviewer also asks about safety considerations and SNN robustness research.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies the robustness gap as a critical issue for safety-critical applications. The reviewer notes that 'ANN seems more robust than SNN' based on the figures, asks about the implications for autonomous driving safety ('What is the advantage of the SNN solution from the perspective of a safety-critical application?'), and specifically requests solutions and experiments to address this problem. The reasoning aligns well with the ground truth description that emphasizes the lack of robustness evaluation as particularly problematic for safety-critical autonomous driving tasks. The reviewer appropriately connects the robustness issue to the safety-critical nature of the application and requests concrete solutions, which matches the ground truth concern about the need for robustness experiments or solutions."
    }
  },
  {
    "openreview_id": "2fiYzs3YkH",
    "reviewer_id": "krnF",
    "evaluation": {
      "flaw_id": "baseline_evaluation_insufficient",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions all three components of the baseline evaluation flaw: (1) 'wrong baseline method was used (DDNM for measurements with noise)' and 'I would suggest the authors to switch to the DDNM+ variant as described in [1] for noisy inverse problems', (2) 'hyperparameter search for baseline models were not conducted' and 'I believe some hyper-parameter search on a small set of images is necessary for fair comparison' specifically mentioning DPS step size tuning, and (3) 'I would recommend the authors to include ΠGDM [2] in their comparisons' identifying the missing baseline.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review demonstrates excellent understanding of why each aspect constitutes a flaw: (1) For DDNM vs DDNM+, the reviewer correctly explains that 'DDNM [1] is designed for noise-free image reconstruction problems' and attributes poor performance to using the wrong variant for noisy settings, (2) For hyperparameter tuning, the reviewer explains that 'DPS is not robust to the choice of step size' making tuning essential for fair comparison, and (3) For missing ΠGDM, the reviewer notes 'it performs much better than DDRM' indicating it's an important omission. The reasoning aligns perfectly with the ground truth's emphasis on inappropriate baselines, lack of tuning, and omitted methods affecting fair evaluation."
    }
  },
  {
    "openreview_id": "2fiYzs3YkH",
    "reviewer_id": "g7gD",
    "evaluation": {
      "flaw_id": "gaussian_noise_assumption",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions the Gaussian noise assumption flaw in Weakness 1: 'The assumption of this method for the measurement noise in Section 3.1 is doubtable. (1) As the authors claim in the limitation section, assuming the Gaussian noise will limit its applicability to other noise types like Poisson or multiplicative noise; (2) The whole method design heavily relies on know the exact standard deviation of the Gaussian noise \\sigma, which is impractical and can lead to robustnesses issues when facing unknown noise level.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns very well with the ground truth description. It correctly identifies both key aspects of the flaw: (1) the limitation of assuming only Gaussian noise when other noise types like Poisson and multiplicative noise exist, and (2) the impracticality of requiring the exact variance σ to be known. The reviewer goes beyond just mentioning the flaw by explaining why it's problematic - noting it limits applicability, is impractical, and can cause robustness issues with unknown noise levels. The reviewer also mentions that noise estimation itself is a challenging problem and questions the practical usage of the method due to this 'impractical scenario.' This demonstrates a thorough understanding of why the Gaussian noise assumption with known variance is a significant limitation."
    }
  },
  {
    "openreview_id": "xzCuBjHQbS",
    "reviewer_id": "Y4cG",
    "evaluation": {
      "flaw_id": "risk_affine_instability",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions the risk-affine nature of RFD and its tendency to choose overly large step sizes in two places: (1) In weakness #3: 'since RFD is measuring average case performance, it's more risk-affine and tends to predict larger learning rate, which may be harmful for convergence in some cases' and (2) In question #4: 'In line 300, it says \"on CIFAR-100, the step sizes given by RFD were too large\"' and in question #5: 'its risk-affine property might be harmful'.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies the core issue described in the ground truth. It recognizes that RFD's risk-affine nature leads it to predict/choose larger learning rates that can be harmful for convergence, which aligns with the ground truth's description of 'overly large step sizes' that 'harm convergence.' The review also specifically notes the CIFAR-100 example where step sizes were 'too large,' directly matching the ground truth evidence. The reviewer demonstrates understanding that this is a problematic characteristic by stating it 'may be harmful for convergence' and asking for more analysis of this limitation. The reasoning correctly captures why this is a flaw rather than just noting it exists."
    }
  },
  {
    "openreview_id": "wfU2CdgmWt",
    "reviewer_id": "Uos4",
    "evaluation": {
      "flaw_id": "limited_realistic_experiments",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the weaknesses section: 'In my opinion, the major weakness of this paper is the lack of an additional numerical experiment, which represents a \"realistic\" setting (for instance, where the expression of the control is not known).' The reviewer also specifically requests testing on 'challenging distributions (namely, multi-modal distributions in relatively high dimension)' and mentions sampling problems as a realistic application.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. The reviewer correctly identifies that the empirical evaluation lacks realistic or challenging tasks, specifically mentioning the need for multi-modal distributions and sampling problems where the ground truth control is unknown. The reviewer explains why this is problematic - stating that 'the SOCM contribution would have more impact with additional sampling numerics' and that this weakness affects their scoring decision. This matches the ground truth description where reviewers emphasized the need for more realistic evaluation, particularly multimodal Gaussian-mixture sampling problems. The reviewer also suggests specific baselines for comparison (relative entropy and log-variance losses), which aligns with the ground truth mention of DDS/log-variance baselines."
    }
  },
  {
    "openreview_id": "wfU2CdgmWt",
    "reviewer_id": "Uos4",
    "evaluation": {
      "flaw_id": "incomplete_complexity_reporting",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions this flaw in the weaknesses section: 'I find that the complexity/accuracy tradeoff of the IDO losses (including SOCM) is not well highlighted to me. The table provided by the authors only considers one setting. To have a full picture, it should be given for all settings.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer's reasoning aligns well with the ground truth description. They correctly identify that only one scenario was provided for computational-cost/accuracy numbers and explain why this is problematic - it prevents getting 'a full picture' of the method's performance across different settings. The reviewer specifically requests that complexity/accuracy tables 'should be given for all settings,' which matches the ground truth description about the need for 'complexity/accuracy trade-off tables for all settings.' The reasoning demonstrates understanding that incomplete reporting limits the reader's ability to fully assess the method's efficiency and performance characteristics."
    }
  },
  {
    "openreview_id": "vWSll6M9pj",
    "reviewer_id": "d9WG",
    "evaluation": {
      "flaw_id": "insufficient_failure_case_analysis",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions the flaw in the weaknesses section: 'Failure cases were not discussed too much.' This is a clear reference to the insufficient analysis of failure cases in the paper.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies that failure cases were not adequately discussed, it provides no reasoning for why this is problematic. The reviewer simply states the fact without explaining the implications. According to the ground truth, this flaw represents a significant shortcoming that affects the publishability of the work, as evidenced by the authors' acknowledgment during rebuttal that they needed to run additional analyses and incorporate failure case results into the camera-ready version. The review fails to articulate why insufficient failure case analysis is detrimental to the work's completeness, reproducibility, or scientific rigor."
    }
  },
  {
    "openreview_id": "FGTDe6EA0B",
    "reviewer_id": "v4Eq",
    "evaluation": {
      "flaw_id": "infinite_language_assumption",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions the infinite language assumption in the final paragraph: 'However, the paper is assuming that all languages in the list are infinite. This is a very strong assumption.' The reviewer also states 'So, in my opinion, it is important to highlight that the assumption that all languages are infinite is being made so that the proof goes through.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. The reviewer correctly identifies that assuming all languages are infinite is 'a very strong assumption' and explains why this assumption is problematic - it's being made primarily 'so that the proof goes through' rather than for natural theoretical reasons. The reviewer also explains the technical consequence: without this assumption, 'if the first language in the list of consistent languages were exactly S_t, then the enumeration process in item 2 would run forever.' While the ground truth mentions this excludes standard language classes where finiteness is undecidable, the reviewer captures the essence of why this is an unnatural and restrictive hypothesis that limits the work's applicability."
    }
  },
  {
    "openreview_id": "yxjWAJzUyV",
    "reviewer_id": "ihUH",
    "evaluation": {
      "flaw_id": "unsupported_stochastic_mdp_claim",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in multiple places: In the weaknesses section, it states 'The authors mention that critic-based variance reduction might be necessary for high-variance trajectory-level rewards in stochastic MDPs, which seems to contradict the criticism of PPO's complexity in the introductory section. The lack of experimental support for REBEL's performance in stochastic MDPs is a significant limitation.' In the questions section, it asks 'The lack of experimental support for REBEL's performance in stochastic MDPs is a significant limitation. Can the authors provide any preliminary results or theoretical insights that support their claims about REBEL's applicability to stochastic environments?'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. It correctly identifies: (1) the lack of experimental support for stochastic MDPs, (2) the potential need for critic-based variance reduction, and (3) the contradiction with the paper's criticism of PPO's complexity. The review recognizes this as a 'significant limitation' and asks for theoretical insights or preliminary results to support the claims, which matches the ground truth's description of insufficient theoretical analysis and empirical evidence. The reviewer also notes the apparent inconsistency between criticizing PPO's complexity while potentially requiring similar complex components for stochastic MDPs, which demonstrates a sophisticated understanding of why this is problematic."
    }
  },
  {
    "openreview_id": "aFOdln7jBV",
    "reviewer_id": "fv8w",
    "evaluation": {
      "flaw_id": "unspecified_gk_generation",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in question (2): 'How did you obtain Eq. (5), the condition about $g_k$? How to guarantee Eq. (5) in implementation with unknown $g^*$, $x^*$, $L_g$?' This directly addresses the issue of how to generate or compute {g_k} in practice, which aligns with the ground truth flaw about the sequence {g_k} being taken as input without specifying how to generate it.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer's reasoning correctly identifies why the unspecified generation of {g_k} is problematic. By asking 'How to guarantee Eq. (5) in implementation with unknown $g^*$, $x^*$, $L_g$?', the reviewer pinpoints the core issue: the algorithm requires {g_k} as input but doesn't specify how to compute it in practice when key parameters are unknown. This aligns well with the ground truth description that leaving {g_k} as an external input makes the method incomplete for implementation purposes. The reviewer correctly identifies the practical implementation gap that makes the algorithm incomplete."
    }
  },
  {
    "openreview_id": "vt2qkE1Oax",
    "reviewer_id": "SSEv",
    "evaluation": {
      "flaw_id": "rigid_motion_assumption",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions the rigid motion assumption flaw in multiple places: 'As mentioned in the limitation, the paper's principle assumes that the object is rigid. However, the task that this paper works on not only includes rigid objects -- it's a general video segmentation task. Then it seems that the low-rank theory can not extend to a general setting.' and 'Do not give some corner cases or failure cases, especially for non-rigid objects.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer correctly identifies why the rigid motion assumption is a flaw, aligning well with the ground truth description. They explain that: (1) the method assumes rigid objects but the task involves general video segmentation which includes non-rigid objects, (2) the low-rank theory cannot extend to general settings due to this limitation, and (3) the method may not generalize beyond rigid motion scenarios. The reviewer also requests analysis of failure cases for non-rigid objects and suggests considering alternatives like local rigid approaches (ARAP loss), demonstrating understanding of both the limitation's implications and potential solutions. This reasoning matches the ground truth's emphasis on generalization issues and the need for further analysis or extension to non-rigid cases."
    }
  },
  {
    "openreview_id": "nTJeOXlWyV",
    "reviewer_id": "QgJJ",
    "evaluation": {
      "flaw_id": "unclear_wong_wang_implementation",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions the unclear Wong-Wang implementation in multiple places: 'RNN (in the WW form that is not clear)', 'Details of WW models are not mentioned in the main paper. Probably, listing PyTorch pseudo code for forward pass will be helpful here', 'The formulation of Φ function is not clear', and 'Figure 3, the Wong-Wang model is mentioned, the structure should be illustrated for the reader to understand it without going to the reference.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. It correctly identifies that the Wong-Wang module lacks clear description and self-contained formulation (specifically mentioning the unclear formulation of Φ function). The review also correctly points out the reproducibility implications by suggesting PyTorch pseudo code would be helpful and noting that readers shouldn't have to go to external references to understand the structure. The reviewer identifies this as problematic because 'RTifing is one of the main contributions of the paper,' which matches the ground truth's emphasis that this is central to the paper's claims. The reasoning captures both the missing detailed explanations and the negative impact on reproducibility that the ground truth describes."
    }
  },
  {
    "openreview_id": "nTJeOXlWyV",
    "reviewer_id": "UxtZ",
    "evaluation": {
      "flaw_id": "unclear_wong_wang_implementation",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions the Wong-Wang architecture issue in the weaknesses section: 'While the deep neural network based methods are sensible comparisons, the adapted Wong-Wang architecture received little test in this manuscript. For insights into the modelling, I believe a more thorough discussion of the concrete model used and which aspects of it are relevant for getting the RT distributions right could improve my trust into this method.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer's reasoning aligns well with the ground truth flaw. While they don't use the exact terminology of 'self-contained description' or 'reproducibility,' they correctly identify that the Wong-Wang architecture lacks sufficient explanation ('more thorough discussion of the concrete model used') and connect this to negative impacts on understanding and trust in the method ('could improve my trust into this method'). The reviewer also notes it 'received little test,' which relates to the insufficient description problem. The reasoning captures the core issue that the implementation details are inadequately described, making it difficult to understand and evaluate the method properly."
    }
  },
  {
    "openreview_id": "CMgxAaRqZh",
    "reviewer_id": "hJdQ",
    "evaluation": {
      "flaw_id": "missing_transferability_evaluation",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions the transferability flaw in multiple places: 1) In weaknesses: 'Transferability is a very important aspect of adversarial attacks but it is never mentioned in the paper.' 2) In questions: 'Does this method affect the transferability of adversarial attacks to other LMs (for example from Llama2-chat-7b to Vicuna-7b) compared to plain GCG?' and 'What is the effect of the filtered set size and the draft model (size, similarity to target model) on transferability?' 3) The review also specifically addresses tokenizer differences: 'GCG operates at the token level and the draft model has a different tokenizer. What is the effect of the draft model tokenizer on the resulting suffix? also on transferability?'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. The reviewer correctly identifies that transferability is 'a very important aspect of adversarial attacks' and notes it is 'never mentioned in the paper,' which matches the ground truth's description of this as a 'major weakness.' The review goes beyond just noting the omission and demonstrates understanding of why this is problematic by asking specific, technically informed questions about transferability across different LLMs and the impact of tokenizer differences. This shows the reviewer understands the practical implications and technical complexities involved, which aligns with the ground truth's emphasis on transferability analysis being 'an essential element' that was missing from the paper."
    }
  },
  {
    "openreview_id": "DAO2BFzMfy",
    "reviewer_id": "aBHC",
    "evaluation": {
      "flaw_id": "high_compute_cost",
      "is_flaw_mentioned": false,
      "mention_reasoning": "The review does not mention the high computational cost or practical burden of collecting and storing tens of thousands of personalized LoRA weight sets. While the review mentions that the paper 'fine-tuned over 65,000 models' in the summary, it does not identify this as a practical limitation or flaw. The review focuses on methodological concerns and comparisons with existing work, but does not address the preprocessing burden or computational practicality issues.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "Since the flaw was not mentioned, there is no reasoning to evaluate. The review acknowledges that the paper 'discussed the limitations of this work' but does not specifically identify or discuss the high computational cost and practical burden of collecting tens of thousands of personalized LoRA weight sets as a significant limitation. The ground truth describes this as an important practical limitation that affects the method's real-world applicability, but the reviewer did not recognize or address this concern."
    }
  },
  {
    "openreview_id": "DAO2BFzMfy",
    "reviewer_id": "aBHC",
    "evaluation": {
      "flaw_id": "multi_identity_merging",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in the Questions section: 'How does w2w perform on multi-concept generation?' This directly alludes to the limitation regarding multiple identities/subjects that the ground truth describes.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review does mention multi-concept generation as a question, it does not provide any reasoning about why this is a limitation or flaw. The ground truth describes this as a significant limitation because the framework cannot accommodate multiple personalized identities within one model, and that merging multiple LoRA identity models leads to identity interpolation and corruption. The review merely poses this as a curious question rather than identifying it as a fundamental limitation of the w2w approach that restricts its applicability to many downstream applications requiring multi-subject generation."
    }
  },
  {
    "openreview_id": "Wc0vlQuoLb",
    "reviewer_id": "4r2S",
    "evaluation": {
      "flaw_id": "ignores_high_entropy_uncertainty",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the Theory section: 'The uncertainty factor is only bigger than 0 if any other token gets assigned a higher probability than the [gold] token. It does not account for the case where a model is uncertain about *any* token and thus predicts a (low) probability for all tokens.' The reviewer provides a concrete example with city names showing how the uncertainty factor becomes 0 when all tokens have low, uniform probabilities.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning perfectly aligns with the ground truth description. The reviewer correctly identifies that: (1) the uncertainty factor λ only becomes positive when non-gold tokens exceed the gold token probability, (2) when the model assigns low, nearly uniform probabilities to all tokens (high entropy uncertainty), λ becomes zero, (3) this actually discourages IDK usage through uncertainty regularization, and (4) this undermines the method's ability to capture uncertainty. The reviewer even provides a specific mathematical example with city names to illustrate the problem, demonstrating deep understanding of why this is a fundamental theoretical flaw."
    }
  },
  {
    "openreview_id": "ktpG37Dzh5",
    "reviewer_id": "L897",
    "evaluation": {
      "flaw_id": "missing_runtime_analysis",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions the missing runtime analysis in the weaknesses section: 'The computational overhead of introducing multiplicative noise is discussed in the Limitations Section, but not considered in the experiments. I understand that this is not related to the novelties of the BMRS method itself, but it would still be helpful to provide runtime information in the tables so that the difference to the baseline can be factored in.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies the core issue described in the ground truth - that quantitative runtime information about the computational overhead of multiplicative-noise layers is missing from the experiments. The reviewer's reasoning aligns well with the ground truth by explaining why this is problematic: without runtime information in the tables, readers cannot properly assess the efficiency claims or factor in the computational differences compared to baselines. This matches the ground truth's emphasis that the missing quantitative information prevents 'a fair assessment of the efficiency claim.' The reviewer demonstrates understanding that while the overhead is acknowledged in the limitations, the lack of experimental runtime data is still a significant weakness for proper evaluation."
    }
  },
  {
    "openreview_id": "wK0Z49myyi",
    "reviewer_id": "67PC",
    "evaluation": {
      "flaw_id": "missing_benchmark_comparison",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in multiple places: 1) In the summary: 'Photometric experiments were exclusively compared with MLP-based methods, specifically NeRF-like, on the synthetic dataset of the vanilla NeRF' 2) In questions 2: 'It would be beneficial to showcase more experimental results on different types of datasets, similar to how BARF, SPARF, and L2G-NeRF were compared on the LLFF dataset' 3) In question 7: 'I recommend that the authors compare with Neus2. Neus2 has compared their work with Instant-NGP and Neus'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth. It correctly identifies that: 1) The experimental scope is too narrow, limited to synthetic NeRF datasets 2) Missing comparisons with real-scene benchmarks like LLFF dataset 3) Missing comparisons with newer baselines like NeuS2 4) The review explains why this is problematic - it makes comparisons imprecise/unfair and reduces conviction in the results. The reviewer specifically calls for 'more experimental results on different types of datasets' and comparison with newer methods like NeuS2, which directly matches the ground truth description of missing real-scene benchmarks and newer baseline comparisons."
    }
  },
  {
    "openreview_id": "wK0Z49myyi",
    "reviewer_id": "Bu9t",
    "evaluation": {
      "flaw_id": "missing_benchmark_comparison",
      "is_flaw_mentioned": false,
      "mention_reasoning": "The review does not mention the absence of results on real-scene benchmarks like LLFF, nor does it discuss missing comparisons with newer baselines such as BAA-NGP, NeuS2, or NoPe-NeRF. While the review states 'The proposed technique has been evaluated across various standard datasets and against meaningful NeRF-like algorithms' and mentions 'Assessment on a large number of datasets' as a strength, it does not identify the specific gap in benchmark coverage that constitutes the planted flaw.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "Since the flaw was not mentioned, there is no reasoning to evaluate. The review actually presents the experimental evaluation as comprehensive ('Assessment on a large number of datasets' and 'Numerous and conclusive results'), which is contrary to the ground truth description that identifies significant gaps in the experimental scope, particularly the absence of real-scene benchmarks and comparisons with newer baselines."
    }
  },
  {
    "openreview_id": "wK0Z49myyi",
    "reviewer_id": "GtQM",
    "evaluation": {
      "flaw_id": "missing_benchmark_comparison",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions both key aspects of the planted flaw: (1) 'Experiments were only conducted on NeRF-synthetic datasets and not on LLFF datasets' and (2) 'Comparison is made with older baseline methods (e.g., SPARF, BARF, L2G) which are more than 2 years old. It's recommended to include more recent methods such as NoPe-NeRF and BAA-NGP.' These directly correspond to the ground truth flaw about missing real-scene benchmarks (LLFF) and newer baselines (BAA-NGP, NoPe-NeRF).",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies what is missing (LLFF dataset and newer methods like NoPe-NeRF and BAA-NGP), it provides only superficial reasoning. The review simply states that experiments were 'only conducted on' synthetic datasets and that comparisons were made with 'older baseline methods,' but it doesn't explain why this narrow experimental scope is problematic. The ground truth indicates this was judged as making experiments 'too narrow' and represents a significant gap requiring broadening of experimental scope. The review lacks depth in explaining the negative implications of this limitation on the paper's contribution and validity."
    }
  },
  {
    "openreview_id": "wK0Z49myyi",
    "reviewer_id": "LukG",
    "evaluation": {
      "flaw_id": "reproducibility_details_insufficient",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions reproducibility concerns in the Weaknesses section, stating 'Reproducibility - I believe that many details are missing (including from the appendix) for one to be able to implement the proposed method.' The reviewer then provides a detailed bulleted list of missing implementation details including SuperPoint/SuperGlue settings, auxiliary ray sampling, fusion function g, loss weight values, and pose initialization procedure.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer correctly identifies why missing implementation details constitute a flaw by explicitly connecting them to reproducibility concerns. The statement 'for one to be able to implement the proposed method' directly addresses the reproducibility impact. The reviewer identifies the same specific missing details mentioned in the ground truth (SuperPoint/SuperGlue settings, auxiliary-ray sampling strategy, fusion function g, loss-weight values, pose-initialization procedure). While the reviewer doesn't use the exact phrase 'critical item that must be addressed before publication,' they clearly frame this as a significant weakness that impacts the paper's value, which aligns with the ground truth's emphasis on this being a critical publication requirement."
    }
  },
  {
    "openreview_id": "cjH0Qsgd0D",
    "reviewer_id": "d4vx",
    "evaluation": {
      "flaw_id": "non_permutation_invariant_encoder",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions the permutation invariance issue in question 6: 'How is it possible to train it for such high-dimensional microscopic input states, if the encoder is not permutation invariant? For example, if the model is trained using a fixed index for each molecule, and then the indices are shuffled at random, the input to the autoencoder completely changes while the conformation (independent of the index) does not change at all.' The reviewer also mentions it in the limitations section: 'The authors do not discuss the limitations of the auto-encoder they use to map to a latent space, in particular regarding permutation invariance of the molecular indices.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning closely aligns with the ground truth description. The reviewer correctly identifies that: (1) the encoder is not permutation-invariant, (2) shuffling atom indices changes the input while the physical conformation remains unchanged, (3) this creates a fundamental problem where the model would need to learn all conformations including permutations, leading to an incredibly high-dimensional space that is not feasible to sample. The reviewer also notes this undermines the encoder's suitability for the problem and mentions it as a limitation that wasn't addressed by the authors. This reasoning matches the ground truth's emphasis on how this undermines generalization and represents a major methodological weakness."
    }
  },
  {
    "openreview_id": "gL5nT4y8fn",
    "reviewer_id": "Chm5",
    "evaluation": {
      "flaw_id": "missing_slm_evaluation",
      "is_flaw_mentioned": false,
      "mention_reasoning": "The review does not mention anything about missing evaluations on smaller language models (Phi-3, MiniCPM, Qwen) or the need to demonstrate practical applicability beyond 7-8B parameter LLMs. While the review discusses scalability concerns and computational costs, it focuses on scaling to 'even larger models' rather than addressing the gap in smaller model evaluations.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "Since the flaw was not mentioned in the review, there is no reasoning to evaluate. The review's discussion of scalability is actually oriented in the opposite direction from the ground truth flaw - it worries about computational costs when scaling up to larger models, while the planted flaw concerns the missing evaluation on smaller, more practical models. The review completely misses the important gap identified by Reviewer Chm5 regarding the need for quantitative results on smaller language models to demonstrate broader practical applicability."
    }
  },
  {
    "openreview_id": "3hcn0UxP72",
    "reviewer_id": "9QVd",
    "evaluation": {
      "flaw_id": "limitations_discussion_in_main_text",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions limitations in two places: (1) In the Weaknesses section, it states 'The main weakness of this work is its scope. The work studies in great detail a very specific architecture, but it is unclear how interesting their conclusions are for either theorists or practitioners.' (2) In the Limitations section, it notes 'The authors discuss their limitations in a dedicated section.' The review also touches on specific limitations like the simple numerical example and questions about extending to more complicated settings.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review does identify that the work has limited scope and mentions that limitations are discussed in a dedicated section, it does not capture the core issue described in the ground truth. The ground truth specifically indicates that multiple reviewers emphasized the limitations should be moved from Appendix F into the main text and made more prominent, covering discrete GD, regularization, and architectural issues. The review doesn't identify the placement problem (limitations being 'hidden' in an appendix) or request that they be moved to a more prominent location in the main paper. The review's reasoning focuses more on the inherent narrow scope rather than the presentation and visibility issue that the ground truth describes."
    }
  },
  {
    "openreview_id": "3hcn0UxP72",
    "reviewer_id": "JxYP",
    "evaluation": {
      "flaw_id": "limitations_discussion_in_main_text",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the Limitations section: 'The paper accurately describes the contribution including its limitations. These limitations are adequately and carefully disclosed in multiple places in the main text and discussed in detail in appendix F.' The reviewer then states: 'If the work is to be accepted I would encourage the authors to include the valuable discussion in appendix F in the main text, and to extend this discussion along the lines of (3).' This directly addresses the need to move limitations from the appendix to the main text.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth. The reviewer correctly identifies that important limitations are relegated to Appendix F and should be moved to the main text. The review also correctly identifies the specific limitations mentioned in the ground truth: (1) restriction to single hidden layer networks, (2) the gap between gradient flow and discrete gradient descent, and (3) that obstructions only exist for scalar outputs. The reviewer specifically notes that point (3) has implications that 'could have more thoroughly discussed' and recommends extending this discussion. The reviewer also questions the practical relevance of the work given these limitations, which aligns with the ground truth's mention that reviewers emphasized the work 'under-states its practical limitations.' The recommendation to move Appendix F content to the main text matches exactly what the ground truth describes as the needed change."
    }
  },
  {
    "openreview_id": "psDrko9v1D",
    "reviewer_id": "uVQ4",
    "evaluation": {
      "flaw_id": "limited_applicability_routing_ilp",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the weaknesses section: 'While the work introduces a new approach for gradient-based combinatorial optimization, it inherits the limitations of gradient-based methods. It may struggle with complex CO problems, such as routing problems, as discussed by the authors.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies and reasons about this flaw. It accurately notes that the HeO framework 'may struggle with complex CO problems, such as routing problems' and acknowledges that this limitation was 'discussed by the authors.' This aligns well with the ground truth description, which mentions that both reviewer uVQ4 and the authors' own Limitations section acknowledge that the method cannot yet tackle routing problems and integer linear programming due to encoding difficulties. The review correctly frames this as a limitation that the gradient-based approach inherits, showing understanding of why this is problematic for the method's broader applicability to combinatorial optimization problems."
    }
  },
  {
    "openreview_id": "VFRyS7Wx08",
    "reviewer_id": "QeNS",
    "evaluation": {
      "flaw_id": "limited_task_scope_binary_success",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this limitation in the Weaknesses section: 'The experimental campaign is concentrated on simple tasks; more complex tasks could prove scalability.' This alludes to the limited scope of tasks the method was evaluated on, though it doesn't specifically mention the binary success/failure criterion aspect.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review identifies that the experimental evaluation is limited to simple tasks and suggests more complex tasks are needed, it frames this as a scalability concern rather than addressing the core limitation described in the ground truth. The ground truth specifically identifies that the method relies on tasks with binary success/failure criteria and its effectiveness is uncertain for tasks with continuous performance metrics. The reviewer's reasoning focuses on task complexity and scalability rather than the fundamental limitation regarding evaluation metrics (binary vs. continuous). The reviewer does not demonstrate understanding of why this limitation affects the method's applicability to different types of performance evaluation."
    }
  },
  {
    "openreview_id": "nIeufGuQ9x",
    "reviewer_id": "2g78",
    "evaluation": {
      "flaw_id": "missing_qualitative_results",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the Weaknesses section: 'The paper lacks qualitative results. I would've liked to see visual comparison of scene flow results (especially for challenging cases) on various datasets.' Additionally, in the Questions section, the reviewer asks 'Can the authors provide some challenging samples of data that your model is robust against?' which further reinforces the concern about missing visual examples.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. The reviewer correctly identifies that the lack of qualitative results 'makes it hard to interpret where the proposed approach performs better than current methods and more importantly where the proposed approach fails.' This matches the ground truth concern that the absence of visual examples makes it 'hard to judge where the method excels or fails.' The reviewer specifically requests visual comparisons for challenging cases, which directly corresponds to the ground truth mention of needing 'visual comparisons and challenging examples.' The reasoning demonstrates understanding of the negative impact this omission has on the ability to evaluate the method's performance and failure modes."
    }
  },
  {
    "openreview_id": "nIeufGuQ9x",
    "reviewer_id": "qmev",
    "evaluation": {
      "flaw_id": "missing_qualitative_results",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions the lack of visualizations in the weaknesses section: 'To enhance understanding of the diffusion process for scene flow estimation, visualizations of source points with added noise at different time steps should be included.' This directly addresses the missing qualitative/visual examples identified in the ground truth.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies why the lack of visualizations is problematic - it states that visualizations are needed 'to enhance understanding of the diffusion process for scene flow estimation.' This aligns well with the ground truth description that notes the lack of qualitative examples makes it 'hard to judge where the method excels or fails.' While the review focuses specifically on visualizing the diffusion process rather than the broader scope of scene-flow predictions mentioned in the ground truth, the core reasoning is sound - that visual examples are necessary for proper evaluation and understanding of the method's performance."
    }
  },
  {
    "openreview_id": "nIeufGuQ9x",
    "reviewer_id": "yrBG",
    "evaluation": {
      "flaw_id": "missing_qualitative_results",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in the weaknesses section: 'The author could consider providing more visualizations of the (reverse) diffusion process other than the only one GIF attached in the supplementary. This will be very helpful.' This directly addresses the lack of qualitative/visual examples mentioned in the ground truth.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies the lack of visualizations, the reasoning is superficial compared to the ground truth. The review simply states that more visualizations 'will be very helpful' without explaining the deeper implications. The ground truth indicates that the missing qualitative results make it 'hard to judge where the method excels or fails,' which is a more substantive criticism about the paper's ability to demonstrate method performance and limitations. The review treats this as a minor suggestion for improvement rather than a significant methodological concern that affects the paper's evaluability."
    }
  },
  {
    "openreview_id": "nIeufGuQ9x",
    "reviewer_id": "gpDD",
    "evaluation": {
      "flaw_id": "unclear_method_section_3_2",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions the flaw in Weakness #1: 'The organization of the technical part can be improved. It is a bit difficult to follow Sec. 3.2. For example, the paper has already introduced the forward process, reverse process and the training loss, and then introduces the details of the reverse process again.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. Both identify that Section 3.2 is hard to follow due to organizational issues. The ground truth mentions it is 'repetitively organised' and the review provides a specific example of this repetition: 'the paper has already introduced the forward process, reverse process and the training loss, and then introduces the details of the reverse process again.' This demonstrates the reviewer understood not just that the section was unclear, but specifically why it was unclear - due to repetitive organization and poor structure, which matches the ground truth's characterization."
    }
  },
  {
    "openreview_id": "6FTlHaxCpR",
    "reviewer_id": "pL6e",
    "evaluation": {
      "flaw_id": "missing_efficiency_evaluation",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in the Questions section: 'With additional optimizations, how much longer would the training time be compared to the baseline and other methods?' This directly alludes to the missing quantitative analysis of training speed that is described in the ground truth flaw.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review does mention the missing efficiency evaluation by asking about training time comparisons, it fails to provide the correct reasoning for why this is a flaw. The ground truth indicates this is a required evaluation that was explicitly requested by reviewers and the Area Chair, with authors agreeing to include comprehensive efficiency metrics (training/inference speed, GPU memory, storage consumption) in the revision. The review treats this as a casual question rather than identifying it as a critical omission that needs to be addressed for publication. The review does not mention the broader scope of efficiency metrics beyond training time, nor does it convey the urgency or requirement nature of this evaluation."
    }
  },
  {
    "openreview_id": "6FTlHaxCpR",
    "reviewer_id": "rwSL",
    "evaluation": {
      "flaw_id": "missing_efficiency_evaluation",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the weaknesses section: 'The time, memory, and storage costs are not revealed: While the proposed method is illustrated as a simple solution, it is crucial to analyze the additional computational burden it imposes. The paper does not provide an analysis of the time, memory, and storage requirements, which are important factors in evaluating the practicality of the method.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. The reviewer correctly identifies that the missing efficiency evaluation is problematic because: (1) it's crucial to analyze the computational burden the method imposes, (2) time, memory, and storage requirements are important factors for evaluating practicality, and (3) without this analysis, the true feasibility of the approach cannot be assessed. The reviewer goes further by connecting this to a specific technical concern about data sampling mechanisms and potential storage implications (N(N-1) optical flow maps). This demonstrates understanding of why efficiency metrics are essential for a complete evaluation, matching the ground truth's emphasis on the necessity of memory, speed, and storage evaluation for publication."
    }
  },
  {
    "openreview_id": "6FTlHaxCpR",
    "reviewer_id": "4hTv",
    "evaluation": {
      "flaw_id": "unvalidated_camera_pose_dependency",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in the weaknesses section: 'The quality of camera pose estimation is not well evaluated. There is only a visualization in the appendix which is very hard to interpret. (See Fu et al. 2024 for a more meaningful evaluation).' The review also notes the dependency on COLMAP: 'COLMAP must find an initial solution for camera pose estimation which requires enough static features in the scene.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies why this is a flaw by explaining that: 1) The method depends on COLMAP producing initial poses ('COLMAP must find an initial solution for camera pose estimation'), 2) There is insufficient quantitative evaluation of pose quality ('The quality of camera pose estimation is not well evaluated'), and 3) The evaluation provided is inadequate ('There is only a visualization in the appendix which is very hard to interpret'). The reviewer also references a more thorough evaluation approach from another paper (Fu et al. 2024), indicating they understand what proper evaluation should look like. This reasoning aligns well with the ground truth description about the lack of quantitative evaluation of pose quality and robustness when SfM fails."
    }
  },
  {
    "openreview_id": "zcEPOB9rCR",
    "reviewer_id": "U4NU",
    "evaluation": {
      "flaw_id": "insufficient_related_work_discussion",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in multiple places: (1) In strengths: 'The paper is well written except for some missing relevant works on diffusion bridge.' (2) In weaknesses: 'Related works on diffusion bridges or diffusion mixtures were not discussed. Diffusion bridges has been studied in [1,2,3,4] with applications to molecules, graphs, point clouds, and images, and more recent works have studied general framework for diffusion bridges [5, 6] which is worth discussing.' (3) The reviewer also notes the need for 'discussing related works and clarifying the novel contributions is necessary in particular for strengthening the contribution of this work.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns very well with the ground truth description. The reviewer correctly identifies that the omission of related work discussion is problematic because: (1) It makes the contribution appear limited without proper contextualization ('Contribution seems limited as using diffusion bridge as generative modeling was already studied'), (2) It fails to clarify novel contributions relative to existing work ('discussing related works and clarifying the novel contributions is necessary'), and (3) It weakens the overall contribution of the work ('strengthening the contribution of this work'). The reviewer provides specific examples of missing citations and explains how this omission impacts the perceived novelty and contribution of the work, which matches the ground truth's indication that this was considered a 'major shortcoming' by multiple reviewers."
    }
  },
  {
    "openreview_id": "zcEPOB9rCR",
    "reviewer_id": "znC7",
    "evaluation": {
      "flaw_id": "insufficient_related_work_discussion",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions insufficient related work discussion in Q3: 'A more reasonable baseline would be to directly apply existing bridge models (e.g., [2]) to the current task by switching the backbone to the one this paper adopts. This would help the audience understand the unique contribution of this work since both bridge models and equivariant (geometric) diffusion models have been proposed in literature.' This indicates the reviewer recognizes that existing bridge models and equivariant diffusion models from literature are not adequately discussed or compared against.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer's reasoning aligns well with the ground truth. They correctly identify that the paper lacks proper discussion and comparison with existing bridge models and equivariant diffusion models, which matches the ground truth's mention of omitting 'diffusion-bridge and equivariant diffusion works.' The reviewer explains the negative impact: it makes it difficult for the audience to 'understand the unique contribution of this work,' which corresponds to the ground truth's concern about needing to 'clarify their novel contributions.' The reviewer also suggests concrete solutions (comparing with existing bridge models using the same backbone) that would address the insufficient related work discussion."
    }
  },
  {
    "openreview_id": "zcEPOB9rCR",
    "reviewer_id": "znC7",
    "evaluation": {
      "flaw_id": "missing_key_baseline_comparisons",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in multiple places: In Weakness 3: 'Additional baselines may be considered. The baselines selected in the paper are not closely connected to the proposed approach.' In Q1: 'A more convincing benchmark protocol would be to compare with methods such as GeoDiff [1] on molecule generation tasks since they are also generative models.' In Q3: 'A more reasonable baseline would be to directly apply existing bridge models (e.g., [2]) to the current task by switching the backbone to the one this paper adopts.' The reviewer specifically references bridge models and mentions Zhou et al.'s Denoising Diffusion Bridge Models paper.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning closely aligns with the ground truth description. The reviewer correctly identifies that: 1) Strong generative baselines are missing, specifically mentioning bridge models which matches the ground truth's emphasis on 'strong generative baselines (e.g., DiffMD and other bridge models)'. 2) The reasoning is sound - the reviewer explains that since the paper positions itself as tackling generative modeling, the experiments should include appropriate generative baselines rather than just direct prediction approaches. 3) The reviewer suggests specific solutions (applying existing bridge models to the current task) which demonstrates understanding of why this comparison is necessary for validating the proposed generative model. The reasoning goes beyond just noting the omission and explains the methodological importance for proper validation."
    }
  },
  {
    "openreview_id": "TxffvJMnBy",
    "reviewer_id": "MH7f",
    "evaluation": {
      "flaw_id": "missing_experimental_baselines",
      "is_flaw_mentioned": false,
      "mention_reasoning": "The review does not mention the lack of experimental baselines, missing comparisons with existing COCO baselines, or the absence of regret/CCV metrics in the empirical evaluation. The review focuses entirely on theoretical contributions, algorithm design, and mathematical bounds without discussing experimental validation or baseline comparisons.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "Since the flaw was not mentioned at all, there is no reasoning to evaluate. The review completely overlooks the critical issue that the empirical section only reports AUC-ROC for the proposed algorithm without any baseline comparisons or proper regret/CCV metrics. This is a significant omission given that the ground truth describes this as a major weakness that prevents demonstrating the claimed practical advantages. The reviewer appears to have focused solely on the theoretical aspects of the work while missing the inadequate experimental validation."
    }
  },
  {
    "openreview_id": "TxffvJMnBy",
    "reviewer_id": "pcig",
    "evaluation": {
      "flaw_id": "overstated_lower_bound_statement",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the Weaknesses section: 'My only real grouse with the paper is the lower bound of Theorem 3, which I find to be overstated. The theorem is presented as \"under assumptions 1, 2, and 3, the regret and CCV are Ω(√T),\" but the proof only defines a single instance where this occurs, and this further requires the strange assumption that the dimension of the domain d, exceeds T.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning perfectly aligns with the ground truth description. The reviewer correctly identifies that: (1) Theorem 3 is stated as a general result but (2) the proof only holds when d ≥ T (high-dimensional problems), and (3) this constitutes an overstatement of the theorem's generality. The reviewer even provides a constructive suggestion for how to reformulate the theorem to make the d-T relationship explicit, proposing language like 'min(regret_T, CCV_T) = Ω(√min(T,d))'. This demonstrates not only correct identification of the flaw but also deep understanding of why it's problematic and how it should be fixed."
    }
  },
  {
    "openreview_id": "NU3tE3lIqf",
    "reviewer_id": "5zzk",
    "evaluation": {
      "flaw_id": "incomplete_baseline_comparison",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions missing comparisons with related work: 'Some reference (and comparison) to related work appears to be missing: [Robust Gaussian Splatting](https://arxiv.org/abs/2404.04211) (April 2024). Note there are also several relevant concurrent works that have since appeared (but which would be unreasonable to call out as a weakness).' The review also notes inconsistent baseline selection: 'Why are the considered baselines different depending on the dataset? Only 3DGS is used on both the NeRF On-the-go and Photo Tourism datasets.'",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review identifies missing comparisons with related work (specifically mentioning Robust Gaussian Splatting), it does not provide the correct reasoning about why this is a flaw. The ground truth emphasizes that missing comparisons with relevant recent 3DGS variants (SWAG, GS-W, RobustGS) is essential for validating claimed state-of-the-art performance. The review mentions one missing baseline but frames it as a minor weakness rather than understanding it as fundamental to validating performance claims. The review also doesn't recognize that these missing comparisons undermine the paper's ability to demonstrate true state-of-the-art performance, which is the core issue identified in the ground truth."
    }
  },
  {
    "openreview_id": "NU3tE3lIqf",
    "reviewer_id": "bi9Y",
    "evaluation": {
      "flaw_id": "incomplete_baseline_comparison",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions the missing comparisons with recent 3DGS variants in the weaknesses section: 'Additionally, could you compare the results with the latest SWAG [5] and GS-W [42], and then discuss the similarities and differences with them?' This directly addresses the absence of quantitative and qualitative comparisons with relevant recent 3DGS variants mentioned in the ground truth.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies the missing comparisons with SWAG and GS-W (two of the three baselines mentioned in the ground truth), the reasoning is superficial. The review simply asks for these comparisons to be added but fails to explain why this omission is problematic. The ground truth emphasizes that 'Including these baselines is essential for validating the claimed state-of-the-art performance,' but the review doesn't articulate this critical impact on the validity of the performance claims. The review treats this as a minor suggestion rather than a fundamental flaw that undermines the paper's contribution claims."
    }
  },
  {
    "openreview_id": "NU3tE3lIqf",
    "reviewer_id": "nPTW",
    "evaluation": {
      "flaw_id": "incomplete_baseline_comparison",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions the missing comparisons with recent 3DGS variants in multiple places: 'many works such as SWAG, GS-W, and Scaffold-GS also mention appearance encoding... I do not see any significant differences between this work and these existing works' and 'In Line 87, you mention two related works, SWAG and GS-W, but the results analysis section does not compare your method with theirs.' The review also states 'A comparison with the stronger baseline should be added in Tables 1, 2, and 3.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies why this is a problematic flaw. It explains that without comparisons to recent relevant methods like SWAG and GS-W (which the review notes also use similar techniques like MLP appearance encoding), it's impossible to assess the true contribution and novelty of the proposed work. The reviewer states 'it is difficult to determine how much the performance enhancements are due to the improvement strategies proposed in this paper' and questions whether the work presents 'original contributions' or is just 'a combination of existing methods.' This reasoning aligns well with the ground truth's emphasis that these comparisons are 'essential for validating the claimed state-of-the-art performance.' The review goes beyond just noting the omission and explains its negative impact on validating the method's effectiveness and distinguishing it from existing work."
    }
  },
  {
    "openreview_id": "4NJBV6Wp0h",
    "reviewer_id": "g5q5",
    "evaluation": {
      "flaw_id": "missing_nonself_baseline",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw twice in the Weaknesses section: 'Additional experiments, particularly with pairs that do not include self-generated text, would strengthen the findings' and 'The study does not consider pairs without self-created summaries, which could provide crucial insights into whether LLM preferences are genuinely self-preferential or random.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies why this missing baseline is problematic. It accurately explains that without pairs that don't include self-generated text, it's difficult to determine 'whether LLM preferences are genuinely self-preferential or random.' This aligns well with the ground truth's concern about isolating 'true self-preference from ordering or random biases.' The reviewer demonstrates understanding that this baseline is necessary to validate the core claims about self-preference behavior."
    }
  },
  {
    "openreview_id": "EiIelh2t7S",
    "reviewer_id": "3a2j",
    "evaluation": {
      "flaw_id": "missing_training_setup",
      "is_flaw_mentioned": false,
      "mention_reasoning": "The review does not mention any issues with missing training setup details, data selection, or hyper-parameter settings for fine-tuning Llama-2-7B to 32k tokens. While the review mentions 'Experimental Rigor and Reproducibility: It includes extensive experiments to back up the theoretical findings, along with detailed setup and results' in the strengths section, this actually contradicts the planted flaw rather than identifying it.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "Since the flaw was not mentioned, there is no reasoning to evaluate. The review actually praises the paper for having 'detailed setup and results' and good 'reproducibility,' which directly contradicts the ground truth flaw that describes missing training setup information that makes it impossible to verify empirical results. The reviewer completely missed this critical reproducibility issue."
    }
  },
  {
    "openreview_id": "EiIelh2t7S",
    "reviewer_id": "aPSb",
    "evaluation": {
      "flaw_id": "unreleased_models",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in weakness #4: 'Will the author plan to release the models (including the fine-tuned Llama2, Baichun2 with varying lengths, and the 2b model series)? This would benefit future work.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies why this is a flaw by noting that releasing the models 'would benefit future work.' This aligns with the ground truth description's emphasis on the need for other researchers to validate and extend the study. While the review doesn't explicitly use terms like 'reproducibility' or 'validation,' it captures the core concern that withholding models limits the scientific community's ability to build upon the research. The reasoning demonstrates understanding of the broader impact on research progress, which matches the ground truth's focus on enabling other researchers to validate or extend the study."
    }
  },
  {
    "openreview_id": "LuqrIkGuru",
    "reviewer_id": "ZRPp",
    "evaluation": {
      "flaw_id": "weak_theoretical_link",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in the weaknesses section: 'There exists a gap between Lemma 1 and the goal of the homophily-increase principle. Increased homophily cannot guarantee the increase of $\\Delta_{sp}$ and $\\Delta_{eo}$.' This directly addresses the lack of rigorous justification for why increasing node-level homophily necessarily worsens group-fairness metrics.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning correctly identifies the core issue described in the ground truth. The reviewer accurately points out that there is a 'gap' between the theoretical foundation (Lemma 1) and the practical goal, specifically noting that 'increased homophily cannot guarantee the increase of $\\Delta_{sp}$ and $\\Delta_{eo}$.' This aligns perfectly with the ground truth description that the paper 'does not rigorously justify why increasing the node-level homophily ratio necessarily worsens group-fairness metrics.' The reviewer correctly identifies that the theoretical connection is missing and that this represents a fundamental weakness in the paper's core principle, matching the ground truth's assessment that the manuscript 'lacks a solid guarantee for its core principle.'"
    }
  },
  {
    "openreview_id": "LuqrIkGuru",
    "reviewer_id": "wXHX",
    "evaluation": {
      "flaw_id": "weak_theoretical_link",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in weakness #2: 'Another concern is that the theoretical effectiveness of \"the node injection strategy is evaluated by an increase in the node-level homophily ratio\". This homophily ratio does not clearly establish a connection with fairness metrics, i.e., DP/EO. Providing a theoretical guarantee that the proposed node injection strategy results in more significant improvements in DP/EO compared to random node injection will enhance the validity.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning correctly aligns with the ground truth flaw description. Both identify that the core issue is the lack of rigorous justification for why increasing node-level homophily ratio would worsen fairness metrics (ΔSP, ΔEO in ground truth; DP/EO in review). The review explicitly states that 'This homophily ratio does not clearly establish a connection with fairness metrics' and calls for 'a theoretical guarantee,' which directly corresponds to the ground truth's mention that 'the theoretical connection is missing' and that the manuscript 'lacks a solid guarantee for its core principle.' The review correctly identifies this as a validity concern that undermines the paper's foundational assumptions."
    }
  },
  {
    "openreview_id": "gVTkMsaaGI",
    "reviewer_id": "4zDq",
    "evaluation": {
      "flaw_id": "scalability_memory_constraint",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions the scalability and memory issues under the 'Weaknesses' section. Key quotes include: 'One main concern I have is how scalable the method is due to trajectory matching, especially as the prior becomes larger' and 'Section H discusses some techniques employed to reduce memory usage, but even this relatively small-scale experiment (50 steps on latent diffusion with LoRA), the authors report that they could fit only 8 timesteps on an A100.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning correctly aligns with the ground truth description. The reviewer identifies that: (1) RTB training has memory constraints that limit scalability, (2) even on an A100 GPU, only 8 timesteps could be fit for a relatively small latent diffusion model, (3) this raises concerns about applicability to larger foundation models, and (4) acknowledges the authors' discussion of memory reduction techniques. The reviewer correctly connects these memory limitations to the broader concern about the method's practical applicability to large-scale diffusion priors, which matches the ground truth's emphasis on doubts about real-world applicability and the need for more efficient implementations."
    }
  },
  {
    "openreview_id": "gVTkMsaaGI",
    "reviewer_id": "VfXT",
    "evaluation": {
      "flaw_id": "overstated_sota_claim",
      "is_flaw_mentioned": false,
      "mention_reasoning": "The review does not mention anything about overstated claims regarding state-of-the-art results in the abstract or anywhere else in the paper. There is no discussion of RTB's performance claims being exaggerated relative to QGPO and D-QL, nor any mention of needing to soften the language around state-of-the-art achievements.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "Since the flaw was not mentioned at all in the review, there is no reasoning provided about why overstating state-of-the-art claims would be problematic. The ground truth flaw specifically concerns the accuracy of performance claims in the abstract, but the reviewer focused entirely on other aspects like experimental design, baseline comparisons, and methodological concerns without addressing the validity of the state-of-the-art claims made by the authors."
    }
  },
  {
    "openreview_id": "4kVHI2uXRE",
    "reviewer_id": "syDC",
    "evaluation": {
      "flaw_id": "rl_algorithm_justification",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the weaknesses section stating 'The motivations behind some algorithmic choices are not clear' and more specifically in question (1): 'Why TD3? why not any other off-policy RL algorithm? Are there any specific reasons behind this choice?'",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies that the TD3 algorithm choice lacks justification, the reasoning is superficial. The review simply asks why TD3 was chosen over other off-policy RL algorithms but doesn't explain why this lack of justification is problematic or what the negative implications are. The ground truth indicates this was flagged as a 'major weakness' that would strengthen the work if addressed, suggesting deeper concerns about the paper's methodological rigor and comparative evaluation. The review treats it more as a minor clarification question rather than a significant methodological flaw that undermines the work's credibility."
    }
  },
  {
    "openreview_id": "4kVHI2uXRE",
    "reviewer_id": "syDC",
    "evaluation": {
      "flaw_id": "missing_multi_seed_evaluation",
      "is_flaw_mentioned": false,
      "mention_reasoning": "The review does not mention anything about random seeds, variance, statistical reliability, or multi-seed evaluation. While the review mentions that 'Additional ablation studies would enhance the persuasiveness of the findings' and asks for more performance evaluation on other datasets, it does not specifically address the critical issue of single-seed evaluation in RL experiments or the need for reporting results across multiple random seeds to establish statistical reliability.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "Since the flaw was not mentioned, there is no reasoning to evaluate. The review fails to identify this critical methodological flaw in RL evaluation. The ground truth emphasizes that multi-seed evaluation is essential for RL studies to measure variance and statistical reliability, and that this was a significant concern raised by reviewers and ACs. The generated review completely misses this important aspect of experimental rigor in reinforcement learning research, focusing instead on general suggestions for additional ablations and dataset evaluation without addressing the fundamental issue of reproducibility and statistical validity of the RL results."
    }
  },
  {
    "openreview_id": "fTKcqr4xuX",
    "reviewer_id": "G7Jo",
    "evaluation": {
      "flaw_id": "rss_explanation_insufficient",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions the RSS explanation issue in Weakness #2: 'The definition of RSS is not intuitive and lacks a sufficient explanation of how it represents the signal content of the noisy distribution relative to the clean distribution.' It also states 'A more thorough justification and explanation would strengthen the paper's theoretical foundation.' Additionally, in the Questions section, the reviewer asks about RSS definition and compares it to KL divergence, noting 'The definition of RSS does not seem intuitive.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns very well with the ground truth. The ground truth states that RSS 'is not intuitive and lacks a clear justification relative to more standard divergences such as KL.' The reviewer makes essentially the same point by noting that RSS is 'not intuitive' and provides a detailed comparison with KL divergence in the Questions section, showing how RSS and KL divergence give different interpretations of the same probability distributions. The reviewer also correctly identifies that this is problematic because 'all theoretical results in the paper are based on the RSS definition,' which matches the ground truth's assertion that RSS is what 'all theoretical results hinge' on. The reviewer appropriately calls for 'a more thorough justification and explanation,' which aligns with the ground truth mention that authors committed to adding clarification."
    }
  },
  {
    "openreview_id": "fTKcqr4xuX",
    "reviewer_id": "1ugH",
    "evaluation": {
      "flaw_id": "unfair_experimental_comparison",
      "is_flaw_mentioned": false,
      "mention_reasoning": "The review does not mention the specific issue of unfair experimental comparison where NI-ERM was evaluated with powerful frozen feature extractors while baselines were not given the same advantage. While the review does mention that 'More baselines should be included' and asks about performance comparisons under various noise levels, it does not identify the fundamental unfairness in the experimental setup regarding feature extractors.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "Since the review does not mention the specific flaw, it cannot provide correct reasoning about it. The ground truth flaw concerns a methodological issue where the proposed method (NI-ERM) had an unfair advantage through access to powerful frozen feature extractors that were not provided to baseline methods, making the comparison invalid. The review's criticism of the experimental section is more general, focusing on the need for more baselines and performance comparisons under different noise levels, but completely misses the core issue of experimental fairness in feature extractor usage."
    }
  },
  {
    "openreview_id": "fTKcqr4xuX",
    "reviewer_id": "G7Jo",
    "evaluation": {
      "flaw_id": "unfair_experimental_comparison",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the 'Experimental Validity Concerns' section under Weaknesses: 'The two-step method employs pre-trained feature extractors. While this demonstrates the effectiveness of NI-ERM, it may not provide a fair comparison with other methods. To more convincingly demonstrate the superiority of NI-ERM, the paper could: a) Compare NI-ERM using feature extractors from models trained with other SOTA noisy label methods. b) Apply SOTA methods to the same high-quality feature extractors used for NI-ERM, training only the final linear layer for a fairer comparison.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning closely aligns with the ground truth description. Both identify that the fundamental issue is unfair experimental comparison where NI-ERM benefits from powerful frozen feature extractors while baselines do not receive the same advantage. The review correctly explains why this is problematic (it doesn't provide a fair comparison) and suggests the same two solutions mentioned in the ground truth: (a) using feature extractors from other SOTA methods for NI-ERM, and (b) giving baseline methods access to the same high-quality feature extractors. The review also correctly identifies that this undermines the validity of claims about NI-ERM's superiority, matching the ground truth's concern about 'validity of empirical claims.'"
    }
  },
  {
    "openreview_id": "fTKcqr4xuX",
    "reviewer_id": "V7Mw",
    "evaluation": {
      "flaw_id": "unfair_experimental_comparison",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the weaknesses section: 'The author employs a highly trained pretrained model, which introduces an unfair factor in comparisons with other methods. Notably, there is experimental evidence suggesting that self-supervised pre-training can enhance the performance of existing methods... I recommend including the author's method applied to various pretrained models in Table 1, or adding the use of pretrained models for existing methods to provide a fair comparison.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer's reasoning aligns well with the ground truth. They correctly identify that using powerful pretrained models for NI-ERM while not providing the same advantage to baseline methods creates an unfair comparison. The reviewer explains why this is problematic (citing evidence that pre-training enhances existing methods) and provides concrete suggestions for remedying the issue: either (a) testing the proposed method with various pretrained models, or (b) giving baseline methods access to pretrained models for fair comparison. This matches the ground truth's description of the flaw and the suggested solutions from other reviewers."
    }
  },
  {
    "openreview_id": "KFmRMvzAZy",
    "reviewer_id": "Um3a",
    "evaluation": {
      "flaw_id": "false_positive_uncertainty",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions the false positive concern in the Weakness section: 'However, independent of the problem of false negatives, I wonder if false positives might be a bigger problem here. What if as a result of GCG algorithm, you are able to elicit a generation from a model which was never seen by the model during the training. I would suggest adding an experiment and some discussion around this.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. The reviewer correctly identifies that false positives occur when 'ACR flags strings the model never saw' (as stated in ground truth) by describing scenarios where 'you are able to elicit a generation from a model which was never seen by the model during the training.' The reviewer also appropriately suggests adding experiments to address this issue, which matches the ground truth mention of authors agreeing to 'add an experiment varying perplexity on unseen text.' While the review doesn't explicitly mention the implications for 'legal or compliance tool' reliability, it does characterize false positives as potentially 'a bigger problem' than false negatives, demonstrating understanding of the severity of this flaw."
    }
  },
  {
    "openreview_id": "74c9EOng9C",
    "reviewer_id": "R2od",
    "evaluation": {
      "flaw_id": "insufficient_ablation_seeds",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions this flaw in the Weaknesses section: 'The ablation studies only show one seed result, which reduces the results' plausibility, especially for Figure 5 (a), where the learning curves are quite noisy.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies why this is a flaw by explaining that showing only one seed result 'reduces the results' plausibility.' This aligns well with the ground truth description's emphasis on 'concerns about statistical reliability.' The reviewer also specifically mentions Figure 5(a) and notes that the learning curves are 'quite noisy,' which demonstrates understanding that single-seed results are particularly problematic when there is high variance in the data. The reasoning correctly captures the core issue of statistical reliability that makes this a methodological weakness."
    }
  },
  {
    "openreview_id": "pASJxzMJb7",
    "reviewer_id": "fUiC",
    "evaluation": {
      "flaw_id": "incomplete_theoretical_proof",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions the theoretical proof issue in the Questions section: 'Also, in the appendix, the proof shows $∥w∥ ≈ 2KL(p(·\\mid w')∥p(· | w))$. Is this a typo, or does the proof work for both cases?' This directly identifies the inconsistency between the stated theorem and the proof in the appendix that the ground truth describes.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the reviewer correctly identified the specific inconsistency between the theorem statement and the appendix proof (KL divergence notation discrepancy), they framed it merely as a potential 'typo' rather than recognizing it as a fundamental theoretical flaw. The ground truth emphasizes that this is an 'incomplete theoretical proof' where 'correct theoretical justification is essential for the paper's core claims.' The reviewer's reasoning lacks the depth to understand that this inconsistency undermines the paper's theoretical foundation - they treat it as a minor clarification question rather than a serious methodological concern that requires correction for the paper's validity."
    }
  },
  {
    "openreview_id": "pASJxzMJb7",
    "reviewer_id": "9YCm",
    "evaluation": {
      "flaw_id": "insufficient_empirical_scope",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions the flaw in the Weaknesses section: 'Limited evaluation on word-level static embeddings: one language studied, one dataset/vocabulary and its frequencies.' Additionally, in the Questions section, the reviewer asks about token-level embeddings and mentions the importance for 'low-resource languages,' showing awareness of the multilingual limitation. The Limitations section also encourages authors to 'add the limitations section addressing their empirical analysis scope.'",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies the narrow empirical scope (one language, one dataset/vocabulary), it fails to capture the full depth of the ground truth flaw. The ground truth describes a comprehensive limitation involving both word-level AND sentence-level evaluation gaps, with specific mention that 'no word-level evaluation' was conducted and that 'a broader, multi-task (word & sentence level) and possibly multilingual evaluation is thus required for publishability.' The review only briefly mentions 'word-level static embeddings' limitation but doesn't discuss the sentence-level tasks or explain why this limited scope is problematic for the paper's publishability and generalizability claims. The reasoning is superficial compared to the ground truth's detailed description of why this constitutes a major limitation requiring comprehensive multi-task and multilingual evaluation."
    }
  },
  {
    "openreview_id": "pASJxzMJb7",
    "reviewer_id": "fUiC",
    "evaluation": {
      "flaw_id": "insufficient_empirical_scope",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review clearly mentions the core aspects of this flaw. It notes that 'experiments are run on a sentence-level similarity tasks' and questions 'Why not run word-level evaluation metrics?' It also identifies the limited scope by asking 'What are these tasks exactly?' and 'Why are you doing sentence-level similarity with word-level embeddings? Why not either: (i) word-level similarity task; (ii) sentence-level embeddings.' The review explicitly calls out the narrow experimental scope.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. It correctly identifies that the experiments are limited to sentence-level tasks when word-level evaluation would be more appropriate given that the proposed method operates at the word level. The reviewer asks for word-level similarity tasks and questions why sentence-level evaluation is being used for word-level embeddings. This matches the ground truth criticism about 'no word-level evaluation' and the need for 'multi-task (word & sentence level)' evaluation. The review also implicitly questions the limited task coverage by asking for better description of the tasks used. While it doesn't explicitly mention the single vocabulary source or multilingual aspects, it captures the core reasoning that the experimental scope is too narrow and inadequate for properly evaluating the proposed method."
    }
  },
  {
    "openreview_id": "SxRblm9aMs",
    "reviewer_id": "WeYu",
    "evaluation": {
      "flaw_id": "insufficient_scalability_analysis",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions scalability concerns in the Questions section: 'Can you elaborate on the scalability of OptGNN for very large graphs or more complex real-world problems? What are the practical limitations in terms of computational resources and time?' This directly addresses the lack of scalability analysis that is described in the ground truth flaw.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review does mention scalability as a concern by asking about it in the questions section, it does not identify this as a specific weakness or flaw in the paper's current analysis. The ground truth describes that the paper lacks 'a comprehensive scalability discussion' and 'clearer, quantitative analysis of how OptGNN scales with graph size and clause-to-variable ratios.' However, the reviewer frames this as a question for clarification rather than critiquing the paper for insufficient analysis. The reviewer does not explain why the lack of scalability analysis is problematic or identify it as a methodological weakness that should have been addressed in the paper."
    }
  },
  {
    "openreview_id": "SxRblm9aMs",
    "reviewer_id": "zqcP",
    "evaluation": {
      "flaw_id": "insufficient_scalability_analysis",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions the scalability flaw in the weaknesses section: 'The paper lacks a comprehensive analysis of how well OptGNN scales with increasing graph sizes and complexity.' Additionally, the first question asks: 'Could you clarify how the proposed approach scales with increasingly large problem instances, particularly regarding computational complexity and performance stability?'",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies that scalability analysis is missing, the reasoning is superficial compared to the ground truth. The ground truth specifies that reviewers asked for 'quantitative analysis of how OptGNN scales with graph size and clause-to-variable ratios' and notes that authors only provided 'informal complexity estimates' in rebuttal, admitting the comprehensive scalability discussion was lacking. The generated review only makes a general statement about missing scalability analysis without mentioning the specific aspects (quantitative analysis, clause-to-variable ratios) or the context that this was raised by multiple reviewers and acknowledged by the authors. The reasoning lacks the depth and specificity described in the ground truth."
    }
  },
  {
    "openreview_id": "4G2DN4Kjk1",
    "reviewer_id": "D4Ej",
    "evaluation": {
      "flaw_id": "limited_experimental_scope",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions the experimental limitations in the weaknesses section: 'Additionally, the experiments are not sufficient for this work. Many aspects should be checked, and only a comparison with one baseline cannot support the claims made in the paper. Several other works in this area can be used for comparison, even if they have been proposed only for specific cases.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies the core issues described in the ground truth: (1) insufficient experimental validation with only one baseline comparison, (2) lack of adequate support for the broad claims made in the paper, and (3) the need for additional comparative baselines. The reviewer's reasoning aligns well with the ground truth description that highlights the 'thin' empirical section using 'only one synthetic baseline' that 'cannot substantiate the broad practical claims.' The review goes beyond just noting the omission by explaining the negative impact - that a single baseline comparison is insufficient to support the paper's claims and that more comprehensive experiments are needed for proper validation."
    }
  },
  {
    "openreview_id": "4G2DN4Kjk1",
    "reviewer_id": "pdPW",
    "evaluation": {
      "flaw_id": "limited_experimental_scope",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions the experimental limitation in the 'More comments' section: 'A real data experiment would be nice, to evidence this work as that of potential practical significance, but this is not necessary and I still find the work strong and a significant contribution without it.' Additionally, in the Questions section, the reviewer asks 'What are the barriers to implementing/trying this on real data?'",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review does mention the absence of real data experiments, the reasoning is incomplete and does not align with the severity described in the ground truth. The ground truth indicates that multiple reviewers found the empirical section 'too thin' with 'only one synthetic baseline and no real-data evaluation' which 'cannot substantiate the broad practical claims,' and that the authors recognized this as a 'critical limitation.' However, the generated review treats this as a nice-to-have rather than a critical flaw, stating it's 'not necessary' and that the work is still 'strong' without it. The review fails to identify that the limited experimental scope undermines the paper's practical claims or that it represents a fundamental weakness in validation."
    }
  },
  {
    "openreview_id": "4G2DN4Kjk1",
    "reviewer_id": "oZju",
    "evaluation": {
      "flaw_id": "hyperparameter_tuning_guidance",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the Weaknesses section: 'There are too many hyper-parameters in Algorithm 1, which are provided to the algorithm as input. However, I found the discussion on how to tune these hyper-parameters in practice to be lacking. It would be beneficial to include more detailed guidelines or heuristics for selecting appropriate values for these hyper-parameters, potentially based on empirical studies or theoretical insights.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning correctly aligns with the ground truth description. The reviewer not only identifies that Algorithm 1 has many hyperparameters with insufficient tuning guidance, but also explains why this is problematic by suggesting the need for 'more detailed guidelines or heuristics for selecting appropriate values.' While the review doesn't explicitly use the terms 'reproducibility' and 'practical adoption' mentioned in the ground truth, the concern about lacking practical tuning guidance directly implies these issues. The reviewer's suggestion for 'empirical studies or theoretical insights' to guide hyperparameter selection demonstrates understanding of the practical implications of this flaw."
    }
  },
  {
    "openreview_id": "l5wEQPcDab",
    "reviewer_id": "k5eY",
    "evaluation": {
      "flaw_id": "strong_full_support_assumption",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions Assumption 2.2 in the Questions section: 'Assumption 2.2 is rather strong, because you assume that the transition model is such that, for any arbitrary policy, each state is visited with a fixed non-zero probability. Do you agree?' This directly refers to the full-support assumption described in the ground truth.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies why Assumption 2.2 is problematic. The reviewer accurately describes the assumption as requiring that 'for any arbitrary policy, each state is visited with a fixed non-zero probability' and explicitly calls it 'rather strong.' This aligns well with the ground truth description that characterizes it as 'unrealistically strong' and notes it 'may fail in realistic settings.' The reviewer demonstrates understanding that this assumption about state visitation probabilities is overly restrictive, though they phrase it as a question rather than a definitive critique. The reasoning captures the core issue that such universal state coverage by any policy is unrealistic."
    }
  },
  {
    "openreview_id": "l5wEQPcDab",
    "reviewer_id": "hiXS",
    "evaluation": {
      "flaw_id": "finite_discrete_scope",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in multiple places: (1) In the Questions section: 'I would appreciate more discussion on how you would expect similar results to generalize to continuous state and action spaces' and (2) In the Limitations section: 'The results in the paper are limited to only finite state and action spaces'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies why this limitation is problematic. In the Questions section, the reviewer explicitly connects the limitation to the paper's broader aspirations (noting the title 'Towards the Transferability...') and asks for discussion on how the finite case serves as a step toward 'more practically useful results,' directly addressing the scope limitation's impact on practical applicability. The reviewer also correctly identifies this as 'key for assessing the importance of this paper,' showing understanding that the restricted scope limits the work's significance and generalizability, which aligns with the ground truth description of how this limitation affects the study's scope and practical relevance."
    }
  },
  {
    "openreview_id": "fYa6ezMxD5",
    "reviewer_id": "6C9L",
    "evaluation": {
      "flaw_id": "attention_overclaim",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions this flaw in the Questions section: 'At line 40, the authors claim that Matformer is applied to both attention and FFN blocks. However, Figure 1 shows that this is only applied to the hidden state of the FFN, and also later sections in the work (Section 3 where the scheme is presented, and Section 4 where results are reported) only use Matformwer on the FFN blocks. It seems like this in an overclaim in the introduction which should be removed.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns very well with the ground truth description. The reviewer correctly identifies that: (1) the introduction claims MatFormer applies to both attention and FFN blocks, (2) the actual method and experiments only cover FFN blocks, (3) this constitutes an 'overclaim in the introduction', and (4) it should be removed unless properly substantiated. The reviewer goes further to ask specific technical questions about how attention implementation would work, demonstrating understanding of why the claim is problematic. The reasoning captures the essence of misrepresentation of scope and contributions mentioned in the ground truth, though using slightly different terminology ('overclaim' vs 'over-claim')."
    }
  },
  {
    "openreview_id": "JHg9eNuw6p",
    "reviewer_id": "Te9g",
    "evaluation": {
      "flaw_id": "retrieval_limited_diversity",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in multiple places: 1) In Strengths section: 'It points out that the current method retrieves furniture and large objects from datasets, limiting the diversity.' 2) In Weaknesses section: 'The paper seems to be using retrieval models or text-to-3D models after acquiring coarse 3D information about an object, which may be unfaithful to the appearance cue provided by the diffusion model.' 3) In Limitations section: 'The diversity of retrieval results is limited.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies both key aspects of the flaw described in the ground truth: (1) Limited diversity due to retrieval from existing databases - explicitly mentioned as 'limiting the diversity' and 'diversity of retrieval results is limited', and (2) Faithfulness issues with 2D diffusion appearance - captured in 'may be unfaithful to the appearance cue provided by the diffusion model.' The reviewer also correctly notes this as a significant limitation that affects the method's claims, aligning with the ground truth's emphasis on this being 'a critical weakness that must be addressed for full open-vocabulary scene generation claims.'"
    }
  },
  {
    "openreview_id": "JHg9eNuw6p",
    "reviewer_id": "YmnU",
    "evaluation": {
      "flaw_id": "retrieval_limited_diversity",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in weakness (ii): 'The proposed method essentially enables generating open-vocabulary scenes, but it currently retrieves large furniture pieces from databases.' Additionally, in question (i), the reviewer asks about the rationale for retrieval: 'The proposed method generates small objects using a text-to-3D generation method... but it retrieves large furniture pieces instead of such generation... What is the reason for the retrieval?'",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies that the method retrieves large furniture from databases rather than generating it, the reasoning about why this is problematic is incomplete and superficial. The ground truth describes specific negative impacts: (1) restricts object diversity and (2) can break faithfulness to the 2D diffusion appearance. The review only briefly mentions that this approach is used 'but' doesn't explain why this is a limitation. The reviewer asks about the rationale but doesn't analyze the negative implications. The ground truth emphasizes this as a 'critical weakness' that undermines 'full open-vocabulary scene generation claims,' but the review treats it as a minor observation rather than a fundamental limitation."
    }
  },
  {
    "openreview_id": "JHg9eNuw6p",
    "reviewer_id": "4D8M",
    "evaluation": {
      "flaw_id": "insufficient_embodied_evaluation",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in weakness #2: 'In 4.2, the paper presents a quite simple example for embodied tasks, this is an important application scenario for 3D scene generation, more results are required to verify the effectiveness of the proposed methods can be used for embodied tasks.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth. It correctly identifies that: (1) the embodied task demonstration in Section 4.2 is 'quite simple', which matches the ground truth's description of it being 'far too simple', (2) embodied tasks are 'an important application scenario for 3D scene generation', indicating the reviewer understands the significance of this evaluation gap, and (3) 'more results are required to verify the effectiveness' for embodied tasks, which directly corresponds to the ground truth's point about inadequate evidence for downstream task utility. The reviewer demonstrates understanding of why this is problematic - the simple example is insufficient to substantiate claims about the method's usefulness for the important robotics/embodied-AI domain."
    }
  },
  {
    "openreview_id": "JHg9eNuw6p",
    "reviewer_id": "Te9g",
    "evaluation": {
      "flaw_id": "insufficient_embodied_evaluation",
      "is_flaw_mentioned": false,
      "mention_reasoning": "The review does not mention that the embodied-task demonstration in Section 4.2 was too simple or inadequate to substantiate the paper's claims for robotics/embodied-AI usefulness. While the review mentions embodied AI tasks in positive contexts (e.g., 'friendly for embodied AI tasks', 'important for embodied AI research like object manipulation tasks'), it does not identify insufficient evaluation of embodied tasks as a weakness or flaw.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "Since the flaw was not mentioned, there is no reasoning to evaluate. The review actually suggests the opposite of the ground truth flaw - it views the work positively for embodied AI applications, stating that the generated scenes 'seem reasonable and friendly for embodied AI tasks' and that fine-grained object generation is 'important for embodied AI research like object manipulation tasks'. The review completely misses the critical issue identified in the ground truth: that the embodied evaluation was insufficient and too simple to support the paper's claims about robotics/embodied-AI utility."
    }
  },
  {
    "openreview_id": "JHg9eNuw6p",
    "reviewer_id": "yxqm",
    "evaluation": {
      "flaw_id": "insufficient_embodied_evaluation",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in the Summary: 'The scenes generated by the method could be useful e.g. for robotics and for training embodied agents, although such experiments are not presented.' It is also mentioned in the Weaknesses section: 'Several of the closest related works, with two examples being Holodeck [Yang 2024c] and procTHOR [Deitke 2022], evaluate how agents for e.g. object navigation perform after training on their proposed datasets, and evaluate how it generalizes to other datasets. Such policy learning experiments are missing from the current paper.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies why this is a flaw by: (1) noting that the paper claims utility for robotics and embodied AI but doesn't provide supporting experiments, (2) comparing to related works (Holodeck, procTHOR) that do include proper embodied evaluation with policy learning experiments, and (3) specifically pointing out that 'policy learning experiments are missing.' This aligns well with the ground truth description that the embodied-task demonstration was judged as 'far too simple to substantiate the paper's claim of usefulness for Robotics/Embodied-AI' and that 'evidence for downstream task utility remains inadequate.' The reviewer understands that without proper evaluation, the claimed applications cannot be substantiated."
    }
  },
  {
    "openreview_id": "JHg9eNuw6p",
    "reviewer_id": "4D8M",
    "evaluation": {
      "flaw_id": "missing_inpainting_ablation",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in the first weakness: 'The effectiveness of the proposed hierarchical inpainting is not well verified, without the control of *3D constraints, how is the quality of the generated scenes?*' This directly addresses the lack of ablation study comparing hierarchical inpainting with 3D constraints versus using the diffusion model alone.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth. The reviewer correctly identifies that the paper lacks verification of the effectiveness of the hierarchical inpainting approach and specifically questions what happens without the 3D constraints. This matches the ground truth's concern about whether hierarchical inpainting with 3D constraints genuinely improves scene quality versus using the diffusion model alone. The reviewer understands that without proper comparative analysis, the core methodological contribution cannot be validated, which is exactly the issue described in the ground truth."
    }
  },
  {
    "openreview_id": "MDgn9aazo0",
    "reviewer_id": "SRyH",
    "evaluation": {
      "flaw_id": "computational_scalability",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions computational scalability concerns in multiple places: In the Weaknesses section: 'The computational efficiency of CCM, especially in large-scale applications, is not extensively discussed.' In the Limitations section: 'The limitation of the Channel Clustering Module (CCM) outlined in the paper includes its scalability to extremely large datasets and the computational overhead introduced by the clustering and embedding processes' and 'the clustering and embedding processes in CCM introduce additional computational overhead, which could be a concern in scenarios where computational efficiency is critical.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning correctly aligns with the ground truth description. The ground truth indicates that reviewers questioned CCM's runtime/memory overhead and scalability to high-dimensional or very large datasets, which the authors acknowledged as a major limitation. The generated review identifies the same core issues: (1) scalability to large datasets, (2) computational overhead from clustering and embedding processes, and (3) concerns about efficiency in resource-constrained scenarios like real-time forecasting. The review correctly explains why this is problematic - it could limit the method's applicability in scenarios where computational efficiency is critical, which matches the ground truth's emphasis on this being a 'major limitation.'"
    }
  },
  {
    "openreview_id": "MDgn9aazo0",
    "reviewer_id": "rxdY",
    "evaluation": {
      "flaw_id": "computational_scalability",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions computational scalability concerns in Weakness #2: 'I noticed that CCM introduces additional model complexity, as it has an independent Feed Forward layer for each cluster. When the value of K is large, this may result in an excessive number of Feed Forward layers, leading to a significant increase in space complexity. On the other hand, the time complexity of CCM is linearly related to K and C. For certain datasets with a higher number of channels (e.g., Traffic), CCM may noticeably increase the time complexity of the base model.' The reviewer also asks in Question #1: 'Can the author provide the percentage increase in runtime and memory usage after integrating CCM for each method on the Traffic dataset?'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth. The reviewer correctly identifies both runtime and memory overhead issues, explains the technical reasons (independent Feed Forward layers for each cluster, linear complexity relationship with K and C), and specifically mentions scalability concerns for high-dimensional datasets (using Traffic dataset as an example). The reviewer also requests specific quantitative data about runtime and memory usage increases, which directly addresses the computational efficiency concerns mentioned in the ground truth. The reasoning goes beyond just noting the problem - it explains the underlying causes and potential implications for practical deployment."
    }
  },
  {
    "openreview_id": "MDgn9aazo0",
    "reviewer_id": "yXcm",
    "evaluation": {
      "flaw_id": "computational_scalability",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions the flaw in multiple places: In Weaknesses section: 'The introduction of CCM increases the model's complexity, particularly for original CD models, which may result in higher computational overhead' and 'The paper acknowledges that the scalability of CCM to extremely large datasets remains untested, which could be a limitation for practical applications requiring the processing of large-scale data.' In Questions section: 'What techniques can be implemented to mitigate the increased computational overhead introduced by CCM, especially for models with high channel and cluster counts?' In Limitations section: 'The increased model complexity introduced by CCM may lead to higher computational costs, particularly for models with numerous channels and clusters.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth. It correctly identifies both aspects of the computational scalability flaw: (1) runtime/memory overhead - mentioned as 'higher computational overhead' and 'higher computational costs', and (2) scalability concerns for large datasets - explicitly stated as 'scalability of CCM to extremely large datasets remains untested'. The review also correctly explains why this is problematic, noting it 'could be a limitation for practical applications requiring the processing of large-scale data' and specifically mentions concerns about 'models with high channel and cluster counts', which corresponds to the ground truth's mention of 'high-dimensional or very large datasets'. The reasoning demonstrates understanding of the practical implications of the scalability issues."
    }
  },
  {
    "openreview_id": "hKloKv7pR2",
    "reviewer_id": "iRBG",
    "evaluation": {
      "flaw_id": "ambiguous_state_distribution",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in weakness #1: 'The authors formulate offline RL as a partial optimal transport problem between $d^\\beta$ and $\\beta(\\cdot\\mid s)$ by regarding the policy $\\pi$ as a transport map. Since the problem depends on the choice of $s$ in $\\beta(\\cdot\\mid s)$, the resulting policy $\\pi$ will also depend on it. This does not make much sense.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies the core issue described in the ground truth. The reviewer recognizes that the formulation creates a dependency on the choice of state $s$, which leads to the policy depending on that choice - this aligns with the ground truth's description of 'one transport map per state and therefore multiple conflicting policies.' The reviewer correctly concludes this 'does not make much sense,' which matches the ground truth's characterization of this as causing 'major confusion' and being an incorrect formulation. While the review doesn't explicitly mention the notation issue (d(s) vs δ(s)) or use the exact phrase 'multiple conflicting policies,' it demonstrates the same fundamental understanding of why this formulation is problematic."
    }
  },
  {
    "openreview_id": "hKloKv7pR2",
    "reviewer_id": "iRBG",
    "evaluation": {
      "flaw_id": "flawed_toy_experiment_reward",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions the toy experiment flaw in weakness #2: 'The explanation of the environment used in the toy example is unclear. Line 193 states that the final state yields a reward of 1, while other states have a zero reward. Then, what would necessitate the agent to seek the shortest path? The reward seems independent of the agent's action.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer correctly identifies the core issue described in the ground truth: that the reward structure (reward only in terminal state with fixed episode length) makes the experiment incapable of demonstrating the claimed advantage. The reviewer astutely questions 'what would necessitate the agent to seek the shortest path?' which directly captures the ground truth concern that 'the discounted return was constant for every possible path—making the experiment incapable of demonstrating the claimed advantage.' The reviewer correctly understands that if all paths yield the same return, there's no incentive for optimization and the experiment fails to prove its point."
    }
  },
  {
    "openreview_id": "bNDwOoxj6W",
    "reviewer_id": "Gx9Z",
    "evaluation": {
      "flaw_id": "insufficient_motivation_and_novelty_explanation",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in multiple places: In Weakness #1: 'The importance of results and their technical novelty are not clearly discussed.' In Question #2: 'Could you elaborate on the importance of the results and the novelty of proofs? The reductions seem very natural.' The reviewer directly identifies both components of the flaw - lack of clear importance/motivation and unclear technical novelty.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. The reviewer correctly identifies that both the importance of results and technical novelty are not clearly conveyed, which matches the ground truth stating that 'the paper does not clearly convey the importance of its results or what is technically novel.' The reviewer even echoes the specific concern mentioned in the ground truth that 'the reductions look very natural' by stating 'The reductions seem very natural.' While the review doesn't explicitly mention the negative impact on reader understanding or the need for camera-ready fixes, it does correctly identify the core issue and asks for elaboration, demonstrating understanding of why this is problematic for the paper's contribution."
    }
  },
  {
    "openreview_id": "bNDwOoxj6W",
    "reviewer_id": "WjE6",
    "evaluation": {
      "flaw_id": "insufficient_motivation_and_novelty_explanation",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in the Weaknesses section: 'The main weakness (other than those mentioned in the Questions and Limitations sections below) is the clarity of the non-technical writing. There is not much to guide the reader through the technical parts, so I worry non-experts will have too much difficulty to see the value of these theoretical results.' Additionally, in the Limitations section: 'Limitations... are clear from a detailed reading, but they could (and should) be more explicitly mentioned as limitations of the work in, e.g., the Intro.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth flaw. The ground truth indicates that reviewers found the paper did not clearly convey the importance of its results or what is technically novel, requiring clearer motivation and novelty discussion. The generated review identifies the same core issue: poor clarity in non-technical writing that makes it difficult for non-experts to see the value of the results, and insufficient explicit discussion of limitations in the introduction. The review correctly identifies that this creates accessibility problems for readers trying to understand the significance and novelty of the work, which matches the ground truth concern about unclear communication of importance and technical novelty."
    }
  },
  {
    "openreview_id": "PqlKliEXyJ",
    "reviewer_id": "nkE1",
    "evaluation": {
      "flaw_id": "requires_location_and_gravity_prior",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in multiple places: In Weaknesses #3: 'The reliance on a coarse pose provided by the UAV sensor as an initial input may affect the method's robustness in scenarios where the initial pose is significantly off.' In Questions: 'Is it possible that the method's performance might degrade in environments with poor GPS signal or where the UAV is operating in areas without prior map knowledge?' In Limitations #1: 'The method assumes the availability of a gravity direction and location prior, which might not always be accurate or available, especially in GPS-denied environments.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. It correctly identifies that the method requires gravity direction and location priors (GPS/IMU), and explains why this is problematic: it affects robustness when initial pose is significantly off, causes performance degradation in GPS-denied environments, and limits practical applicability. The review captures the core issue that the method's robustness outside of scenarios with accurate priors remains unverified, which matches the ground truth concern about broadening the method's practical applicability."
    }
  },
  {
    "openreview_id": "7HFQfRjdcn",
    "reviewer_id": "ywDS",
    "evaluation": {
      "flaw_id": "incorrect_theorem_proof",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions the flaw in Theorem 3.7's proof: 'First the proof of Theorem 3.7. is wrong; I think what the authors meant in the argument of the Theorem is $\\|u(\\theta) - u(\\theta + \\epsilon)\\|$ rather than their dot product (the dot product claim is obviously wrong since if epsilon goes to zero right hand side goes to zero while left hand side is constant). even with $\\|u(\\theta) - u(\\theta + \\epsilon)\\|$ instead, the proof in appendix is wrong; the correct formula of length given metric $M_\\theta$ on a manifold is...'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. Both identify that the proof of the main stability theorem is mathematically incorrect due to flawed metric arguments. The reviewer provides specific technical details about why the proof fails: (1) the dot product claim is wrong because of asymptotic behavior, and (2) the manifold length formula used is incorrect. The reviewer also notes that 'the final argument is expected to be true, perhaps with different constants,' suggesting the theorem could be salvaged with proper proof, which matches the ground truth's mention that authors promised a corrected proof. The reviewer correctly identifies this as a fundamental issue undermining the theoretical foundation, similar to how the ground truth emphasizes this theorem 'underlies the paper's core claim' about bounded boundary shifts."
    }
  },
  {
    "openreview_id": "QVSP1uk7b5",
    "reviewer_id": "dtJc",
    "evaluation": {
      "flaw_id": "missing_video_consistency",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in weakness #1: 'For the task of 3D Reconstruction, 3D Consistency is highly important. However, lack of videos of the generated result is a major drawback that highly affects my decision since just two views are not enough to judge the quality of the output and created mesh. This is also a standard in the baseline papers such as DreamFusion that the authors compare with.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns very well with the ground truth description. The reviewer correctly identifies that: (1) 3D consistency is crucial for 3D reconstruction tasks, (2) two static views are insufficient to assess this consistency, (3) videos/multi-view evidence are needed to properly evaluate the method, and (4) this is a standard requirement established by prior work like DreamFusion. The reviewer also notes this significantly impacts their evaluation decision, matching the severity described in the ground truth. The reasoning goes beyond simply noting an omission and explains why this affects the ability to properly assess the method's 3D consistency."
    }
  },
  {
    "openreview_id": "5GCgNFZSyo",
    "reviewer_id": "Zw3g",
    "evaluation": {
      "flaw_id": "missing_convergence_for_la_minucb",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in the Weaknesses section: 'The introduction of the Lookahead strategy in 7 is a bit out of the blue and not well connected to the remaining paper; and does not come with theoretical guarantees.' Additionally, in the Limitations section, the reviewer states: 'The paper briefly discusses limitations, mainly the missing analysis of the lookahead approach.'",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies that the lookahead (LA-MinUCB) approach lacks theoretical guarantees, the reasoning is superficial and misses the deeper implications outlined in the ground truth. The review treats this as a minor organizational issue ('out of the blue') rather than recognizing it as a major weakness that undermines the paper's claim that the algorithm is principled rather than heuristic. The ground truth emphasizes that LA-MinUCB is the paper's strongest empirical performer, making the lack of theoretical convergence guarantees a critical flaw that leaves key claims unsupported. The review fails to connect the missing analysis to the broader impact on the paper's theoretical contributions and the significance of having the best-performing algorithm be theoretically ungrounded."
    }
  },
  {
    "openreview_id": "NrwASKGm7A",
    "reviewer_id": "i3ot",
    "evaluation": {
      "flaw_id": "missing_evaluation_of_early_steps",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions this flaw in the weaknesses section: 'My biggest concern for this paper is that the model essentially does three things: Factual Existence Judgment, Reference Information Extraction, and Hallucination-Type Judgment. Each step relies on the results from previous step. For example, if the extracted reference information is limited, it will greatly impact the hallucination judgement as well. The experiments mainly focus on the third step without discussions on the previous two steps. So I'm not certain how stable the model actually is.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. The reviewer correctly identifies that: (1) the three-phase pipeline exists (Factual Existence Judgment → Reference Information Extraction → Hallucination-Type Judgment), (2) each step depends on previous steps, (3) experiments only focus on the final step without evaluating the first two steps, and (4) this creates uncertainty about system stability. The reviewer's concern about stability matches the ground truth's emphasis that without evaluating early steps, we cannot determine if the system is reliable enough to be stable. While the review doesn't mention the rebuttal acknowledgment or publication readiness aspects from the ground truth, it captures the core technical issue and its implications accurately."
    }
  },
  {
    "openreview_id": "NrwASKGm7A",
    "reviewer_id": "4rBJ",
    "evaluation": {
      "flaw_id": "insufficient_em_justification_and_convergence_analysis",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions issues with the EM algorithm in the weaknesses section: 'One of the big drawbacks of the EM algorithm is its sensitivity to initial conditions. If you don't start with the right parameters, the algorithm can easily get stuck in a local maximum instead of finding the best possible solution.' This directly addresses the sensitivity to initialization mentioned in the ground truth flaw.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies EM's sensitivity to initialization (mentioning local maxima issues), it fails to address the core problems outlined in the ground truth: (1) lack of justification for why EM was chosen as the algorithm, (2) absence of convergence analysis, and (3) lack of empirical or theoretical evidence that EM actually converges in this specific application. The review treats these as general algorithmic limitations rather than identifying them as missing methodological components that undermine the paper's core self-training claims. The ground truth emphasizes these are critical gaps that need rigorous justification and evidence, but the review only discusses general EM drawbacks without connecting them to the paper's insufficient treatment of these issues."
    }
  },
  {
    "openreview_id": "NrwASKGm7A",
    "reviewer_id": "5wgD",
    "evaluation": {
      "flaw_id": "insufficient_em_justification_and_convergence_analysis",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions both key components of the flaw: (1) 'The rationale behind using the EM algorithm to solve this problem is not clearly articulated. What considerations led to this choice?' and (2) 'How does the paper ensure that the EM algorithm converges through iterations?' These questions appear in both the Weaknesses and Questions sections.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies that the EM algorithm choice lacks justification and that convergence is not addressed, it fails to explain why these omissions are problematic. The ground truth emphasizes that these issues are 'critical to support the core self-training claim' and that the lack of 'rigorous justification and empirical or theoretical evidence of EM convergence' undermines the paper's fundamental contribution. The review treats these as simple questions rather than serious methodological flaws that compromise the validity of the entire approach. It doesn't explain the negative implications for reproducibility, theoretical soundness, or the credibility of the self-training framework."
    }
  },
  {
    "openreview_id": "zO55ovdLJw",
    "reviewer_id": "64zT",
    "evaluation": {
      "flaw_id": "unclear_prompt_design",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions unclear technical descriptions in weakness #1: 'The paper lacks a detailed explanation or discussion on the efficacy of different prompt designs' and weakness #2: 'The paper lacks visualization of each learnable prompt (e.g., deep correlated prompts, dynamic prompts, and common prompts).' These comments directly address the clarity issues with prompt types and framework description identified in the ground truth.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies why the unclear prompt design is problematic. It explains that without detailed explanations, readers cannot validate whether improvements are genuine or just due to more parameters (weakness #1), and without visualizations, it's impossible to verify if components work as expected (weakness #2). This aligns well with the ground truth's emphasis on interpretability and reproducibility issues. The reviewer also notes the lack of discussion makes it hard to understand individual component contributions, which matches the ground truth's concern about the core contribution being 'hard to interpret and reproduce.'"
    }
  },
  {
    "openreview_id": "zO55ovdLJw",
    "reviewer_id": "nnhg",
    "evaluation": {
      "flaw_id": "unclear_prompt_design",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions clarity issues with the prompt design in multiple places: 'The main weakness of the paper is clarity. There are three different sets of prompts that are appended to the intermediate representations. However, the only difference between them seems to be the type of architecture the method uses to compute them. The explanation is very limited and Figure 1 does not illustrate where do these prompts come from.' The reviewer also asks 'What are exactly correlated prompts, dynamic prompts, and modal-common prompts? What make them correlated, dynamic and modal-common? This is not clear in the paper at all.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth. The reviewer correctly identifies that the unclear technical descriptions of the prompt types make it 'really hard to understand how the motivation of each type of prompt fits the design' and later asks for clarification on 'the mechanism of the different types of prompts, input, output at train and test time for each one of them.' This directly corresponds to the ground truth's mention that the technical description of the three prompt types was judged to be insufficiently clear. The reviewer also implicitly addresses reproducibility concerns by requesting detailed explanations of the mechanisms, which aligns with the ground truth's note that 'without these clarifications the core contribution is hard to interpret and reproduce.'"
    }
  },
  {
    "openreview_id": "yVzWlFhpRW",
    "reviewer_id": "PubC",
    "evaluation": {
      "flaw_id": "distributional_mask_off_policy_bias",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in multiple places: 'I think the distributional mask algorithm is off-policy by nature, meaning it's use with on-policy methods like PPO is biased and will cause performance loss or divergence' and 'For the gradient of the distributional mask in proposition 4, isn't this affected by off-policyness due to the constrained sampling of actions from the policy distribution?'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer's reasoning aligns well with the ground truth. They correctly identify that: (1) the distributional mask is inherently off-policy when used with on-policy PPO, (2) this creates bias that can cause performance degradation or divergence, (3) this may explain the observed underperformance in two experimental tasks, and (4) the off-policy nature stems from constrained sampling that doesn't follow the true policy distribution. The reviewer also notes that this is a 'major issue' that should be addressed or the method should be omitted from on-policy contexts. This reasoning matches the ground truth's description of the flaw as creating intractable gradient terms, requiring approximations that introduce bias, and degrading performance in observed tasks."
    }
  },
  {
    "openreview_id": "yVzWlFhpRW",
    "reviewer_id": "T9Vb",
    "evaluation": {
      "flaw_id": "obtaining_state_specific_relevant_action_sets",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in multiple places: 'It is not clear how easy it is to recover the action masking criteria, especially under the more complex generator or distributional schemes, and it seems like this would be rare' and 'Some suggestions in the main work of how the distributional or generator action space restriction could be defined as a function of the dynamics could be relevant since it seems like these functions have to be hand-engineered, and it is not obvious how to do that in domains where the dynamics are less well defined.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies the core issue described in the ground truth - that obtaining the state-dependent convex relevant action sets is problematic for practical applicability. The reviewer explicitly states that recovering action masking criteria would be 'rare' and questions 'how easy it is to recover' them, which aligns with the ground truth concern about how 'realistic it is to acquire' these sets. The reviewer also correctly identifies that these functions appear to need to be 'hand-engineered' and notes the difficulty in domains 'where the dynamics are less well defined,' which directly corresponds to the ground truth description of this being a 'major challenge in practice' that 'seriously constrains' the framework's practical applicability."
    }
  },
  {
    "openreview_id": "fVRCsK4EoM",
    "reviewer_id": "85gD",
    "evaluation": {
      "flaw_id": "missing_comparative_experiments",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions missing comparisons in two places: 1) In weakness #1, it asks 'Why would this practice better than simply finetuning the model on high-quality (human prefered) inpainting data?' suggesting missing comparisons with direct fine-tuning approaches. 2) In weakness #2, it states 'There are missing citations and comparisons with the following methods:' and lists three specific inpainting methods, indicating awareness of missing comparative experiments.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies that comparisons are missing, the reasoning is incomplete and doesn't align well with the ground truth. The ground truth specifically mentions missing comparisons with 'recent diffusion-RL alignment approaches (e.g. Human Preference Score, ImageReward, DPOK, D3PO, diffusion-DPO)' and 'strong direct fine-tuning or state-of-the-art inpainting baselines,' and explains that this makes 'evidence for the paper's claimed superiority remains incomplete.' The review only mentions missing comparisons with three specific inpainting methods and briefly questions fine-tuning alternatives, but fails to identify the critical missing diffusion-RL alignment approaches or explain how this incompleteness undermines the paper's claims of superiority. The review's reasoning is more superficial, focusing on individual missing citations rather than the systematic gap in comparative evaluation that prevents validation of the method's claimed advantages."
    }
  },
  {
    "openreview_id": "fVRCsK4EoM",
    "reviewer_id": "G8Cx",
    "evaluation": {
      "flaw_id": "missing_comparative_experiments",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in Weakness 1, specifically stating: 'There has been a lot of work introducing human preferences into diffusion models, such as Human Preference Score (ICCV 2023), ImageReward (NeurIPS 2023), DPOK (NeurIPS 2023), and D3PO (CVPR 2024)' and 'It is necessary to supplement the comparison with these methods, including differences in methodology and advantages in experimental results.' The reviewer identifies the exact methods mentioned in the ground truth (Human Preference Score, ImageReward, DPOK, D3PO) and explicitly calls for comparative experiments.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. The reviewer correctly identifies that comparative experiments with recent diffusion-RL alignment approaches are missing and explains why this is problematic - specifically noting that 'It is necessary to supplement the comparison with these methods, including differences in methodology and advantages in experimental results.' This captures the essence of the ground truth concern that without these comparisons, the evidence for the paper's claimed superiority remains incomplete. The reviewer mentions the same specific methods (Human Preference Score, ImageReward, DPOK, D3PO) that are referenced in the ground truth, demonstrating accurate identification of the missing comparisons."
    }
  },
  {
    "openreview_id": "fVRCsK4EoM",
    "reviewer_id": "Hftn",
    "evaluation": {
      "flaw_id": "missing_comparative_experiments",
      "is_flaw_mentioned": false,
      "mention_reasoning": "The review does not mention the absence of head-to-head comparisons with recent diffusion-RL alignment approaches (Human Preference Score, ImageReward, DPOK, D3PO, diffusion-DPO) or comparisons with strong direct fine-tuning or state-of-the-art inpainting baselines. While the review discusses various technical issues and missing implementation details, it does not address the lack of comparative experimental evaluation against relevant baselines and competing methods.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "Since the flaw was not mentioned in the review, there is no reasoning to evaluate. The ground truth flaw specifically concerns the absence of comparative experiments with recent diffusion-RL alignment approaches and strong inpainting baselines, which would leave the evidence for the paper's claimed superiority incomplete. The generated review focuses on other issues like theoretical justification, implementation details, dataset annotation specifics, and hyperparameter selection, but completely misses this critical evaluation gap that multiple reviewers and the Area Chair identified as problematic."
    }
  },
  {
    "openreview_id": "fVRCsK4EoM",
    "reviewer_id": "Hftn",
    "evaluation": {
      "flaw_id": "limited_dataset_annotation_details",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in weakness point 5: 'There are details missing regarding the dataset annotation. Such as the number of annotators employed, whether training was required for them, and the amount of time each individual spent on the annotation task.'",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies that dataset annotation details are missing and even lists some specific missing information (number of annotators, training requirements, time spent), it fails to explain why this is problematic. The ground truth emphasizes that without these details, 'the validity and ethical soundness of the 51k-image human-preference dataset are not yet documented.' The review does not mention the implications for validity, ethical soundness, or the broader impact on the dataset's credibility. It simply notes the absence of information without explaining the negative consequences of this omission."
    }
  },
  {
    "openreview_id": "CeOwahuQic",
    "reviewer_id": "QQ9h",
    "evaluation": {
      "flaw_id": "persona_generation_bias",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in the Questions section: 'At page 3, the authors state that only GPT-4 is used to generate 53 types of personas. Why? Which was the outcome using other LLMs? Not realistic personas?' This directly addresses the concern that all personas were generated solely with GPT-4.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review identifies that only GPT-4 was used to generate personas and questions this choice, it fails to provide the critical reasoning found in the ground truth. The review asks about the outcome of using other LLMs but doesn't explain why this is problematic for the study's validity and generality. The ground truth emphasizes that persona quality may differ across LLMs, which could bias the study, and that this impacts the validity and generality of experimental results. The review treats this more as a curiosity ('Why?') rather than identifying it as a methodological flaw that undermines the study's conclusions. The reviewer doesn't connect this issue to broader concerns about experimental validity or generalizability across different LLM architectures."
    }
  },
  {
    "openreview_id": "waQ5X4qc3W",
    "reviewer_id": "K39P",
    "evaluation": {
      "flaw_id": "undefined_stability_metric",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in multiple places: 'The proposed stability metric is interesting, but it's unclear whether it will correlate with downstream generative models performance' and 'it's unclear, why it follows from the propositions that we should improve the stability of the latent space (where stability is also not defined well)' and in the Questions section: 'How exactly do theoretical results suggest that one should focus on stability of the latent space?'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer correctly identifies both key aspects of the ground truth flaw: (1) that the stability metric is not properly defined ('stability is also not defined well'), and (2) that its correlation with generative model performance is not established ('it's unclear whether it will correlate with downstream generative models performance'). The reviewer goes further to question the theoretical justification for focusing on latent space stability, which aligns with the ground truth concern about the lack of theoretical linkage. The reasoning demonstrates understanding of why this is problematic - without proper definition and validation, the core contribution lacks scientific rigor."
    }
  },
  {
    "openreview_id": "waQ5X4qc3W",
    "reviewer_id": "Vc7B",
    "evaluation": {
      "flaw_id": "undefined_stability_metric",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions the flaw in two key places: (1) 'The term \"stability\" or \"stabilize\" is a bit confusing. Explicit explanation is needed.' and (2) 'The metric is not very clearly defined. What's the name of the metric? What is the definition? How to compute it? All these information should be highlighted.' These directly address the undefined nature of the stability concept and metric.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies that the stability concept and metric are poorly defined, it fails to capture the deeper issue highlighted in the ground truth. The ground truth specifically mentions that the stability metric is not 'shown to correlate with generative-model performance' - this is a crucial aspect about the metric's validity and theoretical foundation that the review misses. The review treats this as primarily a presentation/clarity issue rather than recognizing the fundamental problem of lacking correlation with actual model performance, which would affect the method's theoretical justification and practical utility."
    }
  },
  {
    "openreview_id": "waQ5X4qc3W",
    "reviewer_id": "Sn2s",
    "evaluation": {
      "flaw_id": "ambiguous_first_evidence_claim",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions this flaw in section 1.2 under Factual Errors: 'In lines 72-73, the authors state that this work provides \"the first evidence that image autoregressive generative models behave analogously to GPT.\" However, the Parti[1] paper has already demonstrated that image autoregressive models have similar scalability to GPT and successfully scaled the model to 20B. The authors have not cited this work.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning correctly aligns with the ground truth description. It identifies that the claim of being 'the first evidence' is factually questionable by providing a specific counterexample (Parti paper) that already demonstrated similar scaling behavior in autoregressive image models. The reviewer correctly points out that Parti scaled to 20B parameters and showed similar scalability to GPT, which directly contradicts the novelty claim. The reasoning captures the core issue that the authors are overstating their novelty by claiming to be 'first' when prior work (specifically Parti) already provided such evidence. This matches the ground truth's note that the claim is 'factually questionable' with Parti as a specific example of prior similar work."
    }
  },
  {
    "openreview_id": "l5SbrtvSRS",
    "reviewer_id": "6PSY",
    "evaluation": {
      "flaw_id": "shared_initialization_dependency",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the Weaknesses section: 'The proposed method relies on shared initializations, which significantly limits its applicability as acknowledged by the authors.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies the core issue and its negative implications. It accurately notes that the method 'relies on shared initializations' and explains why this is problematic by stating it 'significantly limits its applicability.' The review also correctly acknowledges that the authors themselves recognize this limitation. While the review's explanation is more concise than the ground truth, it captures the essential reasoning - that the dependency on shared initializations creates a significant limitation that restricts the method's broader applicability. The reasoning aligns well with the ground truth's emphasis on how this restriction 'critically limits' the scope and 'must be addressed for broader applicability.'"
    }
  },
  {
    "openreview_id": "aAaV4ZbQ9j",
    "reviewer_id": "kzEx",
    "evaluation": {
      "flaw_id": "missing_full_loss_and_procedure",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions this flaw in the weaknesses section: 'I find the methodology section on its own quite unclear since it's not clear how to actually use the objective functions that are outlined. Appendix sections D4 and D5 are helpful but ideally it would be possible to follow the main text on its own. Particularly, I think the text would benefit from showing the full loss function in the methodology and including a short outline of the training and sampling procedures and referring to the appendix.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning correctly aligns with the ground truth description. The reviewer identifies that: (1) the methodology section is unclear on its own, (2) it's not clear how to use the objective functions, (3) the main text should be self-contained, and (4) specifically recommends showing the full loss function and including an outline of training and sampling procedures. This matches the ground truth's description that the complete training objective and self-contained outline of procedures are missing, making the methodology unclear. The reviewer correctly explains why this is problematic - it prevents readers from understanding the methodology without relying on appendix sections."
    }
  },
  {
    "openreview_id": "aAaV4ZbQ9j",
    "reviewer_id": "kzEx",
    "evaluation": {
      "flaw_id": "insufficient_baseline_comparisons",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the weaknesses section: 'The baselines for some tasks are a bit weak, particularly for the unconstrained molecular optimisation. For this task techniques like evolutionary algorithms and reinforcement learning fine-tuning have been proposed and widely used before. It would be very useful to see a comparison of ChemFlow with methods like these, as well as an evaluation of the training and sampling time for each.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. The reviewer correctly identifies that: (1) the baselines are weak specifically for unconstrained molecular optimization, (2) evolutionary algorithms and reinforcement learning fine-tuning methods (which matches the ground truth's mention of EA and REINVENT) are established and widely-used approaches that should be included, and (3) training/sampling time comparisons are missing. The reviewer explains why this is problematic by noting these are 'widely used' methods, making their omission a significant gap in evaluation comprehensiveness. This matches the ground truth's indication that these are 'strong, widely-used baselines' whose omission was problematic enough that the authors committed to adding them in the camera-ready version."
    }
  },
  {
    "openreview_id": "ctXYOoAgRy",
    "reviewer_id": "TbKY",
    "evaluation": {
      "flaw_id": "overgeneralized_claims",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in weakness W1: 'Most importantly, the claims are very broad (\"LLMs are this / LLMs do that\"), however this is verified on two very similar models: the included Mistral and Vicuna models both have 7 billion parameters, which puts them on the modest end of modern LLMs. How can we be sure that models with more parameters (13 / 30 / 70 / ...) behave in a similar manner without testing?, what about the influence of the context length on the performance?'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns perfectly with the ground truth. It correctly identifies that: (1) the paper makes broad claims about 'LLMs' in general, (2) the evidence is limited to only two similar 7B parameter models (Mistral-7B and Vicuna-7B), (3) this creates a generalizability problem since larger models may behave differently, and (4) other factors like context length could affect the findings. The reviewer also provides the appropriate remedy, suggesting 'the paper text should be adjusted to reflect the actual findings of the presented experiments and would avoid bold claims about \"all LLMs\"' rather than demanding expensive additional experiments. This matches the ground truth description of the flaw and the reasonable solution."
    }
  },
  {
    "openreview_id": "ctXYOoAgRy",
    "reviewer_id": "Nru9",
    "evaluation": {
      "flaw_id": "overlap_neuron_analysis",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions the overlapping neurons issue in multiple places: 'I am curious why you do not remove the overlapping neurons. In this way, you can detect the real language-specific ones' and 'if we look at the columns, it seems that all the language-specific neurons are covered by English ones, suggesting that they are actually not language-specific. It is very likely that these neurons are essential for general understanding and are language-agnostic.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth flaw description. The reviewer correctly identifies that overlapping neurons between languages may actually be language-agnostic rather than language-specific, which directly questions the validity of the neuron classification - exactly as described in the ground truth. The reviewer notes that 'all the language-specific neurons are covered by English ones, suggesting that they are actually not language-specific' and argues these neurons are 'essential for general understanding and are language-agnostic.' This reasoning matches the ground truth's concern that overlapping neurons 'may actually be language-agnostic, questioning the validity of the neuron classification.' The reviewer also correctly identifies that without removing overlapping neurons, the main claims about language-specific neurons remain unconvincing, which aligns with the ground truth statement that 'without this additional analysis, the main claim about language-specific neurons would remain weak.'"
    }
  },
  {
    "openreview_id": "mSHs6C7Nfa",
    "reviewer_id": "aFYc",
    "evaluation": {
      "flaw_id": "remove_weak_update_rule",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions Section 5.3 and criticizes it extensively: 'Section 5.3 is also very much unnecessary. I even doubt its correctness. You seem to be proposing a new sampler/solver with a very weak (intuitive) motivation. Designing a solver require a lot more than that. And then \".. detailed analysis is provided in appendix E\" — appendix E barely has any details! Also, obvious question, why are FIDs going up (fig.4) with higher NFE?'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns very well with the ground truth. The reviewer correctly identifies that Section 5.3 introduces a new sampler/solver (ODE update rule) with 'very weak (intuitive) motivation,' which matches the ground truth's description of 'lacks rigorous motivation.' The reviewer notes that designing a solver requires 'a lot more than that,' indicating insufficient validation. The reviewer also points out the inconsistent performance by questioning why FIDs go up with higher NFE, which aligns with the ground truth's mention of 'inconsistent advantages (helps only for a subset of NFEs).' The reviewer concludes the section is 'very much unnecessary,' effectively arguing it should be omitted, which matches the ground truth description that it should be deleted from the camera-ready version."
    }
  },
  {
    "openreview_id": "mSHs6C7Nfa",
    "reviewer_id": "aFYc",
    "evaluation": {
      "flaw_id": "lack_of_formal_proof_for_2rf_optimality",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions this flaw in the major concerns section: 'Let's talk about the observation of section 3 itself (i.e. 2-RF being optimal). Authors must be clear about whether they are making a theoretical assertion (L64-65) or just an empirical observation... If you are providing a *guarantee*, you must provide better formal proofs... But if it is a guarantee, empirical evidence is not enough.' The reviewer also notes in Question 2: 'Is it possible to have trajectory crossings *at all* when samples are from $p_{xz}^2$ ? If no, can you prove it formally?'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer's reasoning aligns well with the ground truth. They correctly identify that the paper oscillates between theoretical assertion and empirical observation, noting that the authors use ambiguous language like 'under *realistic* setting' and '*rarely* intersect each other' while making what appears to be a theoretical claim. The reviewer appropriately demands formal proofs for theoretical guarantees and questions whether empirical evidence alone is sufficient. This matches the ground truth description that the claim lacks formal proof and that reviewer aFYc requested formal guarantees. The reviewer's analysis captures the core issue that the argument is intuitive rather than rigorously proven."
    }
  },
  {
    "openreview_id": "LKdCkV31T7",
    "reviewer_id": "ZfCH",
    "evaluation": {
      "flaw_id": "missing_efficiency_analysis",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions the efficiency issue in the Weaknesses section: 'According to the Supplementary Material, nearly two hours of optimization time is required for just four frames of input, which is much more than the previous method.' It also asks for 'Quantitative comparisons of efficiency with other methods' and requests analysis of 'training/inference time for each module' in the Questions section.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies why the efficiency issue is problematic, aligning well with the ground truth. It notes that two hours for four frames is 'much more than the previous method,' questioning the practical viability of the approach. The reviewer suggests this 'may not be meaningful or necessary for a single sample fitting' and requests quantitative comparisons with other methods and module-wise timing analysis. This reasoning matches the ground truth's emphasis on questioning 'the method's practical usability' and the need for 'timing numbers and module-wise comparisons.' The review goes beyond just noting the inefficiency to explain why it's concerning for practical applications."
    }
  },
  {
    "openreview_id": "LKdCkV31T7",
    "reviewer_id": "ddrR",
    "evaluation": {
      "flaw_id": "missing_efficiency_analysis",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions the flaw in the weaknesses section: 'There are no details of the experiment platform or discussion of the computational efficiency of the proposed method.'",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies that computational efficiency details are missing, it fails to provide the deeper reasoning present in the ground truth. The ground truth emphasizes specific concerns about practical usability (two hours for only four frames), questions about the method's real-world applicability, and establishes this as a consensus issue requiring thorough analysis for publication. The review simply notes the absence of efficiency discussion without explaining why this omission is problematic for the method's practical adoption or scientific rigor. The reasoning is superficial compared to the ground truth's emphasis on practical usability concerns."
    }
  },
  {
    "openreview_id": "LKdCkV31T7",
    "reviewer_id": "Z54i",
    "evaluation": {
      "flaw_id": "limited_baseline_and_robustness_evaluation",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions aspects of the flaw in two places: (1) In weaknesses section: 'The most related work reported in this work might be the CVPR 23 paper Neuralpci. Does that mean no more recent related works?' which touches on insufficient comparison with relevant prior work, and (2) In questions section: 'How does NeuroGauss4D-PCI ensure robustness against varied data sparsity in real-world scenarios?' and 'Can the model handle rapidly changing dynamic scenes...' which address robustness evaluation concerns.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review touches on both components of the flaw (limited baseline comparison and robustness evaluation), the reasoning is superficial and incomplete compared to the ground truth. The review merely asks whether there are more recent related works rather than identifying the lack of detailed comparison with NeuralPCI as a significant limitation. For robustness, the review only poses questions about robustness scenarios but doesn't identify the absence of challenging scene evaluations as a major flaw. The ground truth describes this as a 'major limitation that must be remedied' with specific acknowledgment from authors and program chairs, but the review treats these issues as minor questions rather than fundamental experimental shortcomings that compromise the paper's scope and evaluation completeness."
    }
  },
  {
    "openreview_id": "LKdCkV31T7",
    "reviewer_id": "ddrR",
    "evaluation": {
      "flaw_id": "limited_baseline_and_robustness_evaluation",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions both components of the flaw: (1) insufficient comparison with NeuralPCI in the weaknesses section: 'A more detailed comparison with this model could be crucial in providing valuable insights and strengthening the paper's contribution to the field.' and (2) limited evaluation scope in the limitations section: 'I wonder if it is convincing that all are testing their work on only two open datasets used in this paper.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer correctly identifies why both aspects are problematic. For the baseline comparison issue, they explain that a more detailed comparison with NeuralPCI 'could be crucial in providing valuable insights and strengthening the paper's contribution to the field,' which aligns with the ground truth's emphasis on insufficient comparison with the most relevant prior work. For the robustness evaluation, they question whether testing on 'only two open datasets' is 'convincing,' which corresponds to the ground truth's concern about not demonstrating robustness on more challenging scenes. The reviewer's reasoning captures the core issues of incomplete experimental scope that weakens the paper's credibility and contribution."
    }
  },
  {
    "openreview_id": "EbSSBvwUWw",
    "reviewer_id": "6YuP",
    "evaluation": {
      "flaw_id": "rotational_non_invariance",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions the rotational non-invariance issue in multiple places: 'Unlike vanilla SGD, Sign SGD is not invariant to the choice of the basis' and 'the Sign SGD algorithm here would fail if the task was instead k-parity in an unknown basis. Many of the prior works discussed in Tables 1 and 2, however, are rotationally invariant.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. It correctly identifies that: (1) Sign-SGD is not rotationally invariant, (2) the method would fail on k-parity in an arbitrary unknown basis, (3) prior rotationally-invariant approaches don't have this limitation, and (4) this makes comparisons to prior work misleading. The review also notes that the z^k activation is 'essentially hardcoding the fact that the target problem is k-parity,' which supports the ground truth's point about the method only working when coordinates are aligned with the standard basis. The reasoning demonstrates understanding of why this is a significant limitation rather than just noting it as a technical detail."
    }
  },
  {
    "openreview_id": "OuKW8cUiuY",
    "reviewer_id": "UCEq",
    "evaluation": {
      "flaw_id": "sampling_efficiency",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions the sampling efficiency issue in two places: (1) In weakness point 4: '[2] achieves good results with just 50 steps of sampling.Could the authors provide an analysis of the variation of performance with different number of sampling steps.' and (2) In question 3: 'Could the authors provide an analysis of the variation of performance with different number of sampling steps.' These statements indirectly reference the sampling step efficiency concern.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review touches on sampling steps, it fails to identify the core problem described in the ground truth. The ground truth specifies that the method requires a full 1000-step DDPM sampling schedule resulting in very slow inference (≈230 s per 256×256 image), which is a major computational cost weakness. The review only suggests comparing performance across different numbers of sampling steps, without recognizing or explaining the fundamental efficiency problem, the specific computational cost implications, or the practical publishability concerns that make this a critical issue requiring faster samplers like DDIM/DPM-Solver."
    }
  },
  {
    "openreview_id": "OuKW8cUiuY",
    "reviewer_id": "Wzuv",
    "evaluation": {
      "flaw_id": "sampling_efficiency",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions the sampling efficiency flaw in the weaknesses section: 'The denoising computational cost is high, and the running time for the reverse diffusion process is lengthy.'",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies that the computational cost is high and the reverse diffusion process takes a long time, the reasoning is superficial compared to the ground truth. The ground truth provides specific technical details: the algorithm requires a full 1000-step DDPM sampling schedule, takes approximately 230 seconds per 256×256 image, and this was flagged by multiple reviewers as a major weakness requiring incorporation of faster samplers like DDIM/DPM-Solver. The review's reasoning lacks these technical specifics and fails to emphasize this as a 'critical issue that must be addressed for practical publishability' as described in the ground truth. The review treats it as just one weakness among others rather than a major practical limitation."
    }
  },
  {
    "openreview_id": "5jYFoldunM",
    "reviewer_id": "L7r5",
    "evaluation": {
      "flaw_id": "missing_real_world_experiments",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in the Questions section: 'Could the authors add more evaluations to test how the proposed method performs with real-world data?' This directly addresses the need for real-world experiments beyond synthetic simulations.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies that real-world data experiments are missing, it fails to provide adequate reasoning for why this is a critical flaw. The review simply asks for 'more evaluations' with real-world data as a question, without explaining that synthetic simulations alone are inadequate to validate practical impact of the attacks. The ground truth indicates this was a significant concern raised by multiple reviewers and the Area-Chair, who argued that solely synthetic simulations are insufficient for validation. The generated review treats this more as a minor suggestion rather than recognizing it as a fundamental limitation that undermines the paper's ability to demonstrate practical applicability of the proposed adversarial attacks."
    }
  },
  {
    "openreview_id": "5jYFoldunM",
    "reviewer_id": "pEvv",
    "evaluation": {
      "flaw_id": "missing_real_world_experiments",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions the lack of real-world experiments in the weaknesses section: 'However, the experimental section does not, in any capacity, consider an end-to-end application such as OOD detection. In fact, it reads purely as a statistical simulation (which is fine as a synthetic experiment). But a minimal experiment with a MNIST/FashionMNIST OOD-classifier, to show that indeed the Z-scores can be affected by INCREASE-c could make the paper much stronger.' The reviewer also asks in the questions section: 'What is the impact of attacking BH procedure on the whole OOD system? Can you break OOD detection in a meaningful way?'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth. The reviewer correctly identifies that while the paper mentions safety applications like OOD detection in the introduction, the experiments are 'purely statistical simulation' and lack real-world validation. The reviewer specifically suggests MNIST/FashionMNIST experiments, which matches exactly what the ground truth indicates was requested by other reviewers. The reviewer explains why this is a flaw: it would make the paper 'much stronger' to demonstrate practical impact, and questions whether the attacks can 'break OOD detection in a meaningful way.' This reasoning matches the ground truth concern about validating 'practical impact' of the attacks beyond synthetic simulations."
    }
  },
  {
    "openreview_id": "5jYFoldunM",
    "reviewer_id": "L7r5",
    "evaluation": {
      "flaw_id": "insufficient_problem_setup_clarity",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions key aspects of the problem setup clarity flaw. Specifically, it questions why attackers perturb z-scores rather than raw data: 'In practical scenarios, an adversary manipulates the input directly, not the z-scores; thus, the insight into how perturbations applied to z-scores translate back to the original data samples or vice-versa is missing in the paper.' It also asks for clarification on methodological choices: 'could the authors provide more insight into how perturbations applied to z-scores translate back to the original data samples or vice-versa?' and questions the justification for z-score perturbation approach.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth flaw description. It correctly identifies that the paper lacks clear justification for why z-scores are perturbed instead of raw data, which matches the ground truth concern about 'why the attacker perturbs z-scores rather than raw data.' The reviewer explains the practical implications of this choice, noting that 'manipulation at the data level can get lost at the z-scores level' and questions whether this approach is realistic since 'an adversary manipulates the input directly, not the z-scores.' The review also asks for better mapping between data and z-scores, and clearer explanation of methodological choices, which directly corresponds to the ground truth description of reviewers requesting 'clearer justification and explanation' of key methodological choices including z-score perturbation and adversary power assumptions."
    }
  },
  {
    "openreview_id": "5jYFoldunM",
    "reviewer_id": "pEvv",
    "evaluation": {
      "flaw_id": "insufficient_problem_setup_clarity",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the Weaknesses section: 'There seems to be a certain lack of justification for choices in the problem setup (addressed in questions below).' It then elaborates extensively in the Questions section with specific 'Problem setup questions' asking for rationale about attacker access to z-scores, choice of ℓ₀ perturbation budget, and adversary knowledge - all directly matching the ground truth flaw description.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer's reasoning aligns well with the ground truth. They identify the same specific methodological choices that lack justification (z-score perturbation vs raw data, ℓ₀ budget choice, adversary knowledge/power) and explain why these need clarification. The reviewer asks 'What is the rationale for the attacker having access to z-scores?' and 'It would be good to have a comment justifying the choice of perturbation,' demonstrating they understand these unjustified choices make the paper less accessible and solid - matching the ground truth's emphasis on the need for 'clearer justification and explanation' to make the work 'accessible and solid.'"
    }
  },
  {
    "openreview_id": "ADV0Pzi3Ol",
    "reviewer_id": "L5Ee",
    "evaluation": {
      "flaw_id": "equation_4_explanation_gap",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions the flaw in Weakness #1: 'Equations 4 and 5 sketch out the optimization problem that defines the dual-correct predictions, and how to optimize for right for the right reason. However, it's unclear if the optimization problem listed here is the correct one to pursue, and the reasons behind such a problem are unclear.' The review explicitly states that the connection between these equations and correct rationales is unclear.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning closely aligns with the ground truth description. The ground truth indicates that the link between Equations 4/5 and 'correct rationales' was unclear, making the optimization objective and method's validity opaque. The review captures this by stating: (1) 'it's unclear if the optimization problem listed here is the correct one to pursue', (2) 'the reasons behind such a problem are unclear', and (3) 'it is unclear how or why such a problem connects with the overall issue of producing correct rationales.' The review correctly identifies that this lack of clarity undermines the foundational understanding of the method, which matches the ground truth's concern about the method's validity remaining opaque."
    }
  },
  {
    "openreview_id": "ADV0Pzi3Ol",
    "reviewer_id": "dtoB",
    "evaluation": {
      "flaw_id": "equation_4_explanation_gap",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the weaknesses section: 'Equation 4 lacks sufficient explanation. Especially the correct rationale.' This directly corresponds to the ground truth flaw about Equation 4/5 and its unclear link to 'correct rationales'.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies that Equation 4 lacks sufficient explanation regarding correct rationales, the reasoning is superficial and incomplete compared to the ground truth. The ground truth explains that this lack of clarity makes 'the optimization objective and therefore the method's validity remain opaque,' highlighting serious implications for the method's validity. The review simply states the equation 'lacks sufficient explanation' without explaining why this is problematic or what consequences this has for understanding the method. The review fails to connect this explanation gap to broader concerns about the method's validity or optimization objective."
    }
  },
  {
    "openreview_id": "ADV0Pzi3Ol",
    "reviewer_id": "WT2A",
    "evaluation": {
      "flaw_id": "limited_domain_scope",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the weaknesses section: 'Limited applications: The paper is framed in the title and intro as providing general methods for forming models with dual-correct predictions. However, the results are only for vision transformers: the faithful explanation method used is based on CLIP-ViT models, and the collected dataset is only relevant for object classification on ImageNet. Since the method and problem are framed generally, there should be more experiments across other domains (e.g. text) and models (e.g. diffusion models).'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning correctly aligns with the ground truth flaw. It identifies the core issue: the paper is 'framed generally' as providing methods for 'dual-correct predictions' but the actual experiments are limited to vision transformers on ImageNet-style data. The reviewer correctly explains why this is problematic - there's a mismatch between the general claims made in the framing and the narrow experimental validation. The reviewer also suggests what would be needed to address this (experiments across other domains like text and other model types like diffusion models), which demonstrates understanding of the scope limitation's negative implications for the paper's claimed generality."
    }
  },
  {
    "openreview_id": "Ur00BNk1v2",
    "reviewer_id": "tVAw",
    "evaluation": {
      "flaw_id": "mllm_comparison_gap",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the Weaknesses section with 'No comparison with other MLLMs' and elaborates on it in the Questions section: 'I would recommend picking one or two more MLLMs and doing a comparison. I think it's fine that this paper serves as an existence proof that there exists an MLLM with planning capabilities suitable for multi-step image editing, but I didn't get a very good sense of how that performance is impacted by product-specific fine tuning and alignment that might happen behind the scenes with gpt 4.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer correctly identifies why the lack of MLLM comparisons is problematic. They explain that without comparisons to other MLLMs, it's unclear whether the observed performance is specific to GPT-4's particular fine-tuning and alignment, or whether it's a more general capability that other MLLMs could achieve. This reasoning aligns well with the ground truth description that characterized this as a 'major weakness' - the reviewer understands that the absence of these comparisons limits the scope and generalizability of the findings, making it difficult to assess whether the approach is broadly applicable or merely an artifact of using a specific, highly-tuned model."
    }
  },
  {
    "openreview_id": "Ur00BNk1v2",
    "reviewer_id": "TXUv",
    "evaluation": {
      "flaw_id": "missing_method_details",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in two places: (1) In the Weaknesses section: 'the paper lacks some technical details about the method such as the underlying model.' (2) In the Questions section: 'Did authors use language-only GPT4 or GPT4-V or GPT4o? For reproducibility, which version of GPT4 was used?'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies both the specific missing detail (which GPT-4 version was used) and the negative implication (reproducibility concerns). The reviewer explicitly asks 'For reproducibility, which version of GPT4 was used?' which directly aligns with the ground truth description that states these missing details were 'hindering reproducibility.' The reasoning is accurate and complete, connecting the missing technical details to their impact on reproducibility."
    }
  },
  {
    "openreview_id": "Ur00BNk1v2",
    "reviewer_id": "tJSg",
    "evaluation": {
      "flaw_id": "insufficient_discussion_of_editing_limitations",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions the flaw in multiple places: (1) In the major concern section: 'All images generated for the complex text is by first generating an initial image, and then utilizing editing tools... This could be very limited if the images initially generated are not very decent... the overall quality and layout may not be very natural in some cases. The editing trace may be easily detected... For example, the generated result with \"hot dogs\" in Figure 1 is not natural or of decent quality... The image is blurred and the hot dogs look a little fake.' (2) In the Limitations section: 'I would suggest the authors discuss the potential limitation brought by the initial generated results and editing artifacts.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns very well with the ground truth description. The reviewer correctly identifies that: (1) The system's dependence on initially generated images can yield unnatural results, (2) Visible editing artifacts can occur (specifically mentioning the 'hot-dog' case from Figure 1 as looking 'blurred' and 'fake'), (3) The editing traces can be easily detected, and (4) This is a limitation that should be properly discussed by the authors. The reviewer goes beyond just noting the flaw exists - they explain the negative implications on image quality, naturalness, and overall system performance, which matches the ground truth's description of 'unnatural results and visible editing artifacts.'"
    }
  },
  {
    "openreview_id": "7v0UyO0B6q",
    "reviewer_id": "AZGY",
    "evaluation": {
      "flaw_id": "imprecise_theorem_statements",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the Weaknesses section: 'I'm not entirely comfortable with the statements of Theorem 2 and 4, because they involve \"≈\", which is not formally defined. For instance, a reader might wonder if the approximation in (6) is \"as good\" as the one on line 148... I understand what the authors mean, but formal statements need to be more precise.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns very well with the ground truth description. The reviewer correctly identifies that the informal '≈' symbol lacks formal definition, which creates ambiguity about the exact meaning and quality of different approximations (comparing equation 6 to line 148). The reviewer also provides a constructive solution that matches what the ground truth indicates was needed: 'I recommend, as a fix, writing \"Suppose that\" and then equation (6) with an equality sign. The authors can then replace all the \"≈\" with an \"=\", and state that the initial assumption does not hold in general.' This demonstrates understanding that the issue requires reformulating results with explicit equalities and clearly specifying assumptions, which is exactly what the ground truth describes as the critical change needed."
    }
  },
  {
    "openreview_id": "HNH1ykRjXf",
    "reviewer_id": "XJxy",
    "evaluation": {
      "flaw_id": "training_data_storage_requirement",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in Weakness 3 under 'Data Privacy Concern': 'Adapting the model at each time step requires storing all historical training data, testing data, and models, raising significant data privacy concerns. For instance, in the MRI example mentioned in the paper, it is questionable whether you are allowed to carry MRI data from 999 clinics just to adapt the model to the 1000th clinic.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies and explains why this is a problematic flaw. It mentions the need to store 'all historical training data' and highlights both privacy concerns and practicality issues. The MRI clinic example effectively illustrates the real-world implications of carrying sensitive data across multiple sites. While the ground truth specifically mentions memory footprint and the need to retrain the last linear layer, the review captures the core problematic aspects: data storage requirements and privacy concerns. The reasoning aligns well with the ground truth's emphasis on this being a 'critical issue' for practical deployment."
    }
  },
  {
    "openreview_id": "HNH1ykRjXf",
    "reviewer_id": "aV82",
    "evaluation": {
      "flaw_id": "training_data_storage_requirement",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in weakness #1: 'The proposed method requires the storage of previous historical data for self-supervised learning (batch accumulation in the paper). This may not be feasible in certain problems with privacy concerns.' It also addresses this in the questions section: 'for the storage of historical data, is it feasible to apply data augmentation techniques to the limited number of data per time stamp and use the augmented data for self-supervised learning? How does the number of historical data stored affect the proposed algorithm?'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies the core issues mentioned in the ground truth. It specifically notes: (1) the requirement to store previous historical data, (2) privacy concerns as a key implication, and (3) practicality issues ('may not be feasible'). While the ground truth mentions additional concerns like memory footprint and the specific detail about retraining the last linear layer, the review captures the essential problematic aspects of data storage requirements and privacy implications. The reviewer also demonstrates understanding by asking follow-up questions about how data storage affects the algorithm, showing awareness that this is a significant limitation that needs further investigation."
    }
  },
  {
    "openreview_id": "HNH1ykRjXf",
    "reviewer_id": "fST3",
    "evaluation": {
      "flaw_id": "training_data_storage_requirement",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions the storage requirement flaw in the weaknesses section: 'Regarding storage costs: It seems that the proposed method requires storing the training set $D_0$. Such a requirement is somewhat unfavorable in practice, as the training set $D_0$ typically consists of a large volume of data.' The reviewer also asks about this issue in the questions section: 'Is there any method to reduce the requirement of storing the train set $D_0$?'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth description. The reviewer correctly identifies that storing the training set D_0 is 'unfavorable in practice' because 'the training set $D_0$ typically consists of a large volume of data,' which captures the practicality and memory-footprint concerns mentioned in the ground truth. While the review doesn't explicitly mention privacy concerns, it does recognize the practical limitations of requiring large amounts of stored data, which is the core issue. The reviewer also asks for methods to reduce this storage requirement, showing understanding that this is a significant limitation that needs addressing."
    }
  },
  {
    "openreview_id": "kQ9LgM2JQT",
    "reviewer_id": "bgQq",
    "evaluation": {
      "flaw_id": "insufficient_q_training_description",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions this flaw in Weakness 2: 'Training of the Q-network is insufficiently described. Lines 163-175 are hard to follow and Algorithm 1 in Section B does not provide sufficient details for understanding the training of QGFN... an unambiguous equation representing the learning objective for Q should be included in Section 4.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth. The reviewer correctly identifies that the Q-network training description is insufficient and explains why this is problematic: the lines are 'hard to follow,' the algorithm 'does not provide sufficient details,' and there's ambiguity about multi-step returns and horizon choice. The reviewer also correctly identifies the negative implications by requesting 'an unambiguous equation representing the learning objective for Q' to be included, which matches the ground truth's mention of needing 'an unambiguous equation and fuller explanation of the Q-learning procedure.' The reviewer demonstrates understanding of the specific technical gaps (learning objective, multi-step return formulation, horizon choice) that align with the ground truth description."
    }
  },
  {
    "openreview_id": "kQ9LgM2JQT",
    "reviewer_id": "eVPz",
    "evaluation": {
      "flaw_id": "missing_compute_runtime_reporting",
      "is_flaw_mentioned": false,
      "mention_reasoning": "The review does not mention anything about computational overhead, training time, inference time, or runtime comparisons between QGFN and standard GFNs. The review focuses on methodological concerns about variant selection, missing complex environments, and missing related work, but does not address the computational practicality or performance overhead of adding a separate Q network.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "Since the flaw was not mentioned in the review, there is no reasoning to evaluate. The ground truth flaw specifically concerns the lack of concrete evidence about training-time overhead and the need for precise training and inference time comparisons to judge the method's real-world practicality. The generated review completely missed this computational efficiency concern and instead focused on other methodological issues like variant selection and evaluation scope."
    }
  },
  {
    "openreview_id": "kQ9LgM2JQT",
    "reviewer_id": "eVPz",
    "evaluation": {
      "flaw_id": "limited_complex_environment_evaluation",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions this flaw in the weaknesses section: 'Lack of more complex environment. Therefore, I wonder about the QGFN's performance on graph combinatorial optimization problems, such as MIA.' This clearly identifies the limitation of not evaluating on harder graph combinatorial optimization benchmarks like MIA, which aligns with the ground truth description mentioning MIS/MIA benchmarks.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer's reasoning aligns well with the ground truth description. The review identifies that the paper lacks evaluation on more complex environments, specifically mentioning graph combinatorial optimization problems like MIA. This directly corresponds to the ground truth flaw which describes the need for 'harder graph combinatorial optimisation benchmarks (e.g., MIS/MIA)'. The reviewer also connects this limitation to a broader concern about how QGFN variants would perform in environments with variable action spaces (as found in graph problems), showing understanding of why this evaluation gap is problematic. The reasoning demonstrates that the limited scope of evaluation environments is a genuine limitation that affects our understanding of the method's broader applicability."
    }
  },
  {
    "openreview_id": "bIa03mAtxQ",
    "reviewer_id": "YTQ3",
    "evaluation": {
      "flaw_id": "missing_sparse_moe_baselines",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the weaknesses section: 'Most importantly: there is not quality comparison between the proposed approach and Sparse MoEs' and 'In vision tasks, Soft MoEs in [\"From Sparse to Soft Mixture of Experts\"] have shown also much better results than vanilla dense transformers and Sparse MoEs. Thus, either of these should be considered as a stronger baseline than the use of vanilla MLPs.' The reviewer also mentions in the rebuttal section: 'I still believe that Sparse MoEs would be a better baseline than Dense MoEs'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer's reasoning aligns well with the ground truth. They correctly identify that comparisons against Sparse/Soft MoEs are essential for judging the merits of the proposed µMoE approach. The reviewer provides substantial justification by citing multiple recent works (Mixtral, DeepSeekMoE, Gemini 1.5, DBRX) showing that Sparse MoEs are present in state-of-the-art models, and mentions the 'From Sparse to Soft Mixture of Experts' paper showing better results in vision tasks. They argue that these should be considered as stronger baselines than vanilla MLPs, which matches the ground truth's emphasis that such comparisons are 'essential for judging µMoE's merits.' The reviewer also acknowledges in the rebuttal section that the authors addressed this concern, which aligns with the ground truth mentioning that authors ran new experiments during rebuttal."
    }
  },
  {
    "openreview_id": "bIa03mAtxQ",
    "reviewer_id": "qC51",
    "evaluation": {
      "flaw_id": "missing_sparse_moe_baselines",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions the missing comparisons with Sparse and Soft MoEs in multiple places: 'comparing it with similar parameter and/or compute sized sparse/soft MoEs' in discussing Figure 2, 'there should be a way to compare it at least with sparse MoE' for Figure 3, and most directly 'Comparison with Sparse and SoftMoE in vision and LLMs are missing' in the weaknesses section. The reviewer also asks 'how \\muMoE perform compared to SoftMoE and SparseMoE' in the questions section.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer correctly identifies why these comparisons are essential, aligning well with the ground truth. They explain that comparisons are needed 'keeping the same parameter size and/or compute fixed across algorithms' and note that 'increasing the capacity of the network by increasing parameter count, while preserving the same compute is one of the main motivations behind MoEs.' This demonstrates understanding that without these baselines, it's impossible to judge µMoE's merits relative to existing MoE approaches, which matches the ground truth's emphasis on these comparisons being 'essential for judging µMoE's merits.' The reviewer also requests both accuracy and efficiency comparisons, which aligns with the ground truth mention of experiments including 'both accuracy and efficiency comparisons.'"
    }
  },
  {
    "openreview_id": "bIa03mAtxQ",
    "reviewer_id": "1sqG",
    "evaluation": {
      "flaw_id": "limited_scalability_validation",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in multiple places: (1) In the weaknesses section: 'The research primarily focuses on vision models and specific datasets, which may limit the generalizability of the findings. Expanding the validation to more diverse models (LLaMA, gemma, and e.t.c) and datasets could provide a more comprehensive view of the µMoE layers' effectiveness.' (2) In the questions section: 'Is it feasible to apply this methodology to large-scale MoE Language Models with a substantial number of experts, similar to the extensive parameter count found in Mixtral 8×7B?' These comments directly address the lack of evaluation at larger scales that characterizes the planted flaw.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning aligns well with the ground truth flaw description. The reviewer correctly identifies that the limited scope of evaluation (focusing on vision models and specific datasets like GPT-2 124M) restricts the generalizability of findings, which matches the ground truth concern about not evaluating µMoE at scales used by current large LLMs. The reviewer also specifically questions the feasibility of applying the methodology to large-scale models like Mixtral 8×7B, directly addressing the scalability concern. The reasoning correctly identifies the negative implication on scope and generalizability, though it doesn't explicitly mention the minimal benefits observed in the GPT-2 124M experiment as noted in the ground truth."
    }
  },
  {
    "openreview_id": "bIa03mAtxQ",
    "reviewer_id": "YTQ3",
    "evaluation": {
      "flaw_id": "limited_scalability_validation",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in multiple places: 'The results on GPT-2 are close to the dense MLPs, but not better than them. This is problematic, since this architecture is the biggest one trained (with 124M params) and probably the one trained on the richest data source. This begs the question of whether the proposed approach can scale well to bigger backbone architectures and datasets.' The reviewer also states in the rebuttal section: 'I also find concerning the fact that when the method is applied on relatively large models, it provides little benefit.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer's reasoning correctly aligns with the ground truth description. The review identifies that: 1) The GPT-2 experiment (124M parameters) shows little to no benefit over dense MLPs, 2) This raises concerns about scalability to larger models and datasets, and 3) The method provides little benefit when applied to relatively large models. The reviewer correctly identifies this as problematic and concerning, understanding the negative implications for the method's applicability to current large-scale language models. The reasoning goes beyond just noting the limitation - it explains why this is a significant issue for the practical utility of the proposed approach."
    }
  },
  {
    "openreview_id": "bIa03mAtxQ",
    "reviewer_id": "1sqG",
    "evaluation": {
      "flaw_id": "lack_of_ood_robustness_analysis",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in the Weaknesses section: 'Lack of Robustness Analysis: The paper would benefit from including experiments that assess the robustness and performance of µMoE layers on out-of-distribution data, offering insights into the model's reliability under varied conditions.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies the absence of out-of-distribution robustness analysis and provides appropriate reasoning for why this is a flaw. The reviewer explains that such experiments would 'offer insights into the model's reliability under varied conditions,' which aligns well with the ground truth description that highlights the need for robustness assessment beyond in-distribution test sets. The reasoning demonstrates understanding of the importance of OOD evaluation for model reliability, matching the spirit of the ground truth concern about the limitation of confining evaluation to in-distribution data only."
    }
  },
  {
    "openreview_id": "UddVRqTrjt",
    "reviewer_id": "4WsD",
    "evaluation": {
      "flaw_id": "scalability_k_to_d_explosion",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in Weakness #2: 'what is the trade-off between breadth and depth of the constructed tree? For example - what is the difference between having more children nodes with low depth and having less children nodes with high depth. This would help in further understanding the advantages and limitations of the proposed method.' This directly addresses the scalability issue with tree depth and branching factor.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review identifies that there are trade-offs between tree breadth and depth, it does not correctly explain why this is a fundamental flaw. The ground truth specifies that the core issue is exponential memory/compute growth (K^d images) that severely limits scalability and contradicts the paper's claim of 'efficient, arbitrarily-deep posterior trees in a single forward pass.' The review treats this as a clarification question about trade-offs rather than recognizing it as a critical scalability limitation that undermines the paper's main contribution. The reviewer does not mention the exponential growth problem, memory constraints, or the contradiction with the paper's efficiency claims."
    }
  },
  {
    "openreview_id": "UddVRqTrjt",
    "reviewer_id": "VHHJ",
    "evaluation": {
      "flaw_id": "scalability_k_to_d_explosion",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions this flaw in the weaknesses section: 'It seems like the architecture of the network required to predict the tree depends on the tree depth and width as K^d, where d is the tree depth and K is the branching factor. This appears to be a very limiting factor in the depth and width of the tree that can be learned.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies the core scalability issue and provides accurate reasoning about why it's a flaw. The reviewer correctly states that the network architecture depends on K^d (matching the ground truth's 'K^d images' requirement), recognizes this creates exponential growth with tree depth and branching factor, and identifies it as 'a very limiting factor in the depth and width of the tree that can be learned.' This aligns well with the ground truth description that memory/compute grow exponentially and severely limit how large a hierarchy can be learned. The reviewer also asks relevant follow-up questions about computational costs, demonstrating understanding of the practical implications."
    }
  },
  {
    "openreview_id": "UddVRqTrjt",
    "reviewer_id": "4WsD",
    "evaluation": {
      "flaw_id": "fixed_balanced_tree_layout",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in weakness point 2: 'One thing that I did not understand is - what is the trade-off between breadth and depth of the constructed tree? For example - what is the difference between having more children nodes with low depth and having less children nodes with high depth. This would help in further understanding the advantages and limitations of the proposed method.' This directly addresses the core issue about the fixed balanced K-ary tree structure and the lack of guidance on tree parameters.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies the issue with the tree breadth/depth trade-off, the reasoning is incomplete compared to the ground truth. The reviewer frames this as something they 'did not understand' and asks for clarification to understand 'advantages and limitations,' but they don't explicitly identify this as a major methodological flaw. The ground truth describes this as a 'major shortcoming' that could cause the method to 'fail to summarise some posteriors effectively.' The reviewer doesn't make the connection that the lack of guidance on choosing K and d parameters, or the absence of adaptive tree layouts, represents a fundamental limitation that could lead to method failure on certain types of posteriors."
    }
  },
  {
    "openreview_id": "6A29LUZhfv",
    "reviewer_id": "4BB9",
    "evaluation": {
      "flaw_id": "english_generalizability",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions this flaw in the weaknesses section: 'Single-Language Focus: The all-mpnet-base-v2 model used in the framework is designed for English only, raising concerns about its adaptability to different linguistic and cultural contexts.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review correctly identifies the core issue and provides appropriate reasoning. It specifically mentions the all-mpnet-base-v2 model being English-only (matching the ground truth's description of MixEval being constructed with this English-only sentence-embedding model) and correctly explains why this is problematic - it raises concerns about adaptability to different linguistic and cultural contexts. This aligns well with the ground truth description that the validity for non-English queries remains untested and represents a significant limitation for cross-lingual coverage. The reviewer demonstrates understanding of the negative implications for generalizability beyond English."
    }
  },
  {
    "openreview_id": "6A29LUZhfv",
    "reviewer_id": "u4BL",
    "evaluation": {
      "flaw_id": "contamination_overfitting_unresolved",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions the contamination and overfitting problem in weakness #2: 'To avoid models overfitting on existing benchmarks, the paper updates the benchmarks by drawing new queries from the internet. However, I believe overfitting occurs due to models overfitting on a fixed benchmark. Without changing the benchmark, the overfitting issue will not be resolved.' The review also notes in weakness #3 that 'Queries drawn from the internet might have already been seen by LLMs' which relates to contamination concerns.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer's reasoning aligns well with the ground truth flaw. They correctly identify that the proposed solution doesn't fundamentally solve the overfitting problem, stating 'Without changing the benchmark, the overfitting issue will not be resolved.' This matches the ground truth's point that 'simply matching web queries to existing, potentially saturated benchmarks does not fundamentally solve the contamination and over-fitting problem.' The reviewer also points out that internet queries may have already been seen by LLMs, which relates to the contamination aspect. Additionally, they note that 'Table 2 does not demonstrate that this method alleviates overfitting,' showing they recognize the lack of evidence that the problem is actually solved. While the review doesn't explicitly mention that the authors acknowledged this as a limitation that only 'mitigates' the problem, the core reasoning about the fundamental inadequacy of the proposed solution is correct."
    }
  },
  {
    "openreview_id": "6A29LUZhfv",
    "reviewer_id": "yGD1",
    "evaluation": {
      "flaw_id": "contamination_overfitting_unresolved",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions the contamination issue in the Major Issues section: 'MixEval dynamically updates by mixing popular benchmarks (e.g., MMLU, BoolQ, GSM8K), which may not mitigate contamination. Most of these benchmarks are saturated and suffer from contamination. Although the authors claim they will \"dynamically expand our benchmark pool with newly released benchmarks to further enhance the mixed benchmark distribution,\" this does not address the root issue of contamination.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning closely aligns with the ground truth description. The reviewer correctly identifies that: (1) simply mixing existing benchmarks that are already saturated and contaminated doesn't solve the fundamental contamination problem, (2) the authors' proposed solution of dynamically expanding the benchmark pool doesn't address the 'root issue' of contamination, and (3) this represents an unresolved limitation. This matches the ground truth's emphasis that matching web queries to existing benchmarks 'does not fundamentally solve the contamination and over-fitting problem' and that the authors can only 'mitigate' but not eliminate these biases, making residual contamination risk a critical limitation."
    }
  },
  {
    "openreview_id": "BQh1SGvROG",
    "reviewer_id": "3x5z",
    "evaluation": {
      "flaw_id": "dynamic_interaction_motivation",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions the Dynamic Interaction mechanism in the Questions section: 'Would be good to elaborate a bit more on how the Dynamic Interaction works and why it is novel' and 'How does the developing pattern of the NCA look over time? Are differences visible between a version with and without dynamic interaction?' This directly addresses the lack of clear explanation about the Dynamic Interaction module.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review does mention the need for more elaboration on Dynamic Interaction, the reasoning is superficial and incomplete compared to the ground truth. The ground truth identifies specific issues: (1) lack of theoretical justification for why Dynamic Interaction is superior to standard NCA concatenation, (2) absence of evidence showing it's not mathematically equivalent to existing transformations, and (3) missing full ablations against strong baselines. The review only asks for general elaboration on 'how it works and why it is novel' and pattern visualization, but fails to identify the deeper methodological concerns about theoretical justification, mathematical equivalence, and rigorous experimental validation that are central to the flaw."
    }
  },
  {
    "openreview_id": "BQh1SGvROG",
    "reviewer_id": "ZhPF",
    "evaluation": {
      "flaw_id": "dynamic_interaction_motivation",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review mentions the flaw in the Questions section: 'What's the motivaiton of using channel-weighted convolution for the perception part part of the NCA (what you call \"dynamic interaction\")? Wouldn't this be equivalent to the transformation that the update MLP does on concatenated channels vectors from standard depth-wise conv layers?' This directly questions the theoretical justification for the Dynamic Interaction module and whether it's mathematically equivalent to existing transformations.",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer's reasoning correctly aligns with the ground truth flaw. The reviewer specifically questions: (1) the motivation/theoretical justification for the Dynamic Interaction module, and (2) whether it's mathematically equivalent to existing approaches (standard concatenation + MLP transformation). This matches the ground truth description which states that 'the current manuscript does not clearly explain why Dynamic Interaction is not mathematically equivalent to existing transformations.' The reviewer correctly identifies that the distinction between the proposed approach and standard methods is unclear, which is precisely the core issue described in the ground truth flaw."
    }
  },
  {
    "openreview_id": "BQh1SGvROG",
    "reviewer_id": "h6zC",
    "evaluation": {
      "flaw_id": "dynamic_interaction_motivation",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions the Dynamic Interaction flaw multiple times. Key quotes include: 'The Dynamic Interaction strategy was not sufficiently motivated. It should be compared against the typical NCA approach of concatenation. Comparing Dynamic Interaction against a simple summation does not motivate the use of Dynamic Interaction over concatenation.' and 'For Multi-scale Dynamic Interaction, I would like to see a comparison with a vanilla concatenation approach. I want to know if the cost of more parameters and FLOPS is worth it with the concatenation approach.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The reviewer's reasoning aligns well with the ground truth flaw description. The reviewer correctly identifies that: 1) The Dynamic Interaction module lacks proper theoretical justification compared to standard NCA concatenation approaches, 2) The empirical evidence is insufficient because comparisons were only made against simple summation rather than against the standard concatenation baseline, and 3) The paper doesn't clearly explain why Dynamic Interaction is superior to existing transformations. The reviewer also notes the lack of full ablations against strong baselines, asking for comparisons with concatenation approaches and other alternatives like 1x1 convolutions. This demonstrates understanding of why this is a methodological flaw that undermines the contribution's validity."
    }
  },
  {
    "openreview_id": "BQh1SGvROG",
    "reviewer_id": "oE4b",
    "evaluation": {
      "flaw_id": "missing_tapadl_baseline",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review explicitly mentions this flaw in multiple places: 'My major concern is the choice of baselines... i strongly dislike the choice of removing the additional ADL loss (according to [22] the ADL loss is more important than TAP).' and 'This is especially misleading since you call your baseline \"TAPADL-RVT\". But the \"adl\" in this method stands for the loss.' The reviewer also directly asks: 'I very strongly encourage including the results for the original \"TAPADL\" [22], with the correct loss for training.'",
      "is_reasoning_correct": true,
      "reasoning_analysis": "The review's reasoning closely aligns with the ground truth description. The reviewer correctly identifies that: 1) The ADL loss was removed from TAPADL, which is its key robustness component, 2) This leads to misleading conclusions, especially for OOD evaluation where 'the difference is not very big even without it', 3) The comparison is 'particularly unfair' and 'misleading', and 4) The reviewer requests results with the original TAPADL including ADL loss. The reviewer also suggests testing AdaNCA + ADL, which matches the ground truth. The reasoning demonstrates understanding of why this omission is problematic rather than just noting it was missing."
    }
  },
  {
    "openreview_id": "BQh1SGvROG",
    "reviewer_id": "ZhPF",
    "evaluation": {
      "flaw_id": "inadequate_vitca_comparison",
      "is_flaw_mentioned": true,
      "mention_reasoning": "The review directly mentions this flaw in the Questions section: 'How does AdaNCA differs from Vision Transformer Cellular Automata model from Attention-based Neural Cellular Automata, Tesfaldet et al. 2022?' This question specifically asks about the differentiation between AdaNCA and the prior ViTCA model, which is exactly what the ground truth flaw describes as missing.",
      "is_reasoning_correct": false,
      "reasoning_analysis": "While the review correctly identifies that there is a lack of comparison with the ViTCA model by asking about the differences, it does not explain why this is a significant flaw. The ground truth indicates that this inadequate comparison undermines the validation of novelty and that a clearer structural and empirical comparison is needed. The review simply poses the question without elaborating on the implications for novelty validation, the need for structural comparison, or the impact on the paper's contributions. The reviewer treats it as a clarifying question rather than identifying it as a fundamental weakness that affects the paper's claim of novelty."
    }
  }
]