#!/usr/bin/env python3
"""
Test script for SEA-E parser
"""

import sys
import json
from review_paper_pairs_vllm import _parse_seae_format

# Example SEA-E output from the user
EXAMPLE_SEAE_OUTPUT = """**Summary:**

The paper investigates the efficient computation of the attention matrix in linear time, a critical task in large language models (LLMs). It demonstrates that when the entries of matrices in the attention operation are bounded, it is possible to compute the attention matrix in near-linear time. The authors provide theoretical evidence by presenting a algorithm that efficiently computes the approximate solution of the attention operation in sub-quadratic time, given certain conditions. The paper also establishes a lower bound on the computational time required for unbounded entries, showing that below certain thresholds, the attention matrices can be effectively computed.



**Strengths:**

- The paper introduces a novel approach that transforms entry-wise exponential attention operators into low-rank low-degree polynomial approximations, significantly reducing the time complexity for computing attention matrices from quadratic to nearly linear, enhancing the speed of large language model inference.

- It verifies the phenomenon that small entries in attention computation can lead to faster attention computation, explaining why techniques like quantization and low degree polynomial approximation work well.

- The theoretical analysis is solid and the algorithm is clear, providing a detailed explanation of the theoretical underpinnings of the proposed method.

- The paper explores an interesting problem of faster attention matrix computation and presents a theoretical analysis, which includes a novel proof method based on the polynomial methods in the fine-grained complexity theory, aiming to find the theoretical guarantees of attention matrix computation.

- The analysis for the lower bound is based on the strong exponential time hypothesis (SETH), which provides a novel lower bound for the attention computation.

- The paper is clearly written and presents a well-structured argument with interesting theoretical implications, such as a low-degree polynomial approximation that maintains the rank of a matrix.



**Weaknesses:**

- The paper's scope is narrow and specific, focusing solely on attention computation and lacks broader applications within or beyond large-scale models.

- The algorithm design is somewhat standard and unsurprising, utilizing existing methods like low-degree polynomial approximations to address matrix multiplication problems, which limits the novelty of the work.

- The paper's assumptions about the dimensions of the input and output matrices limit the generalizability of the findings, which might not extend to other deep learning models like CNNs and RNNs.

- The proof of the paper is not self-contained and requires knowledge of the references for understanding, which could make it less accessible to readers unfamiliar with the related work.

- The paper's proof heavily relies on complex and unproven claims, particularly in the proof of Lemma 12, which could undermine the reliability of the main technical contribution.

- The practical aspects of the proposed upper bound are somewhat theoretical and may not be directly applicable in real-world scenarios.



**Questions:**

- Have the authors implemented the presented algorithm and obtained running time on actual data?

- Regarding the proof of Lemma 12, could the authors clarify the assumptions and implications of the inequalities used? Are these inequalities correct and well-established in the literature?

- How does the proposed algorithm handle the practical aspects, such as the choice of parameters in different scenarios?

- Considering the theoretical upper bounds, how do they compare with existing algorithms in terms of efficiency and computational time?

- Are there plans to extend the analysis to consider smaller bounds of d or explore the implications for different types of language models like autoregressive models?



**Soundness:**

3 good



**Presentation:**

3 good



**Contribution:**

3 good



**Rating:**

6 weak accept



**Paper Decision:**

- Decision: Accept

- Reasons: The paper presents a novel approach to computing the attention matrix in nearly linear time, which is significant for large language models. The theoretical contributions are robust, and the methodology, while relying on known techniques, provides a novel perspective on the problem. The paper's presentation is generally clear and well-structured, making it accessible to readers. Despite some limitations in novelty and practical applicability, the paper's contribution to understanding the computational efficiency of attention matrices is valuable. The paper's theoretical soundness and the potential impact on the field justify its acceptance, albeit with a suggestion for further exploration and practical validation of the proposed algorithms.</s>
"""

def test_parser():
    """Test the SEA-E parser"""
    print("=" * 80)
    print("Testing SEA-E Parser")
    print("=" * 80)
    
    # Parse the example
    result = _parse_seae_format(EXAMPLE_SEAE_OUTPUT)
    
    # Print results in a readable format
    print("\n" + "=" * 80)
    print("PARSED RESULTS")
    print("=" * 80)
    
    print(f"\nüìù SUMMARY:")
    print(f"   {result['summary'][:200]}...")
    
    print(f"\n‚úÖ STRENGTHS ({len(result['strengths'])} items):")
    for i, strength in enumerate(result['strengths'], 1):
        print(f"   {i}. {strength[:100]}...")
    
    print(f"\n‚ö†Ô∏è  WEAKNESSES ({len(result['weaknesses'])} items):")
    for i, weakness in enumerate(result['weaknesses'], 1):
        print(f"   {i}. {weakness[:100]}...")
    
    print(f"\n‚ùì QUESTIONS ({len(result['questions'])} items):")
    for i, question in enumerate(result['questions'], 1):
        print(f"   {i}. {question[:100]}...")
    
    print(f"\nüìä RATINGS:")
    print(f"   Soundness:     {result['soundness']}")
    print(f"   Presentation:  {result['presentation']}")
    print(f"   Contribution:  {result['contribution']}")
    print(f"   Overall:       {result['rating']}")
    
    print(f"\nüéØ PAPER DECISION:")
    decision_lines = result['paper_decision'].split('\n')
    for line in decision_lines[:3]:
        print(f"   {line}")
    
    # Print full JSON for inspection
    print("\n" + "=" * 80)
    print("FULL JSON OUTPUT")
    print("=" * 80)
    print(json.dumps(result, indent=2, ensure_ascii=False))
    
    # Validation checks
    print("\n" + "=" * 80)
    print("VALIDATION CHECKS")
    print("=" * 80)
    
    checks = {
        "Summary exists": bool(result['summary']),
        "Has strengths": len(result['strengths']) > 0,
        "Has weaknesses": len(result['weaknesses']) > 0,
        "Has questions": len(result['questions']) > 0,
        "Soundness rating exists": bool(result['soundness']),
        "Presentation rating exists": bool(result['presentation']),
        "Contribution rating exists": bool(result['contribution']),
        "Overall rating exists": bool(result['rating']),
        "Paper decision exists": bool(result['paper_decision']),
    }
    
    all_passed = True
    for check, passed in checks.items():
        status = "‚úÖ PASS" if passed else "‚ùå FAIL"
        print(f"   {status}: {check}")
        if not passed:
            all_passed = False
    
    print("\n" + "=" * 80)
    if all_passed:
        print("‚úÖ ALL CHECKS PASSED!")
    else:
        print("‚ùå SOME CHECKS FAILED!")
    print("=" * 80)
    
    return 0 if all_passed else 1

if __name__ == "__main__":
    sys.exit(test_parser())

