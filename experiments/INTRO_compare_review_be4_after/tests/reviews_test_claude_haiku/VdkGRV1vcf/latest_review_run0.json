{
  "summary": "This paper proposes an efficient computational framework for Maximum Mean Discrepancy (MMD) gradient flows using Riesz kernels. The key innovation is proving that the MMD with Riesz kernels coincides with sliced MMD in 1D, enabling efficient gradient computation via sorting algorithms with complexity O((M+N)log(M+N)) instead of O(MN+N²). The authors apply this to train generative models by approximating MMD particle flows with neural networks, demonstrating results on MNIST, FashionMNIST, CIFAR10, and CelebA.",
  "strengths": [
    "Strong theoretical contributions: Theorem 1 elegantly shows that sliced Riesz kernels are Riesz kernels, with clear mathematical exposition and rigorous proofs. The sorting-based algorithm with linear-log complexity is a significant computational advance.",
    "Comprehensive mathematical framework: The paper establishes important connections (Theorem 2) between MMD with distance kernels and Wasserstein-1 distance, providing valuable theoretical insights about the relationship between different probability metrics.",
    "Practical efficiency gains: The demonstrated speedups in gradient computation (Figure 1, left) are substantial and well-documented. The O(√d/P) convergence rate (Theorem 4) shows favorable scaling in high dimensions, which is crucial for real applications.",
    "Solid experimental validation: Image generation results are competitive with state-of-the-art methods on standard benchmarks (Table 1), with particularly strong FID scores on MNIST (3.1) and FashionMNIST (11.3).",
    "Well-structured presentation: Mathematical proofs are detailed and rigorous, algorithmic descriptions are clear, and the iterative training approach (Remark 6) is practical and well-motivated for GPU memory constraints."
  ],
  "weaknesses": [
    "Limited novelty in generative modeling: While the MMD gradient flow optimization is novel, the generative model architecture using sequential UNets trained separately is relatively straightforward. The iterative training procedure is not particularly innovative compared to recent gradient flow-based generative models.",
    "Restricted to Riesz kernels: The computational advantages only apply specifically to Riesz kernels with r=1 (negative distance kernel). The framework doesn't readily generalize to other kernels commonly used in practice, limiting applicability. The comparison with other kernels (Appendix 11) shows issues but doesn't provide solutions.",
    "Performance gaps on complex datasets: While results on CIFAR10 (FID 54.8) and CelebA (FID 32.1) are reasonable, they lag behind leading diffusion models and GANs. The pyramidal schedule and locally-connected projections introduced for CIFAR10 feel somewhat ad-hoc and undermine the generality of the approach.",
    "Incomplete experimental analysis: (1) No ablation study on key hyperparameters like momentum parameter m, step size τ, or total number of networks L. (2) Limited comparison with other particle-based generative methods. (3) The batching limitation (Limitations section) is acknowledged but not addressed, which is significant for practical scalability.",
    "Dimension-quadratic complexity trade-off: While reducing sample complexity from O(N²) to O(N log N), the method introduces O(d²(N+M)log(N+M)) complexity to achieve fixed approximation error (Remark 5). For very high dimensions, this could be prohibitive, conflicting with claims about high-dimensional scalability."
  ],
  "clarity_score": 8,
  "novelty_score": 7,
  "technical_quality_score": 8,
  "experimental_rigor_score": 6,
  "overall_score": 7,
  "confidence": 4,
  "recommendation": "Accept",
  "detailed_comments": "This is a solid paper with genuine theoretical contributions and practical algorithmic improvements. Theorem 1 and the associated sorting algorithm represent meaningful advances in efficient computation of MMD gradients. The mathematical framework is rigorous and the presentation is generally clear. However, the generative modeling application, while competent, feels somewhat incremental—the main contribution is in the optimization machinery rather than in novel generative model design. The experimental results are competitive but not state-of-the-art, and the restriction to Riesz kernels limits broader impact. The paper would be strengthened by either (1) deeper investigation of why other kernels don't work well and potential solutions, or (2) more extensive ablations and comparisons for the generative modeling component. The dimension-quadratic complexity trade-off deserves more critical discussion. Despite these limitations, the theoretical insights and computational efficiencies are valuable contributions to the field of gradient flows and should be of interest to the NeurIPS/ICML community.",
  "paper_id": "VdkGRV1vcf",
  "version": "latest",
  "run_id": 0,
  "model_type": "Anthropic",
  "success": true,
  "soundness": 8,
  "presentation": 8,
  "contribution": 7,
  "rating": 7
}