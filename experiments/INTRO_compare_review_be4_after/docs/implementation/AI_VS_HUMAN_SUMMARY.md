# AI vs Human Review Score Comparison - Implementation Summary

## Overview

Added functionality to compare AI-generated review scores against official human review scores from OpenReview, enabling quantitative evaluation of AI reviewer accuracy.

## New Files

### 1. `fetch_human_scores.py`
**Purpose:** Fetch official human review scores from OpenReview API

**Key Features:**
- Uses OpenReview API v2 (`https://api2.openreview.net`)
- Follows [official OpenReview guide](https://docs.openreview.net/how-to-guides/data-retrieval-and-modification/how-to-get-all-notes-for-submissions-reviews-rebuttals-etc)
- Gets submissions with `details='replies'` to retrieve all reviews efficiently
- Filters replies for official reviews (invitations ending with "Official_Review")
- Extracts numerical scores from official reviews
- Aggregates across multiple reviewers (mean Â± std)
- Adds columns to CSV: `num_reviews`, `human_{metric}_mean`, `human_{metric}_std`
- Rate limiting (0.5s delay) to respect API
- Exponential backoff retry on errors
- **Debug mode** (`--debug`) for troubleshooting

**Usage:**
```bash
python fetch_human_scores.py --csv_file ./data/ICLR2024_pairs/filtered_pairs.csv
```

**Output:** `filtered_pairs_with_human_scores.csv` with 11 additional columns

### 2. `calculate_mse_mae.py`
**Purpose:** Calculate error metrics comparing AI vs Human scores

**Key Features:**
- Loads human scores from CSV, AI scores from review JSONs
- Supports all model formats (SEA-E, CycleReviewer, GenericStructured, Anthropic)
- Calculates MSE, MAE, RMSE, Pearson correlation
- Generates 3 types of plots
- Handles multiple runs (averages AI scores across runs)

**Metrics Computed:**
- **MSE** (Mean Squared Error)
- **MAE** (Mean Absolute Error) - Most interpretable
- **RMSE** (Root MSE) - Penalizes large errors
- **Correlation** - Agreement strength (-1 to 1)

**Usage:**
```bash
python calculate_mse_mae.py \
  --csv_file ./data/ICLR2024_pairs/filtered_pairs_with_human_scores.csv \
  --reviews_dir ./reviews_vllm_Llama3-1_70B_3_runs/ \
  --output_dir ./ai_vs_human_evaluation/
```

**Output Files:**
- `ai_vs_human_results.csv` - Summary statistics
- `ai_vs_human_detailed.csv` - All score pairs
- `ai_vs_human_results.json` - Full results
- 3 PNG plots (scatter, error distribution, bar chart)

### 3. `AI_VS_HUMAN_GUIDE.md`
**Purpose:** Comprehensive user documentation

**Contents:**
- Step-by-step workflow
- Detailed usage examples
- Metric interpretations
- Plot explanations
- Troubleshooting
- Research questions addressed

## Visualization Plots

### 1. Scatter Plot: AI vs Human Scores
- 4 subplots (one per metric)
- Red dashed line = perfect agreement
- Green line = regression line
- Statistics box: correlation, MAE, RMSE

### 2. Error Distribution Plot
- Histogram + KDE of prediction errors (AI - Human)
- Black dashed line at zero (perfect prediction)
- Red line at mean error
- Shows systematic bias if mean â‰  0

### 3. MAE/RMSE Bar Chart
- Side-by-side comparison across metrics
- Blue = MAE, Red = RMSE
- Lower is better

## Technical Implementation

### Score Extraction Logic

**Human scores (from OpenReview):**
```python
# Parse "3 good" â†’ 3, "8: accept, good paper" â†’ 8
score = float(content['rating'].split(':')[0].strip())
```

**AI scores (from review JSONs):**
- SEA-E: Direct extraction
- CycleReviewer: Average across 4 reviewers
- GenericStructured: Direct extraction
- Anthropic: Uses mapped fields (soundness, presentation, contribution, rating)

### Error Calculation

```python
# For each metric
human_scores = [3.0, 2.5, 3.5, ...]
ai_scores = [3.2, 2.3, 3.7, ...]

mae = mean(abs(ai_scores - human_scores))
mse = mean((ai_scores - human_scores) ** 2)
rmse = sqrt(mse)
correlation = pearson_r(ai_scores, human_scores)
```

## Workflow Integration

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ filtered_pairs.csv  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ fetch_human_scores.py       â”‚ â† Queries OpenReview API
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ filtered_pairs_with_human_scores.csvâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                â”‚
           â–¼                â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ AI Reviewsâ”‚    â”‚ Human Scores â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚
          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ calculate_mse_mae.py â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ MSE/MAE Results     â”‚
       â”‚ + Plots             â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Dependencies

All dependencies already in `requirements.txt`:
- `requests` - For OpenReview API calls
- `pandas` - Data manipulation
- `numpy` - Numerical computations
- `matplotlib` - Plotting
- `seaborn` - Statistical visualization
- `scipy` - KDE for error distributions

## API Usage Notes

### OpenReview API v2
- Endpoint: `https://api2.openreview.net`
- No authentication required for public reviews
- Rate limiting implemented (0.5s delay)
- Retry logic with exponential backoff
- Follows [official API v2 guide](https://docs.openreview.net/how-to-guides/data-retrieval-and-modification/how-to-get-all-notes-for-submissions-reviews-rebuttals-etc)

### Query Pattern
**Correct approach** (per OpenReview docs):
```python
# Get submission with all replies
GET /notes?id={paper_id}&details=replies

# Access reviews from replies
submission = response.json()['notes'][0]
replies = submission['details']['replies']

# Filter for official reviews
review_notes = [
    reply for reply in replies 
    if any(inv.endswith('Official_Review') for inv in reply['invitations'])
]
```

**Why this works better:**
- Gets submission and all replies in one API call (faster)
- Reviews are nested as replies to submissions
- Invitations follow pattern: `{venue_id}/Submission{N}/-/Official_Review`
- More reliable than searching by forum ID

## Research Applications

### Questions Answered
1. **Accuracy**: How close are AI scores to human consensus?
2. **Bias**: Does AI systematically over/under-score?
3. **Consistency**: Which metrics show best AI-human agreement?
4. **Improvement**: Do AI scores align better for revised papers?

### Example Findings
```
SOUNDNESS:
  MAE: 0.42 (on 1-4 scale)
  Correlation: 0.68 (strong positive)
  Interpretation: AI captures ~68% of human variance

RATING:
  MAE: 0.82 (on 1-10 scale)
  Correlation: 0.71 (strong positive)
  Interpretation: AI overall judgments align well with humans
```

## Error Handling

### fetch_human_scores.py
- **API timeout**: Retry up to 3 times with exponential backoff
- **Missing reviews**: Gracefully handles (num_reviews = 0)
- **Malformed content**: Tries multiple field name variations
- **Rate limiting**: Automatic 0.5s delay between requests

### calculate_mse_mae.py
- **Missing AI reviews**: Skips papers without reviews
- **Score mismatch**: Only compares when both scores available
- **String parsing**: Handles various score formats ("3 good", "8: accept")
- **Plot failures**: Continues execution, prints warning

## Code Structure

```python
# fetch_human_scores.py
â”œâ”€â”€ get_paper_reviews(paper_id) â†’ API call
â”œâ”€â”€ extract_scores_from_review(note) â†’ Parse scores
â”œâ”€â”€ aggregate_review_scores(paper_id) â†’ Mean/std across reviewers
â””â”€â”€ main() â†’ Process all papers, save CSV

# calculate_mse_mae.py
â”œâ”€â”€ load_ai_reviews(dir, paper_id, version) â†’ Load JSONs
â”œâ”€â”€ extract_ai_scores(review_data) â†’ Handle all formats
â”œâ”€â”€ calculate_errors(human_df, ai_df) â†’ Compute metrics
â”œâ”€â”€ create_comparison_plots(merged_df) â†’ Generate visualizations
â””â”€â”€ main() â†’ Full workflow
```

## Performance

### fetch_human_scores.py
- **Time**: ~0.5s per paper (rate limiting)
- **125 papers**: ~1-2 minutes
- **Network**: Requires stable internet

### calculate_mse_mae.py
- **Time**: < 10 seconds for 125 papers
- **Memory**: Minimal (all data fits in memory)
- **Plotting**: ~5 seconds

## Future Enhancements

Potential additions:
1. **Confidence intervals** for MAE/correlation
2. **Per-paper analysis** (which papers show biggest AI-human disagreement)
3. **Temporal trends** (does accuracy improve over time?)
4. **Calibration curves** (are AI confidence scores well-calibrated?)
5. **Multi-model comparison** (compare MAE across different AI models)

## Summary

| Aspect | Details |
|--------|---------|
| **Files Added** | 3 (2 scripts + 1 guide) |
| **Lines of Code** | ~800 lines |
| **External APIs** | OpenReview API v2 |
| **Metrics** | MSE, MAE, RMSE, Correlation |
| **Plot Types** | 3 (scatter, error dist, bar chart) |
| **Documentation** | Complete user guide |
| **Integration** | Seamless with existing workflow |

This addition enables quantitative evaluation of AI reviewer accuracy against the gold standard of human expert reviews! ğŸ¯ğŸ“Š

