{
  "summary": "This paper investigates the role of Polyak (heavy-ball) momentum in stochastic gradient descent (SGD). It provides a theoretical analysis suggesting that momentum offers negligible benefits over vanilla SGD in the small learning rate, noise-dominated regime. The authors argue that momentum's primary practical benefit is stabilizing training against curvature-induced instability, which allows for larger learning rates, rather than reducing gradient variance. This theory is supported by two analyses: a weak approximation result showing SGD and SGDM are close in O(1/η) steps, and a stochastic differential equation (SDE) analysis showing they share the same implicit bias on a manifold of minimizers in O(1/η²) steps. Experiments on ImageNet (small-batch) and language model fine-tuning are presented to support these claims, with a specific experiment using SVAG to isolate and remove the benefits of momentum.",
  "strengths": [
    "The paper tackles a fundamental and widely misunderstood topic: the precise role of momentum in deep learning.",
    "The core conceptual move—decoupling momentum's effect on curvature-instability (large-LR) from its effect on gradient-noise (small-LR)—is insightful and well-argued.",
    "The warm-up example in Section 3.1 is an excellent, simple illustration of how momentum reduces single-step variance but *not* the total trajectory variance, directly challenging common folklore.",
    "The theoretical results in both the O(1/η) and O(1/η²) regimes are comprehensive, building on established SDE approximation frameworks to provide a unified negative result.",
    "The ImageNet experiment (Figure 1b) is well-designed, correctly using the effective learning rate γ/(1−β) for comparison and clearly demonstrating the claims in that specific setting.",
    "The SVAG experiment (Figure 3) is a clever methodological idea, providing a way to empirically test the theory by 'dialing' the training regime from curvature-dominated to noise-dominated."
  ],
  "weaknesses": [
    "The paper's theoretical presentation is severely undermined by inconsistent and confusing assumptions about the gradient noise scale σ.",
    "Specifically, Theorem 9 (Regime I) assumes σ ≤ η⁻¹/² while Theorem 14 (Regime II) assumes σ ≡ 1. These different scalings, while standard in their respective SDE sub-literatures, are contradictory and their use is never justified, explained, or reconciled.",
    "The experimental methodology for the CIFAR-10 (Figure 3) and Language Model (Figure 4) experiments is opaquely and inconsistently described.",
    "The authors mix notation from their re-parameterization (Def 3), the standard formulation (Eq 1), and a third formulation (Def 16), making it impossible for a reviewer to verify if the 'effective learning rate' comparison was correctly implemented.",
    "The analysis is strictly limited to heavy-ball momentum and vanilla SGD, ignoring Nesterov momentum and, more critically, adaptive methods like Adam. This omission misses a key opportunity to contrast why momentum's β₁ is critical in Adam but (as argued here) marginal in SGD.",
    "The theoretical assumptions (e.g., C∞-smooth loss) are standard but strong, and the gap between these asymptotic-in-η results and practical, finite-LR settings is not fully bridged.",
    "The weak approximation bound in Theorem 9, O(√(η/(1−β))), suggests the approximation worsens as β → 1, which appears to contradict the paper's conclusion that momentum has marginal value."
  ],
  "questions": [
    "Please clarify and justify the contradictory noise scaling assumptions. Why is σ ≤ η⁻¹/² required for Theorem 9, while σ ≡ 1 is used for Theorem 14? The paper must provide a clear rationale for why each scaling is appropriate for its respective regime.",
    "Please rewrite the experimental details (especially Section 5.2 and Appendix 11.2) to be unambiguous. For *every* experiment, please state *explicitly* which optimizer formulation (Eq 1, Def 2, Def 3, or Def 16) was used for each line in the plots, and what the precise hyperparameters (η, γ, β, μ, τ, etc.) were for each.",
    "The SVAG experiment (Fig 3) is the paper's strongest empirical evidence. Can you confirm if the comparison was SGD(Def 2, $\bar{\\eta}$=0.2) vs SGDM(Def 3, η=0.2, β=0.9)? If so, please state this clearly, as the current text is contradictory.",
    "How does your analysis explain the role of β₁ in Adam? If momentum's value is marginal for noise reduction, why is it so essential for adaptive methods, which are also often used in noise-dominated, small-LR fine-tuning regimes?",
    "Regarding Theorem 9's O(√(η/(1−β))) bound: this suggests the approximation breaks down as β→1. Doesn't this imply that high momentum *does* make SGDM behave differently from SGD, even if not 'better'? Please elaborate on this."
  ],
  "limitations_and_societal_impact": "The authors acknowledge their analysis is limited to small-LR regimes and does not apply to large-batch training where momentum is empirically beneficial. They do not, however, sufficiently discuss the limitations of their strong theoretical assumptions (e.g., C∞-smoothness) which are not met in practice by ReLU networks. A discussion of how these assumptions might affect the conclusions would be beneficial. The paper does not address societal impact, which is appropriate as the work is purely theoretical and algorithmic.",
  "soundness": 2,
  "presentation": 1,
  "contribution": 3,
  "overall_score": 4,
  "confidence": 5,
  "rating": 4,
  "paper_id": "3JjJezzVkT",
  "version": "latest",
  "run_id": 0,
  "model_type": "gemini_manual_CriticalNeurIPS",
  "success": true,
  "source": "manual_gemini_ui"
}