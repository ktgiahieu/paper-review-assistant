{
  "summary": "This paper introduces 'Neural Architecture Retrieval' (NAR), a new problem focused on retrieving structurally similar neural architectures from a large database. The authors propose a graph representation learning framework that addresses the challenges of large graph sizes and repetitive motifs. Their method involves a 'motif sampling' strategy to identify subgraphs, which are then embedded using a GCN. These motif embeddings serve as nodes in a 'macro graph' representation of the full architecture. The model is trained using a multi-level contrastive learning objective, supervised by coarse-grained architecture family labels. The authors demonstrate their method's superiority over standard GNN baselines on a new dataset of 12,000 real-world architectures and a synthetic NAS dataset.",
  "strengths": [
    "The paper introduces a novel and relevant problem: Neural Architecture Retrieval (NAR), which could be a valuable tool for researchers given the proliferation of model architectures.",
    "The core idea of a hierarchical representation (motifs-to-macro-graph) is a sound and intuitive approach to handle the dual challenges of large, variable-sized graphs and the importance of repeated structural motifs.",
    "The authors have curated and released a new, large-scale dataset of 12,000 real-world neural architectures, which could be a significant contribution to the community.",
    "The two-stage contrastive learning framework (at the motif-level and graph-level) is a logical way to structure the representation learning problem."
  ],
  "weaknesses": [
    "The paper's definition of 'similarity' is fundamentally flawed. The methodology does not learn structural similarity from first principles; it learns to reproduce a human-defined, coarse-grained taxonomy (e.g., 'cnn-block', 'attention-block').",
    "The ground-truth for supervision and evaluation is derived from 'key phrases' and 'regular expressions' (Sec 4.1), not from emergent structural properties. This means the model is effectively a text-label-based classifier, not a graph structure retriever.",
    "The evaluation is circular: the system is trained to cluster architectures by 'family' and then evaluated on its ability to retrieve architectures from the same 'family'. This does not validate the claim of retrieving *similar designs*.",
    "The proposed 'motif sampling strategy' (Fig 1, Eq 2) is opaque, poorly explained, and not justified against decades of established literature on frequent subgraph mining, graphlet discovery, or graphlet-based kernels.",
    "The problem formulation critically omits the context of task and dataset. As acknowledged by the authors in other forums, architectural similarity is meaningless without considering the task (e.g., vision, NLP). A 'ResNet-like' graph for audio is not 'similar' in a practical sense to one for vision, yet this method would likely group them.",
    "The baselines (GCN, GAT) are weak strawmen. They are applied naively to the whole graph and trained on the same flawed classification task, rather than comparing against strong, established graph-level embedding (e.g., Graph2Vec) or hierarchical pooling methods."
  ],
  "questions": [
    "Can you provide a precise, algorithmic definition of the 'motif sampling strategy' (Sec 3.2, Fig 1) and justify why this novel method is superior to well-established, standard algorithms for frequent subgraph mining (e.g., gSpan)?",
    "How can you validate that your method is learning *structural* similarity, not just reproducing the *taxonomic labels* you provide as supervision? For example, what is the retrieval performance if you use an unsupervised objective and evaluate against a structural metric like graph edit distance?",
    "The core premise of task-agnostic retrieval is questionable. How would your framework handle two topologically-identical graphs where one uses 2D convolutions (for vision) and the other uses 1D convolutions (for audio)? Would they be retrieved as identical, and would this be a desirable outcome for a researcher?",
    "Why was the literature on hierarchical graph pooling (e.g., DiffPool) and program representation learning (e.g., code2vec, AST/CFG embedding) omitted? These seem highly relevant to your hierarchical approach and the graph-like nature of computational graphs.",
    "Could you clarify how the 'macro graph' connectivity is robustly determined? The 'overlapped edges' concept seems brittle and needs a formal definition."
  ],
  "limitations_and_societal_impact": "The authors do not address limitations. The primary limitation, which fundamentally undermines the current claims, is the task-agnostic nature of the retrieval. The authors must acknowledge that 'architectural similarity' is ill-defined without task context. A 'similar' architecture for a different task is not a useful retrieval. They must also acknowledge the limitation of their ground-truth labeling, which is based on human-defined keywords, not emergent structure, and thus biases the system to existing taxonomies, potentially stifling the discovery of novel, cross-domain architectural paradigms. Societal impact is not discussed but is likely minimal, as the work is a specialized tool for researchers.",
  "soundness": 2,
  "presentation": 3,
  "contribution": 2,
  "overall_score": 4,
  "confidence": 5,
  "rating": 4,
  "paper_id": "1JtTPYBKqt",
  "version": "v1",
  "run_id": 0,
  "model_type": "gemini_manual_CriticalNeurIPS",
  "success": true,
  "source": "manual_gemini_ui"
}