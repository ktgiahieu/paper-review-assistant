{
  "summary": "This paper introduces TD InfoNCE, a novel algorithm that combines temporal difference (TD) learning with the InfoNCE contrastive objective. The method is framed as a non-parametric, off-policy approach for learning the discounted state occupancy measure, effectively a deep learning analogue of the Successor Representation. The authors derive a TD(0)-style estimator for the InfoNCE loss, which allows the model to learn from off-policy data and 'stitch' trajectories. This representation is then applied to goal-conditioned reinforcement learning (GCRL), where it is used to define both the critic (a self-supervised value function) and the policy objective. Experiments on tabular gridworlds and standard online/offline GCRL benchmarks (Fetch, AntMaze) demonstrate significant improvements in sample efficiency and final performance over Monte Carlo contrastive methods and other baselines, especially in stochastic environments.",
  "strengths": [
    "The core conceptual contribution—framing the estimation of the discounted state occupancy measure as a contrastive learning problem and then deriving a TD(0) estimator (TD InfoNCE) instead of the standard Monte Carlo estimator (used by 'Contrastive RL')—is elegant, well-motivated, and provides a clear bridge between classic SR literature (Dayan, 1993) and modern deep representation learning.",
    "The empirical results are strong and comprehensive. The 1500x sample-efficiency gain over the MC baseline and 20x gain over tabular SR in the gridworld setting (Fig. 7) provide clear evidence for the method's effectiveness.",
    "The qualitative gridworld experiments (Fig. 8, Sec 4.4) that demonstrate 'stitching' and 'shortcut-finding' behavior are excellent. They provide compelling, intuitive proof that the method successfully performs dynamic programming, a key advantage of TD over MC methods.",
    "The comparison against Quasimetric RL (QRL) in stochastic environments (Fig. 6) is a critical result. It highlights a fundamental weakness in QRL's deterministic-world assumption (its Bellman-like *inequality*) and shows the strength of this work's density-based Bellman *equality*.",
    "The algorithm is shown to be effective in both online (Fig. 4) and offline (Table 1) GCRL settings, demonstrating its versatility and validating the off-policy claims.",
    "The ablation study in Appendix 10.2 that isolates the performance gain over C-Learning (Eysenbach et al., 2020) to the choice of a categorical (InfoNCE) vs. binary (NCE) loss is a valuable scientific contribution."
  ],
  "weaknesses": [
    "There is a significant gap between the algorithm that is practically implemented (Eq. 10) and the idealized algorithm that is theoretically analyzed (Appendix 6, Eq. 16). The practical algorithm uses a finite-sample, 'plug-in' estimate for the importance weight `w` (Eq. 9) which itself depends on the critic `f` being optimized.",
    "The convergence proof in Appendix 6 applies only to an idealized variant `bar{L}` that replaces this problematic finite-sample sum with an exact expectation (Eq. 14). The paper acknowledges this gap but does not analyze the bias or variance implications of this 'nested Monte Carlo' estimation on the practical algorithm's convergence.",
    "The claim in Appendix 10.2 that the N-squared negative examples has 'little effect' on performance is counter-intuitive. This N-squared computation (Eq 23) seems computationally expensive and is a key difference from C-Learning and QRL. This warrants a more thorough ablation, perhaps showing performance vs. wall-clock time, not just gradient steps or samples.",
    "The nature of the 'stochasticity' in the experiment in Fig. 6 is unclear. The text states 'observations are corrupted,' which implies observation noise. However, the accompanying argument about QRL's deterministic assumption suggests *transition* noise. This is a critical distinction that must be clarified."
  ],
  "questions": [
    "Can the authors bridge the theoretical gap? Does the practical algorithm (Eq. 10) using the stop-gradient on the finite-sample weight `w` (Eq. 9) also correspond to a contraction operator, or does it introduce biases that are not present in the idealized `bar{L}` objective?",
    "Please clarify the stochasticity experiment (Sec 4.1, Fig 6). Was the noise applied to the observations (as written) or to the environment transitions? If it was observation noise, why would this uniquely disadvantage QRL, whose quasimetric inequality should still hold?",
    "Could you elaborate on the computational cost? The `N^2` comparisons in the loss (Eq. 22-23) seem significantly more expensive per-batch than the `N*k` comparisons in QRL or C-Learning. How does TD InfoNCE compare to baselines in terms of wall-clock time to reach a given success rate?",
    "The method is a TD(0) estimator. Have you considered extending this to a TD(lambda) or multi-step return, analogous to n-step Q-learning? This might provide a better bias-variance trade-off than the pure one-step (TD) and infinite-step (MC) estimators you compare.",
    "How sensitive is the method to the number of negative samples `N` in the batch? The derivation relies on `w` (Eq 9) being a good estimate of the importance weight, which depends on the finite-sample denominator. How does performance degrade as `N` decreases?"
  ],
  "limitations_and_societal_impact": "The authors adequately address limitations in the main paper (Sec 5), correctly identifying the increased complexity/hyperparameters and the theoretical gap in the convergence proof. They do not discuss broader societal impacts, but for this type of foundational algorithmic work, the direct impacts are minimal and a detailed discussion is not strictly necessary. The work is a general-purpose RL algorithm and carries the same societal implications as the broader field (e.g., potential for automation).",
  "soundness": 3,
  "presentation": 4,
  "contribution": 4,
  "overall_score": 8,
  "confidence": 5,
  "rating": 8,
  "paper_id": "0akLDTFR9x",
  "version": "v1",
  "run_id": 0,
  "model_type": "gemini_manual_CriticalNeurIPS",
  "success": true,
  "source": "manual_gemini_ui"
}