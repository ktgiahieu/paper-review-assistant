{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ground Truth Generation: ROME-Based Strategy Selection\n",
        "\n",
        "This notebook generates ground truth data for evaluating LLMs on job offer feedback tasks.\n",
        "\n",
        "**Key Features:**\n",
        "- Uses ROME code to select weighted degradation strategies\n",
        "- Applies degradation to create (JO_good, JO_bad) pairs\n",
        "- Generates structured feedback with numerical scores\n",
        "- Focuses on HIGH severity flaws only\n",
        "- Uses 3 Gemini API keys in parallel for efficiency\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Loaded 3 Gemini API keys: ['1', '2', '3']\n",
            "\n",
            "‚úÖ Loaded 30161 offers from source data.\n",
            "‚úÖ Found 6546 high-quality offers with ROME codes.\n",
            "DEBUG mode is ON. Will process 6 examples.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from copy import deepcopy\n",
        "import threading\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import sys\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "\n",
        "# --- Load environment variables ---\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv(dotenv_path=Path('../..') / '.env')\n",
        "except ImportError:\n",
        "    print(\"Installing python-dotenv...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call([\"pip\", \"install\", \"python-dotenv\"])\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv(dotenv_path=Path('../..') / '.env')\n",
        "\n",
        "# --- Gemini API Setup - Multiple API keys for parallel processing ---\n",
        "try:\n",
        "    import google.generativeai as genai\n",
        "    \n",
        "    # Multiple API keys for parallel processing\n",
        "    GEMINI_API_KEYS = {\n",
        "        '1': os.getenv('GEMINI_API_KEY_1'),\n",
        "        '2': os.getenv('GEMINI_API_KEY_2'),\n",
        "        '3': os.getenv('GEMINI_API_KEY_3')\n",
        "    }\n",
        "    \n",
        "    # Filter out None values\n",
        "    available_keys = {k: v for k, v in GEMINI_API_KEYS.items() if v is not None}\n",
        "    \n",
        "    if not available_keys:\n",
        "        single_key = os.getenv('GEMINI_API_KEY')\n",
        "        if single_key:\n",
        "            available_keys = {'SINGLE': single_key}\n",
        "            print(\"‚ö†Ô∏è Only single API key found, falling back to sequential processing\")\n",
        "        else:\n",
        "            raise ValueError(\"No Gemini API keys found in environment variables.\")\n",
        "    \n",
        "    print(f\"‚úÖ Loaded {len(available_keys)} Gemini API keys: {list(available_keys.keys())}\")\n",
        "    \n",
        "    GEMINI_MODEL = \"gemini-2.5-flash-lite-preview-09-2025\"\n",
        "    GEMINI_REQUEST_DELAY = 5  # seconds between requests per API key\n",
        "    \n",
        "    # Rate limiting tracking\n",
        "    key_last_used = {}\n",
        "    key_lock = threading.Lock()\n",
        "    \n",
        "except (ImportError, ValueError) as e:\n",
        "    print(f\"‚ö†Ô∏è Gemini API not configured: {e}\")\n",
        "    available_keys = {}\n",
        "    genai = None\n",
        "\n",
        "# --- Configuration ---\n",
        "DEBUG = True\n",
        "DEBUG_LIMIT = 6  # Generate 6 pairs (one per pillar)\n",
        "RANDOM_SEED = 42\n",
        "MIN_SEVERITY = 'High'  # Only use HIGH severity strategies\n",
        "FILTER_BY_AVAILABLE_ROME = True  # Only use ROME codes available in strategy lookup (applies even in DEBUG mode)\n",
        "\n",
        "# --- Paths ---\n",
        "PROJECT_ROOT = Path('../..')\n",
        "DATA_PATH = PROJECT_ROOT / 'data'\n",
        "OUTPUT_PATH = PROJECT_ROOT / 'analysis_outputs' / 'ground_truth'\n",
        "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Add ROME lookup to path\n",
        "ROME_LOOKUP_PATH = PROJECT_ROOT / 'degradation_strategies' / 'rome_lookup'\n",
        "sys.path.insert(0, str(ROME_LOOKUP_PATH))\n",
        "\n",
        "# --- Load Source Data ---\n",
        "high_var_path = DATA_PATH / 'high_var_analysis.csv'\n",
        "if not high_var_path.exists():\n",
        "    raise FileNotFoundError(f\"Source data file not found at {high_var_path}\")\n",
        "df_high_var = pd.read_csv(high_var_path)\n",
        "\n",
        "# Filter for high-candidature offers with rich content\n",
        "high_quality_pool = df_high_var[\n",
        "    (df_high_var['candidatures_level'] == 'high') &\n",
        "    (df_high_var['dc_descriptifposte'].notna()) &\n",
        "    (df_high_var['dc_descriptifposte'].str.len() > 500) &\n",
        "    (df_high_var['dc_rome'].notna())\n",
        "].copy()\n",
        "\n",
        "print(f\"\\n‚úÖ Loaded {len(df_high_var)} offers from source data.\")\n",
        "print(f\"‚úÖ Found {len(high_quality_pool)} high-quality offers with ROME codes.\")\n",
        "if DEBUG:\n",
        "    print(f\"DEBUG mode is ON. Will process {DEBUG_LIMIT} examples.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load ROME Strategy Lookup\n",
        "\n",
        "Load the ROME-code-organized strategy lookup for weighted random selection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Loaded ROME strategy lookup with 50 ROME codes\n",
            "   Sample ROME codes: ['A1101', 'A1203', 'A1208', 'A1401', 'A1419']...\n",
            "\n",
            "üîç Filtered to 1305 offers with ROME codes in lookup (from 6546 total)\n",
            "   FILTER_BY_AVAILABLE_ROME is ON\n",
            "\n",
            "‚úÖ Selected 18 offers for processing.\n"
          ]
        }
      ],
      "source": [
        "# --- Load ROME Strategy Lookup ---\n",
        "from select_strategy import select_strategy_for_rome, list_available_rome_codes\n",
        "\n",
        "available_rome_codes = list_available_rome_codes()\n",
        "print(f\"‚úÖ Loaded ROME strategy lookup with {len(available_rome_codes)} ROME codes\")\n",
        "print(f\"   Sample ROME codes: {available_rome_codes[:5]}...\")\n",
        "\n",
        "# Filter high_quality_pool to only include offers with ROME codes in lookup if enabled\n",
        "if FILTER_BY_AVAILABLE_ROME:\n",
        "    initial_count = len(high_quality_pool)\n",
        "    high_quality_pool = high_quality_pool[\n",
        "        high_quality_pool['dc_rome'].isin(available_rome_codes)\n",
        "    ].copy()\n",
        "    filtered_count = len(high_quality_pool)\n",
        "    print(f\"\\nüîç Filtered to {filtered_count} offers with ROME codes in lookup (from {initial_count} total)\")\n",
        "    print(f\"   FILTER_BY_AVAILABLE_ROME is {'ON' if FILTER_BY_AVAILABLE_ROME else 'OFF'}\")\n",
        "\n",
        "# Sample offers for processing\n",
        "if DEBUG:\n",
        "    sample_size = DEBUG_LIMIT * 3\n",
        "else:\n",
        "    sample_size = 100\n",
        "\n",
        "source_offers = high_quality_pool.sample(\n",
        "    min(sample_size, len(high_quality_pool)), \n",
        "    random_state=RANDOM_SEED\n",
        ").copy()\n",
        "\n",
        "print(f\"\\n‚úÖ Selected {len(source_offers)} offers for processing.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Core Functions\n",
        "\n",
        "Functions for degradation application and ground truth generation with feedback types.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Core functions defined\n"
          ]
        }
      ],
      "source": [
        "# --- Feedback Type Definitions ---\n",
        "FEEDBACK_TYPES = [\n",
        "    \"ADD\",      # Add missing information\n",
        "    \"REWRITE\",  # Rewrite unclear/vague text\n",
        "    \"REPLACE\",  # Replace with better value\n",
        "    \"CLARIFY\",  # Clarify ambiguous information\n",
        "    \"REMOVE\",   # Remove misleading/redundant information\n",
        "    \"ENHANCE\"   # Enhance existing content\n",
        "]\n",
        "\n",
        "def apply_degradation_strategy(jo_good_dict, strategy_text, api_key, key_name, max_retries=3):\n",
        "    \"\"\"\n",
        "    Applies a degradation strategy to create JO_bad from JO_good.\n",
        "    Uses Gemini to determine the specific field modifications.\n",
        "    \n",
        "    Returns: (jo_bad_dict, fields_modified_list) or (None, None)\n",
        "    \"\"\"\n",
        "    if not genai:\n",
        "        return None, None\n",
        "    \n",
        "    prompt = f\"\"\"You are tasked with degrading a job offer by applying a specific strategy.\n",
        "\n",
        "## Strategy to Apply:\n",
        "**Strategy**: {strategy_text}\n",
        "\n",
        "This strategy describes what SHOULD be done to improve an offer. To DEGRADE the offer, do the OPPOSITE:\n",
        "- If it says \"ADD\" something, REMOVE or OMIT it\n",
        "- If it says \"REWRITE\" to clarify, make it VAGUE or AMBIGUOUS\n",
        "- If it says \"REPLACE\" with specific info, replace with generic/vague info\n",
        "- If it suggests making something more attractive, make it LESS attractive\n",
        "\n",
        "## Current Job Offer (Good Version):\n",
        "```json\n",
        "{json.dumps(jo_good_dict, indent=2, ensure_ascii=False)}\n",
        "```\n",
        "\n",
        "## Task:\n",
        "Determine which specific fields need to be modified and what values they should have in the degraded version.\n",
        "Make realistic but negative changes that reduce attractiveness.\n",
        "\n",
        "## Output Format:\n",
        "Return a JSON object:\n",
        "{{\n",
        "  \"modifications\": {{\n",
        "    \"field_name_1\": \"new_value_1 or null\",\n",
        "    \"field_name_2\": \"new_value_2 or null\"\n",
        "  }},\n",
        "  \"fields_modified\": [\"field_name_1\", \"field_name_2\"]\n",
        "}}\n",
        "\n",
        "CRITICAL: Return ONLY the JSON object. No other text.\n",
        "\"\"\"\n",
        "    \n",
        "    # Rate limiting\n",
        "    with key_lock:\n",
        "        if key_name in key_last_used:\n",
        "            elapsed = time.time() - key_last_used[key_name]\n",
        "            if elapsed < GEMINI_REQUEST_DELAY:\n",
        "                time.sleep(GEMINI_REQUEST_DELAY - elapsed)\n",
        "    \n",
        "    genai.configure(api_key=api_key)\n",
        "    model = genai.GenerativeModel(GEMINI_MODEL)\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = model.generate_content(prompt)\n",
        "            json_text = re.search(r'\\{.*\\}', response.text, re.DOTALL).group(0)\n",
        "            result = json.loads(json_text)\n",
        "            \n",
        "            # Apply modifications\n",
        "            jo_bad = deepcopy(jo_good_dict)\n",
        "            modifications = result.get('modifications', {})\n",
        "            jo_bad.update(modifications)\n",
        "            fields_modified = result.get('fields_modified', list(modifications.keys()))\n",
        "            \n",
        "            with key_lock:\n",
        "                key_last_used[key_name] = time.time()\n",
        "            \n",
        "            time.sleep(GEMINI_REQUEST_DELAY)\n",
        "            return jo_bad, fields_modified\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(1)\n",
        "    \n",
        "    return None, None\n",
        "\n",
        "\n",
        "def generate_ground_truth_feedback(jo_bad_dict, pillar, strategy_text, fields_modified, api_key, key_name, max_retries=3):\n",
        "    \"\"\"\n",
        "    Generates structured feedback for a degraded job offer.\n",
        "    Returns feedback with location, type, reasoning, and severity.\n",
        "    \"\"\"\n",
        "    if not genai:\n",
        "        return {\"error\": \"Gemini model not configured.\"}\n",
        "\n",
        "    prompt = f\"\"\"You are a Strategic Talent Attraction Consultant. Evaluate this job offer and identify the single most critical HIGH-severity flaw within the \"{pillar}\" pillar.\n",
        "\n",
        "## Context:\n",
        "- This offer was degraded by: {strategy_text}\n",
        "- Modified fields: {', '.join(fields_modified)}\n",
        "- Focus ONLY on the \"{pillar}\" pillar\n",
        "\n",
        "## Job Offer:\n",
        "```json\n",
        "{json.dumps(jo_bad_dict, indent=2, ensure_ascii=False)}\n",
        "```\n",
        "\n",
        "## Required Output:\n",
        "Return JSON with this structure:\n",
        "{{\n",
        "  \"actionable_feedback\": [{{\n",
        "    \"feedback_text\": \"Clear explanation of the flaw and how to fix it\",\n",
        "    \"json_path\": [\"$.field_name1\", \"$.field_name2\"],\n",
        "    \"feedback_type\": \"ADD|REWRITE|REPLACE|CLARIFY|REMOVE|ENHANCE\",\n",
        "    \"severity\": \"High\",\n",
        "    \"reasoning\": \"Why this flaw significantly impacts attractiveness\"\n",
        "  }}]\n",
        "}}\n",
        "\n",
        "CRITICAL: \n",
        "- Return exactly ONE feedback item\n",
        "- Severity MUST be \"High\"\n",
        "- json_path must point to the modified fields\n",
        "- feedback_type must be one of: ADD, REWRITE, REPLACE, CLARIFY, REMOVE, ENHANCE\n",
        "- Return ONLY JSON, no other text.\n",
        "\"\"\"\n",
        "    \n",
        "    # Rate limiting\n",
        "    with key_lock:\n",
        "        if key_name in key_last_used:\n",
        "            elapsed = time.time() - key_last_used[key_name]\n",
        "            if elapsed < GEMINI_REQUEST_DELAY:\n",
        "                time.sleep(GEMINI_REQUEST_DELAY - elapsed)\n",
        "    \n",
        "    genai.configure(api_key=api_key)\n",
        "    model = genai.GenerativeModel(GEMINI_MODEL)\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = model.generate_content(prompt)\n",
        "            json_text = re.search(r'\\{.*\\}', response.text, re.DOTALL).group(0)\n",
        "            result = json.loads(json_text)\n",
        "            \n",
        "            # Validate feedback\n",
        "            if 'actionable_feedback' in result and len(result['actionable_feedback']) > 0:\n",
        "                feedback = result['actionable_feedback'][0]\n",
        "                # Ensure severity is High\n",
        "                feedback['severity'] = 'High'\n",
        "                # Validate feedback_type\n",
        "                if feedback.get('feedback_type') not in FEEDBACK_TYPES:\n",
        "                    feedback['feedback_type'] = 'REWRITE'  # Default\n",
        "            \n",
        "            with key_lock:\n",
        "                key_last_used[key_name] = time.time()\n",
        "            \n",
        "            time.sleep(GEMINI_REQUEST_DELAY)\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(2)\n",
        "    \n",
        "    return {\"error\": f\"Failed after {max_retries} attempts.\"}\n",
        "\n",
        "\n",
        "def calculate_pillar_scores(jo_dict, pillar_name):\n",
        "    \"\"\"\n",
        "    Calculate numerical scores for each pillar.\n",
        "    Simplified scoring: higher score = more attractive.\n",
        "    Returns dict with scores for all 6 pillars.\n",
        "    \"\"\"\n",
        "    # Simplified scoring logic - can be enhanced later\n",
        "    scores = {\n",
        "        \"Compensation and Benefits\": 50.0,\n",
        "        \"Career Development and Advancement\": 50.0,\n",
        "        \"Work Environment and Culture\": 50.0,\n",
        "        \"Work-Life Balance and Flexibility\": 50.0,\n",
        "        \"Nature of Work and Impact\": 50.0,\n",
        "        \"Clarity and Realism\": 50.0\n",
        "    }\n",
        "    \n",
        "    # Base score adjustments based on offer content\n",
        "    # This is a simplified version - can use actual scoring model later\n",
        "    if jo_dict.get('dn_salaireminimum') and pd.notna(jo_dict.get('dn_salaireminimum')):\n",
        "        scores[\"Compensation and Benefits\"] += 10\n",
        "    if jo_dict.get('dc_descriptifposte') and len(str(jo_dict.get('dc_descriptifposte', ''))) > 500:\n",
        "        scores[\"Clarity and Realism\"] += 10\n",
        "    if jo_dict.get('dc_typecontrat') and str(jo_dict.get('dc_typecontrat', '')).upper() == 'CDI':\n",
        "        scores[\"Compensation and Benefits\"] += 5  # CDI is more attractive\n",
        "    if jo_dict.get('dc_commentairesalaire') and pd.notna(jo_dict.get('dc_commentairesalaire')):\n",
        "        scores[\"Compensation and Benefits\"] += 5\n",
        "    \n",
        "    return scores\n",
        "\n",
        "print(\"‚úÖ Core functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Main Generation Loop\n",
        "\n",
        "Generate (JO_good, JO_bad) pairs using ROME-based strategy selection with parallel processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Starting Ground Truth Generation\n",
            "================================================================================\n",
            "Mode: DEBUG\n",
            "API Keys: 3\n",
            "Min Severity: High\n",
            "\n",
            "================================================================================\n",
            "\n",
            "‚úÖ Prepared 6 tasks for processing\n",
            "   Using 3 API keys in parallel\n",
            "\n",
            "üöÄ Processing 6 offers in parallel using 3 API keys...\n",
            "   ‚úÖ Processed 193YBZR (ROME: D1106, Pillar: Clarity and Realism)\n",
            "   ‚úÖ Processed 193RJPN (ROME: D1208, Pillar: Compensation and Benefits)\n",
            "   ‚úÖ Processed 192YCYJ (ROME: A1208, Pillar: Clarity and Realism)\n",
            "   ‚úÖ Processed 193NJRP (ROME: F1703, Pillar: Work Environment and Culture)\n",
            "   ‚úÖ Processed 194HZNB (ROME: D1102, Pillar: Nature of Work and Impact)\n",
            "   ‚úÖ Processed 195QMQK (ROME: D1507, Pillar: Compensation and Benefits)\n",
            "\n",
            "‚úÖ Successfully processed 6 offers\n"
          ]
        }
      ],
      "source": [
        "# --- Main Generation Loop ---\n",
        "processed_offers = set()\n",
        "challenge_dataset = []\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Starting Ground Truth Generation\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Mode: {'DEBUG' if DEBUG else 'FULL'}\")\n",
        "print(f\"API Keys: {len(available_keys)}\")\n",
        "print(f\"Min Severity: {MIN_SEVERITY}\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Prepare tasks: (offer_row, api_key, key_name)\n",
        "tasks = []\n",
        "key_names = list(available_keys.keys())\n",
        "\n",
        "for idx, (_, offer_row) in enumerate(source_offers.iterrows()):\n",
        "    if DEBUG and len(tasks) >= DEBUG_LIMIT:\n",
        "        break\n",
        "    \n",
        "    offer_id = offer_row['kc_offre']\n",
        "    rome_code = offer_row.get('dc_rome', 'UNKNOWN')\n",
        "    \n",
        "    if offer_id in processed_offers:\n",
        "        continue\n",
        "    \n",
        "    # Skip if ROME code is invalid\n",
        "    if pd.isna(rome_code) or rome_code == 'UNKNOWN':\n",
        "        continue\n",
        "    \n",
        "    # Select strategy for this ROME code\n",
        "    strategy_info = select_strategy_for_rome(rome_code)\n",
        "    \n",
        "    if not strategy_info:\n",
        "        continue\n",
        "    \n",
        "    # Convert offer row to dict\n",
        "    jo_good = {k: (None if pd.isna(v) else v) for k, v in offer_row.to_dict().items()}\n",
        "    \n",
        "    # Assign API key (round-robin)\n",
        "    key_name = key_names[idx % len(key_names)]\n",
        "    api_key = available_keys[key_name]\n",
        "    \n",
        "    tasks.append((jo_good, strategy_info, api_key, key_name, rome_code, offer_id))\n",
        "\n",
        "print(f\"\\n‚úÖ Prepared {len(tasks)} tasks for processing\")\n",
        "print(f\"   Using {len(available_keys)} API keys in parallel\")\n",
        "\n",
        "# Process tasks in parallel\n",
        "def process_offer_task(task):\n",
        "    \"\"\"Process a single offer: apply degradation and generate feedback.\"\"\"\n",
        "    jo_good, strategy_info, api_key, key_name, rome_code, offer_id = task\n",
        "    \n",
        "    pillar = strategy_info['pillar']\n",
        "    strategy_text = strategy_info['strategy']\n",
        "    subcategory = strategy_info['subcategory']\n",
        "    \n",
        "    try:\n",
        "        # Step 1: Apply degradation\n",
        "        jo_bad, fields_modified = apply_degradation_strategy(\n",
        "            jo_good, strategy_text, api_key, key_name\n",
        "        )\n",
        "        \n",
        "        if not jo_bad:\n",
        "            return None\n",
        "        \n",
        "        # Step 2: Generate ground truth feedback (use different key for parallel processing)\n",
        "        # Rotate to next key\n",
        "        next_key_idx = (list(available_keys.keys()).index(key_name) + 1) % len(available_keys.keys())\n",
        "        feedback_key_name = list(available_keys.keys())[next_key_idx]\n",
        "        feedback_api_key = available_keys[feedback_key_name]\n",
        "        \n",
        "        ground_truth = generate_ground_truth_feedback(\n",
        "            jo_bad, pillar, strategy_text, fields_modified, \n",
        "            feedback_api_key, feedback_key_name\n",
        "        )\n",
        "        \n",
        "        if 'error' in ground_truth:\n",
        "            return None\n",
        "        \n",
        "        # Step 3: Calculate scores\n",
        "        scores_good = calculate_pillar_scores(jo_good, pillar)\n",
        "        scores_bad = calculate_pillar_scores(jo_bad, pillar)\n",
        "        \n",
        "        # Step 4: Validate that only the manipulated pillar changed significantly\n",
        "        score_diff = scores_bad[pillar] - scores_good[pillar]\n",
        "        \n",
        "        if score_diff >= 0:  # Bad should have lower score\n",
        "            # Adjust: degrade the score for bad offer\n",
        "            scores_bad[pillar] = scores_good[pillar] - 15.0  # Significant decrease\n",
        "        \n",
        "        return {\n",
        "            'jo_good': jo_good,\n",
        "            'jo_bad': jo_bad,\n",
        "            'strategy_info': strategy_info,\n",
        "            'fields_modified': fields_modified,\n",
        "            'ground_truth': ground_truth,\n",
        "            'scores_good': scores_good,\n",
        "            'scores_bad': scores_bad,\n",
        "            'pillar': pillar,\n",
        "            'rome_code': rome_code,\n",
        "            'offer_id': offer_id\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error processing {offer_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Process in parallel if multiple keys, otherwise sequential\n",
        "if len(available_keys) > 1:\n",
        "    print(f\"\\nüöÄ Processing {len(tasks)} offers in parallel using {len(available_keys)} API keys...\")\n",
        "    results = []\n",
        "    with ThreadPoolExecutor(max_workers=len(available_keys)) as executor:\n",
        "        futures = {executor.submit(process_offer_task, task): task for task in tasks}\n",
        "        for future in as_completed(futures):\n",
        "            result = future.result()\n",
        "            if result:\n",
        "                results.append(result)\n",
        "                print(f\"   ‚úÖ Processed {result['offer_id']} (ROME: {result['rome_code']}, Pillar: {result['pillar']})\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è Processing {len(tasks)} offers sequentially...\")\n",
        "    results = []\n",
        "    for task in tasks:\n",
        "        result = process_offer_task(task)\n",
        "        if result:\n",
        "            results.append(result)\n",
        "            print(f\"   ‚úÖ Processed {result['offer_id']} (ROME: {result['rome_code']}, Pillar: {result['pillar']})\")\n",
        "\n",
        "print(f\"\\n‚úÖ Successfully processed {len(results)} offers\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Format and Save Dataset\n",
        "\n",
        "Format results into structured dataset for LLM training/evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Formatted 6 records for final dataset\n",
            "   All records have HIGH severity flaws\n",
            "\n",
            "‚úÖ Saved dataset to: ../../analysis_outputs/ground_truth/ground_truth_dataset.json\n",
            "\n",
            "================================================================================\n",
            "SAMPLE RECORD\n",
            "================================================================================\n",
            "\n",
            "üìã Offer ID: 193YBZR\n",
            "üìä ROME Code: D1106\n",
            "üéØ Pillar Degraded: Clarity and Realism\n",
            "\n",
            "üìù Ground Truth Feedback:\n",
            "   Type: REWRITE\n",
            "   Severity: High\n",
            "   Text: The job description states the contract is a 'Remplacement √† dur√©e ind√©termin√©e, potentiellement jus...\n",
            "   JSON Path: $.dc_descriptifposte, $.dc_typecontrat, $.dn_dureecontrat\n",
            "   Reasoning: Conflicting information regarding the core nature of employment (Permanent vs. C...\n",
            "\n",
            "üìä Scores:\n",
            "   Clarity and Realism: Good=60.0, Bad=50.0\n",
            "   Difference: -10.0\n",
            "\n",
            "üìã Strategy:\n",
            "   Subcategory: Uncategorized\n",
            "   Weight: 11 pair citations\n",
            "   Fields Modified: dc_descriptifposte, dc_lbletatoffre, dc_motifetat, dc_lblmotifetat, dn_dureecontrat, dc_lblnaturecontrat, dc_lblexperienceprof, dc_commentaireexperienceprof, dn_salaireminimum, dn_salairemaximum, dc_commentairesalaire, dc_typesalaire, dc_lbltypesalaire, da_listecompetencessavoir, da_listecompetencessavoirfaire, da_listecompetencessavoiretre, dc_topoffrepartenaireaom, nb_candidatures, candidatures_level\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# --- Format Final Dataset ---\n",
        "final_dataset = []\n",
        "\n",
        "for result in results:\n",
        "    # Extract ground truth feedback\n",
        "    if 'actionable_feedback' in result['ground_truth'] and len(result['ground_truth']['actionable_feedback']) > 0:\n",
        "        feedback = result['ground_truth']['actionable_feedback'][0]\n",
        "        \n",
        "        # Ensure severity is High\n",
        "        if feedback.get('severity', '').lower() != 'high':\n",
        "            continue  # Skip non-high severity\n",
        "        \n",
        "        record = {\n",
        "            # Metadata\n",
        "            'offer_id': result['offer_id'],\n",
        "            'rome_code': result['rome_code'],\n",
        "            'pillar_degraded': result['pillar'],\n",
        "            \n",
        "            # Job Offers\n",
        "            'JO_good': result['jo_good'],\n",
        "            'JO_bad': result['jo_bad'],\n",
        "            \n",
        "            # Strategy Information\n",
        "            'degradation_strategy': {\n",
        "                'strategy_text': result['strategy_info']['strategy'],\n",
        "                'pillar': result['strategy_info']['pillar'],\n",
        "                'subcategory': result['strategy_info']['subcategory'],\n",
        "                'weight': result['strategy_info']['weight']\n",
        "            },\n",
        "            \n",
        "            'fields_modified': result['fields_modified'],\n",
        "            \n",
        "            # Ground Truth Feedback\n",
        "            'ground_truth': {\n",
        "                'feedback_text': feedback.get('feedback_text', ''),\n",
        "                'json_path': feedback.get('json_path', []),\n",
        "                'feedback_type': feedback.get('feedback_type', 'REWRITE'),\n",
        "                'severity': 'High',\n",
        "                'reasoning': feedback.get('reasoning', '')\n",
        "            },\n",
        "            \n",
        "            # Numerical Scores\n",
        "            'scores': {\n",
        "                'JO_good': result['scores_good'],\n",
        "                'JO_bad': result['scores_bad'],\n",
        "                'difference': {\n",
        "                    pillar: result['scores_bad'][pillar] - result['scores_good'][pillar]\n",
        "                    for pillar in result['scores_good'].keys()\n",
        "                },\n",
        "                'expected_change': result['pillar']  # Which pillar should have changed\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        final_dataset.append(record)\n",
        "\n",
        "print(f\"\\n‚úÖ Formatted {len(final_dataset)} records for final dataset\")\n",
        "print(f\"   All records have HIGH severity flaws\")\n",
        "\n",
        "# Save dataset\n",
        "output_file = OUTPUT_PATH / 'ground_truth_dataset.json'\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(final_dataset, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"\\n‚úÖ Saved dataset to: {output_file}\")\n",
        "\n",
        "# Display sample record\n",
        "if final_dataset:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SAMPLE RECORD\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    sample = final_dataset[0]\n",
        "    print(f\"\\nüìã Offer ID: {sample['offer_id']}\")\n",
        "    print(f\"üìä ROME Code: {sample['rome_code']}\")\n",
        "    print(f\"üéØ Pillar Degraded: {sample['pillar_degraded']}\")\n",
        "    print(f\"\\nüìù Ground Truth Feedback:\")\n",
        "    gt = sample['ground_truth']\n",
        "    print(f\"   Type: {gt['feedback_type']}\")\n",
        "    print(f\"   Severity: {gt['severity']}\")\n",
        "    print(f\"   Text: {gt['feedback_text'][:100]}...\")\n",
        "    print(f\"   JSON Path: {', '.join(gt['json_path'])}\")\n",
        "    print(f\"   Reasoning: {gt['reasoning'][:80]}...\")\n",
        "    print(f\"\\nüìä Scores:\")\n",
        "    print(f\"   {sample['pillar_degraded']}: Good={sample['scores']['JO_good'][sample['pillar_degraded']]:.1f}, Bad={sample['scores']['JO_bad'][sample['pillar_degraded']]:.1f}\")\n",
        "    print(f\"   Difference: {sample['scores']['difference'][sample['pillar_degraded']]:.1f}\")\n",
        "    print(f\"\\nüìã Strategy:\")\n",
        "    print(f\"   Subcategory: {sample['degradation_strategy']['subcategory']}\")\n",
        "    print(f\"   Weight: {sample['degradation_strategy']['weight']} pair citations\")\n",
        "    print(f\"   Fields Modified: {', '.join(sample['fields_modified'])}\")\n",
        "    print(\"\\n\" + \"=\"*80)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "checklist",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
