#!/usr/bin/env python3
"""
Aggregate treatment effects by flaw/error category.

This script combines the per-paper score differences produced by
`evaluate_good_bad_plant_effect.py` with the paper-to-category mapping emitted by
`extract_error_categories.py`.  For each category (either the coarse group
1/2/3/4/5 or the fine-grained subcategory such as 1a, 1b, â€¦) it computes
effect-size summaries (Cohen's d with confidence intervals) for each available
comparison and review metric.

Example:
    python analyze_error_type_effects.py \
        --score_differences ../../sampled_data/.../planted_errors/score_differences.csv \
        --category_mapping ../../data/paper_error_categories.csv \
        --category_level subcategory \
        --output_dir ../../sampled_data/.../planted_errors/error_type_analysis
"""

import argparse
import json
from pathlib import Path
from typing import Dict, List, Optional

import pandas as pd

from evaluate_good_bad_plant_effect import (
    compute_statistics,
    infer_expected_direction,
)


METRICS = ["soundness", "presentation", "contribution", "rating"]


def build_comparisons(
    df: pd.DataFrame,
    baseline_label: str,
    good_label: str,
    bad_label: str,
    sham_label: Optional[str] = None,
) -> List[Dict]:
    """
    Infer the treatment-control comparisons that are available in the score table.
    """
    comparisons: List[Dict] = []

    sham_label = sham_label or good_label

    if "planted_error_score" in df.columns:
        comparisons.append(
            {
                "label": f"{bad_label} vs {baseline_label}",
                "treatment_score_col": "planted_error_score",
                "control_score_col": "baseline_score",
                "treatment_label": bad_label,
                "control_label": baseline_label,
                "expected_direction": infer_expected_direction(
                    bad_label, baseline_label
                ),
            }
        )
    if "sham_surgery_score" in df.columns:
        comparisons.append(
            {
                "label": f"{sham_label} vs {baseline_label}",
                "treatment_score_col": "sham_surgery_score",
                "control_score_col": "baseline_score",
                "treatment_label": sham_label,
                "control_label": baseline_label,
                "expected_direction": infer_expected_direction(
                    sham_label, baseline_label
                ),
            }
        )
    if (
        "planted_error_vs_sham_score" in df.columns
        and "sham_surgery_score" in df.columns
    ):
        comparisons.append(
            {
                "label": f"{bad_label} vs {sham_label}",
                "treatment_score_col": "planted_error_vs_sham_score",
                "control_score_col": "sham_surgery_score",
                "treatment_label": bad_label,
                "control_label": sham_label,
                "expected_direction": infer_expected_direction(bad_label, sham_label),
            }
        )
    if "good_score" in df.columns:
        comparisons.append(
            {
                "label": f"{good_label} vs {baseline_label}",
                "treatment_score_col": "good_score",
                "control_score_col": "baseline_score",
                "treatment_label": good_label,
                "control_label": baseline_label,
                "expected_direction": infer_expected_direction(
                    good_label, baseline_label
                ),
            }
        )
    if "bad_score" in df.columns:
        comparisons.append(
            {
                "label": f"{bad_label} vs {baseline_label}",
                "treatment_score_col": "bad_score",
                "control_score_col": "baseline_score",
                "treatment_label": bad_label,
                "control_label": baseline_label,
                "expected_direction": infer_expected_direction(
                    bad_label, baseline_label
                ),
            }
        )

    if not comparisons:
        raise ValueError(
            "No treatment/control columns were detected in score_differences. "
            "Ensure the CSV was generated by evaluate_good_bad_plant_effect.py."
        )

    return comparisons


def load_category_labels(mapping_df: pd.DataFrame, level: str) -> Dict[str, str]:
    """
    Build a lookup from category identifier to human-readable label.
    """
    if level == "subcategory":
        id_col = "category_id"
        label_col = "category_label"
    else:
        id_col = "category_group"
        label_col = "category_group_label"

    labels: Dict[str, str] = {}
    for _, row in mapping_df.drop_duplicates(subset=[id_col]).iterrows():
        identifier = row[id_col]
        if pd.isna(identifier):
            continue
        labels[str(identifier)] = row.get(label_col)

    return labels


def main() -> None:
    parser = argparse.ArgumentParser(
        description=(
            "Compute aggregated treatment effects for each flaw/error category."
        )
    )
    parser.add_argument(
        "--score_differences",
        type=Path,
        required=True,
        help="CSV produced by evaluate_good_bad_plant_effect.py",
    )
    parser.add_argument(
        "--category_mapping",
        type=Path,
        required=True,
        help="Output CSV from extract_error_categories.py",
    )
    parser.add_argument(
        "--category_level",
        choices=["subcategory", "group"],
        default="subcategory",
        help=(
            "Aggregate by fine-grained subcategory (e.g., 1a, 1b) or "
            "coarse group (1, 2, ...)."
        ),
    )
    parser.add_argument(
        "--output_dir",
        type=Path,
        required=True,
        help="Directory for the JSON/CSV summaries",
    )
    parser.add_argument(
        "--baseline_label",
        type=str,
        default="Baseline",
        help="Human-readable label for the baseline/control condition",
    )
    parser.add_argument(
        "--good_label",
        type=str,
        default="Sham Surgery",
        help="Label for the 'good' or neutral manipulation (if present)",
    )
    parser.add_argument(
        "--bad_label",
        type=str,
        default="Planted Error",
        help="Label for the adverse manipulation (if present)",
    )
    parser.add_argument(
        "--sham_label",
        type=str,
        default=None,
        help="Optional override label specifically for sham_surgery comparisons",
    )

    args = parser.parse_args()

    score_path: Path = args.score_differences
    mapping_path: Path = args.category_mapping
    output_dir: Path = args.output_dir

    if not score_path.exists():
        raise FileNotFoundError(f"Score differences CSV not found: {score_path}")
    if not mapping_path.exists():
        raise FileNotFoundError(f"Category mapping CSV not found: {mapping_path}")

    df_scores = pd.read_csv(score_path)
    df_mapping = pd.read_csv(mapping_path)

    category_column = "category_id" if args.category_level == "subcategory" else "category_group"
    category_labels = load_category_labels(df_mapping, args.category_level)

    comparisons = build_comparisons(
        df_scores,
        baseline_label=args.baseline_label,
        good_label=args.good_label,
        bad_label=args.bad_label,
        sham_label=args.sham_label,
    )

    summary_rows: List[Dict] = []
    json_summary: Dict[str, Dict] = {}

    for category_value in sorted(
        {str(v) for v in df_mapping[category_column].dropna().unique()}
    ):
        relevant_papers = df_mapping[
            df_mapping[category_column] == category_value
        ]["paper_id"].unique()

        filtered_scores = df_scores[df_scores["paper_id"].isin(relevant_papers)]
        if filtered_scores.empty:
            continue

        stats = compute_statistics(filtered_scores, comparisons, METRICS)
        if not stats:
            continue

        category_label = category_labels.get(category_value)
        cat_entry = json_summary.setdefault(
            category_value,
            {"label": category_label, "results": {}},
        )

        for metric in METRICS:
            metric_results = stats.get(metric, [])
            if not metric_results:
                continue

            cat_entry["results"].setdefault(metric, [])

            for result in metric_results:
                summary_rows.append(
                    {
                        "category_level": args.category_level,
                        "category_id": category_value,
                        "category_label": category_label,
                        "metric": metric,
                        "comparison": result["comparison"],
                        "treatment_label": result["treatment_label"],
                        "control_label": result["control_label"],
                        "expected_direction": result.get("expected_direction"),
                        "matches_expectation": result.get("matches_expectation"),
                        "n_pairs": result["n_pairs"],
                        "treatment_mean": result["treatment_mean"],
                        "control_mean": result["control_mean"],
                        "mean_difference": result["mean_difference"],
                        "cohen_d": result["cohen_d"],
                        "standard_error": result.get("standard_error"),
                        "ci_low": result.get("ci_low"),
                        "ci_high": result.get("ci_high"),
                    }
                )

                cat_entry["results"][metric].append(
                    {
                        "comparison": result["comparison"],
                        "treatment_label": result["treatment_label"],
                        "control_label": result["control_label"],
                        "expected_direction": result.get("expected_direction"),
                        "matches_expectation": result.get("matches_expectation"),
                        "n_pairs": result["n_pairs"],
                        "treatment_mean": result["treatment_mean"],
                        "control_mean": result["control_mean"],
                        "mean_difference": result["mean_difference"],
                        "cohen_d": result["cohen_d"],
                        "standard_error": result.get("standard_error"),
                        "ci_low": result.get("ci_low"),
                        "ci_high": result.get("ci_high"),
                    }
                )

    if not summary_rows:
        raise ValueError(
            "No category-level summaries were produced. "
            "Check that the mapping overlaps with the score_differences file."
        )

    summary_df = pd.DataFrame(summary_rows)
    summary_df = summary_df.sort_values(
        ["category_id", "metric", "comparison"]
    ).reset_index(drop=True)

    output_dir.mkdir(parents=True, exist_ok=True)
    csv_path = output_dir / "error_type_effects.csv"
    json_path = output_dir / "error_type_effects.json"

    summary_df.to_csv(csv_path, index=False)
    with json_path.open("w", encoding="utf-8") as f:
        json.dump(json_summary, f, indent=2)

    print(f"Saved category summaries to {csv_path} and {json_path}")


if __name__ == "__main__":
    main()

